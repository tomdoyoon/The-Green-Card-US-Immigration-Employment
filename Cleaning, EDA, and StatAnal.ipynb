{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4958a082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.9.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.9.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as st\n",
    "import cufflinks as cf\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected = True)\n",
    "cf.go_offline()\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go \n",
    "import datetime as dt\n",
    "import dataframe_image as dfi\n",
    "import Levenshtein as lev\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ff8e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"GCST.csv\", low_memory = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045f6217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PW_CODE CLEAN UP\n",
    "df[\"PW_SOC_CODE\"] = df[\"PW_SOC_CODE\"].astype(str)\n",
    "# PW_SOC_CODES that do not follow the standard XX-XXXX, missing \"-\"\n",
    "wrongsoc = df[~df[\"PW_SOC_CODE\"].str.contains(\"-\", na = False)]\n",
    "# DROPPING ANY APPLICANTS THAT DO NOT HAVE \"-\" IN PW SOC CODE\n",
    "df = df[df[\"PW_SOC_CODE\"].str.contains(\"-\", na = False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3865da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESET DF INDEX\n",
    "df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdb55a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLITTING PW_SOC_CODE FOR CLEANING DECIMALS\n",
    "# COL1 will be pre \"-\", COL2 will be after \"-\"... DROP THE DECIMALS (OLDER FORMAT, NOW MORE CONSOLIDATED)\n",
    "symbol = df[\"PW_SOC_CODE\"].str.find(\"-\")\n",
    "list1 = []\n",
    "for x in range(len(df)):\n",
    "    j = df[\"PW_SOC_CODE\"][x][:symbol[x]]\n",
    "    list1.append(j)\n",
    "list2 = []\n",
    "for x in range(len(df)):\n",
    "    j = df[\"PW_SOC_CODE\"][x][symbol[x]:]\n",
    "    list2.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63203222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COL1: HAS SOME WEIRD FORMATTING FROM CSV... TURNED IT INTO DATES... NOVEMBER IS IN THERE, SHOULD BE 11\n",
    "col1 = pd.DataFrame(list1)\n",
    "#col1[col1[0].str.len()>2][0].unique()\n",
    "col1[0] = col1[0].replace(\"Nov\",11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcfbba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(col1[0]==\"Nov\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa161d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROPPING ANYTHING AFTER DECIMAL FOR COL2\n",
    "list2 = [sub.replace('-', '') for sub in list2]\n",
    "col2 = pd.DataFrame(list2)\n",
    "decimal = col2[col2[0].str.find(\".\")>0][0].str[:-3]\n",
    "col2[0].loc[decimal.index] = decimal\n",
    "col2[0] = col2[0].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7046b59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF COL1 HAS NOV THAN, COL2 SHOULD BE THE YEAR in the 1900s\n",
    "col2[0] = np.where(col1[0]==\"Nov\",\"19\"+col2[0],col2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d81607a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF COL2 HAS NOV, THEN IT SHOULD BE JUST THE 2 DIGITS FROM COL1 AS COL2... FLIP THE TOP LOGIC\n",
    "p1 = np.where(col2[0]==\"Nov\",col1[0],col2[0])\n",
    "p2 = np.where(col2[0]==\"Nov\",11,col1[0])\n",
    "col2[0] = p1\n",
    "col1[0] = p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0e0846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TURNING 2 DIGIT Col2 and adding 00 at the end\n",
    "col2[0] = np.where(col2[0].str.len()<4,col2[0]+\"00\",col2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981713e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PUTTING BACK TOGETHER PW CODE CODE\n",
    "df[\"PW_SOC_CODE\"] = col1.astype(str)+\"-\"+col2.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9544fb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESET INDEX\n",
    "df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88ee0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGING DECISION DATE AND APPLICATION RECEIVE DATE INTO MONTH OF YEAR \n",
    "df[\"MONTH\"] = pd.to_datetime(df[\"DECISION_DATE\"]).apply(lambda x:x.month)\n",
    "df[\"YEAR\"] = pd.to_datetime(df[\"DECISION_DATE\"]).apply(lambda x:x.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241ca713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing certified and certified expired to 1. and withdrawn and denied to 0: CLASSIFICATION MODEL OUTPUT\n",
    "df[\"CASE_STATUS\"] = df[\"CASE_STATUS\"].replace([\"CERTIFIED-EXPIRED\",\"CERTIFIED\"],[1,1]).replace([\"WITHDRAWN\",\"DENIED\"],[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25abfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW MANY WAGE OFFERED AND PW_WAGE ARE NULL? HOW MANY UNITS ARE NULL?\n",
    "print(\"Number of PW Wage null:\",len(df[df[\"PW_WAGE\"].isnull()]))\n",
    "print(\"Number of Wage offered null:\",len(df[df[\"WAGE_OFFER_FROM\"].isnull()]))\n",
    "print(\"Number of PW Wage Unit Null:\",len(df[df[\"PW_UNIT_OF_PAY\"].isnull()]))\n",
    "print(\"Number of Wage Unit null:\",len(df[df[\"WAGE_OFFER_UNIT_OF_PAY\"].isnull()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631a6d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF COPY TO SEE HOW MANY WERE DROPPED\n",
    "dfc = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd05e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROP ALL APPLICANTS WITH NO WAGE UNIT\n",
    "df = df[df[\"WAGE_OFFER_UNIT_OF_PAY\"].notnull()]\n",
    "# DROP ALL APPLICANTS WITH NO PW UNIT\n",
    "df = df[df[\"PW_UNIT_OF_PAY\"].notnull()]\n",
    "# DROP ALL APPLICANTS WITH NO WAGE OFFERED\n",
    "df = df[df[\"WAGE_OFFER_FROM\"].notnull()]\n",
    "# DROP ALL APPLICANTS WITH NO PW WAGE\n",
    "df = df[df[\"PW_WAGE\"].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc53ef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW MANY DROPPED DUE TO NULL\n",
    "len(dfc)-len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1337bd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF COPY TO SEE HOW MANY WERE DROPPED\n",
    "dfc = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241464b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TURNING ALL WAGES TO FLOATS\n",
    "df[\"WAGE_OFFER_FROM\"] = df[\"WAGE_OFFER_FROM\"].str.replace(\",\",\"\")\n",
    "df = df[df[\"WAGE_OFFER_FROM\"]!=\"#############\"]\n",
    "df[\"PW_WAGE\"] = df[\"PW_WAGE\"].str.replace(\",\",\"\")\n",
    "df = df[df[\"PW_WAGE\"]!=\"#############\"]\n",
    "df[\"WAGE_OFFER_FROM\"] = df[\"WAGE_OFFER_FROM\"].astype(float)\n",
    "df[\"PW_WAGE\"] = df[\"PW_WAGE\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0a39bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW MANY DROPPED DUE TO #######?\n",
    "len(dfc)-len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc87dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STANDARD THE PW and SALARY UNIT OF PAY\n",
    "df[\"PW_UNIT_OF_PAY\"] = df[\"PW_UNIT_OF_PAY\"].replace([\"HOUR\",\"YEAR\",\"WEEK\",\"MONTH\",\"BI-WEEKLY\"],[\"HR\",\"YR\",\"WK\",\"MTH\",\"BI\"])\n",
    "df[\"WAGE_OFFER_UNIT_OF_PAY\"] = df[\"WAGE_OFFER_UNIT_OF_PAY\"].replace([\"HOUR\",\"YEAR\",\"WEEK\",\"MONTH\",\"BI-WEEKLY\"],[\"HR\",\"YR\",\"WK\",\"MTH\",\"BI\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc2991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISTRIBUTION OF UNIT OF PAY\n",
    "pd.DataFrame([df[\"WAGE_OFFER_UNIT_OF_PAY\"].value_counts(),df[\"PW_UNIT_OF_PAY\"].value_counts(),df[\"WAGE_OFFER_UNIT_OF_PAY\"].value_counts()-df[\"PW_UNIT_OF_PAY\"].value_counts()]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135f61c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANNUALIZING WAGE OFFER AND PW\n",
    "# Annualized Salaries: Actual Salary: multiplier\n",
    "def annual(row):\n",
    "    if row[\"WAGE_OFFER_UNIT_OF_PAY\"] == \"YR\":\n",
    "        return 1\n",
    "    elif row[\"WAGE_OFFER_UNIT_OF_PAY\"] == \"MTH\": \n",
    "        return 12\n",
    "    elif row[\"WAGE_OFFER_UNIT_OF_PAY\"] == \"WK\": \n",
    "        return 52\n",
    "    elif row[\"WAGE_OFFER_UNIT_OF_PAY\"] == \"BI\": \n",
    "        return 26\n",
    "    elif row[\"WAGE_OFFER_UNIT_OF_PAY\"] == \"HR\": \n",
    "        return 2080\n",
    "mult = pd.DataFrame(df.apply(lambda row:annual(row), axis = 1))\n",
    "mult.columns = [\"WAGE_OFFER_FROM\"]\n",
    "# ANNUALIZED WAGE OFFER\n",
    "df[\"WO_A\"] = pd.DataFrame(df[\"WAGE_OFFER_FROM\"])*mult\n",
    "\n",
    "# Annualized Salaries: PW Salary\n",
    "def annuali(row):\n",
    "    if row[\"PW_UNIT_OF_PAY\"] == \"YR\":\n",
    "        return 1\n",
    "    elif row[\"PW_UNIT_OF_PAY\"] == \"MTH\": \n",
    "        return 12\n",
    "    elif row[\"PW_UNIT_OF_PAY\"] == \"WK\": \n",
    "        return 52\n",
    "    elif row[\"PW_UNIT_OF_PAY\"] == \"BI\": \n",
    "        return 26\n",
    "    elif row[\"PW_UNIT_OF_PAY\"] == \"HR\": \n",
    "        return 2080\n",
    "multi = pd.DataFrame(df.apply(lambda row:annuali(row), axis = 1))\n",
    "multi.columns = [\"PW_WAGE\"]\n",
    "# ANNUALIZED WAGE PW\n",
    "df[\"PW_A\"] = pd.DataFrame(df[\"PW_WAGE\"])*multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dd1df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANNUALIZED WAGE OFFER - ANNUALIZED PW\n",
    "df[\"WO-PW\"] = df[\"WO_A\"] - df[\"PW_A\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d023de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STANDARDIZING THE NULLS\n",
    "df = df.replace(np.nan,\"\", regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b79ff33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HAVE WAGE OFFER BEEN < ANNUALIZED PW? THIS SOULD NEVER HAPPEN...\n",
    "lowwage = df[df[\"WO-PW\"]<0][[\"CASE_STATUS\",\"WAGE_OFFER_UNIT_OF_PAY\",\"PW_UNIT_OF_PAY\",\"WAGE_OFFER_FROM\",\"PW_WAGE\",\"WO_A\",\"PW_A\"]]\n",
    "lowwage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e55710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW MANY HAVE SUCCESSFUL APPLICATIONS WHEN WAGE < PW?\n",
    "print(\"Number of Successful applicants:\",lowwage[\"CASE_STATUS\"].sum())\n",
    "print(\"Out of:\",len(lowwage),\",\",lowwage[\"CASE_STATUS\"].sum()/len(lowwage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f56e45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUCESSFULL APPLICATIONS WITH THE SAME UNIT\n",
    "lowwage[lowwage[\"WAGE_OFFER_UNIT_OF_PAY\"]==lowwage[\"PW_UNIT_OF_PAY\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b9741a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A UNIQUE IDENTIFIER: THIS WILL BE TO UPDATE ANY OUTLIERS\n",
    "df[\"ID\"] = df[\"CASE_NUMBER\"].astype(str)+df[\"DECISION_DATE\"].astype(str)+df[\"CASE_STATUS\"].astype(str)+df[\"RECEIVED_DATE\"].astype(str)+df[\"DAYS_TO_DECIDE\"].astype(str)+df[\"EMPLOYER_NAME\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dd2399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATTERN FOR MIS-LABELED: UNIT OF PAY IS NOT YEAR BUT THE WAGE OFFER IS YEAR AMOUNT\n",
    "# FIND THIS BY UNIT OF PAY NOT YEAR, BUT WAGE OFFER > btm 5% OF WAGE OFFER of each YEAR\n",
    "top = df[df[\"WAGE_OFFER_UNIT_OF_PAY\"]==\"YR\"].groupby(\"YEAR\")[\"WAGE_OFFER_FROM\"].quantile(.05) \n",
    "top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258f1570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING THE LIST FOR THE FOR LOOP TO REPLACE IF ANNUALIZED SALARY IS > OUTLIER CUTOFF\n",
    "year = top.index.tolist()\n",
    "top = top.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c124b406",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GRABBING THE APPLICANTS THAT ARE NON YR BUT > btm 5% of YEAR WAGE\n",
    "todrop = pd.DataFrame()\n",
    "for i in range(len(year)):\n",
    "    j = df[(df[\"YEAR\"]==year[i])&(df[\"WAGE_OFFER_FROM\"]>=top[i])&(df[\"WAGE_OFFER_UNIT_OF_PAY\"]!=\"YR\")]\n",
    "    todrop = pd.concat([todrop,j])\n",
    "todrop[[\"CASE_STATUS\",\"WAGE_OFFER_UNIT_OF_PAY\",\"PW_UNIT_OF_PAY\",\"WAGE_OFFER_FROM\",\"PW_WAGE\",\"WO_A\",\"PW_A\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5968f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGING THE WO_A TO THE WAGE OFFER (LEAVE UNIT OF PAY AS WHAT THE APPLICATION SAYS, ONLY CHANGE MY* Annualized WAGES)\n",
    "todrop[\"WO_A\"]=todrop[\"WAGE_OFFER_FROM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64763cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK IF UNIQUE IDENTIFIER IS UNIQUE\n",
    "if len(df) == df[\"ID\"].nunique():\n",
    "    print(\"ID IS A UNIQUE IDENTIFIER\")\n",
    "else:\n",
    "    print(\"ID IS NOT A UNIQUE IDENTIFIER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce30f5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHAT IS THE MAX REPEAT OF IDs\n",
    "df[\"ID\"].value_counts()[df[\"ID\"].value_counts()>1].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9045b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISOLATE THE APPLICATIONS WITH THE SAME ID\n",
    "idcheck = df[\"ID\"].value_counts()[df[\"ID\"].value_counts()>1].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37771ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK TO SEE IF EACH DUPLICATE ID HAS IDENTICAL FIELDS (WOULD MEAN A MISTAKE AND THEN WE DROP). IF 0 THEN ALL IDENTICAL\n",
    "empty = []\n",
    "for i in range(len(idcheck)):\n",
    "    df[df[\"ID\"] == idcheck[i]]\n",
    "    j = df[df[\"ID\"] == idcheck[1]]\n",
    "    if (j.iloc[0] == j.iloc[1]).sum() == len(j.columns):\n",
    "        y = 0\n",
    "        empty.append(y)\n",
    "    else:\n",
    "        y=[1]\n",
    "sum(empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21690bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROP THE REPEAT APPLICATION FROM THE DF\n",
    "df = df.drop_duplicates(subset=[\"ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd167477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGING THE EXISTING APPLICANTS WITH UPDATED WAGE and UNIT OF PAY IN THE ORIGINAL DF\n",
    "# MATCH APPLICATIONS IN TODROP DF WITH ORIGINAL DF AND UPDATE WAGE DATA\n",
    "todrop = todrop.set_index(\"ID\")\n",
    "df = df.set_index(\"ID\")\n",
    "df.loc[todrop.index] = todrop\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d71932e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATTERN FOR MIS-LABELED: UNIT OF PAY IS NOT YEAR BUT THE PW WAGE IS YEAR AMOUNT\n",
    "# FIND THIS BY UNIT OF PAY NOT YEAR, BUT PW WAGE > btm 5% OF PW WAGE of each YEAR\n",
    "top1 = df[df[\"PW_UNIT_OF_PAY\"]==\"YR\"].groupby(\"YEAR\")[\"PW_WAGE\"].quantile(.05) \n",
    "top1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422a5de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING THE LIST FOR THE FOR LOOP TO REPLACE IF ANNUALIZED SALARY IS > OUTLIER CUTOFF\n",
    "year1 = top1.index.tolist()\n",
    "top1 = top1.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65596cf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GRABBING THE APPLICANTS THAT ARE OVER NON YR BUT > btm 5% of PW_WAGE\n",
    "todrop1 = pd.DataFrame()\n",
    "for i in range(len(year)):\n",
    "    j = df[(df[\"YEAR\"]==year1[i])&(df[\"PW_WAGE\"]>=top1[i])&(df[\"PW_UNIT_OF_PAY\"]!=\"YR\")]\n",
    "    todrop1 = pd.concat([todrop1,j])\n",
    "todrop1[[\"CASE_STATUS\",\"WAGE_OFFER_UNIT_OF_PAY\",\"PW_UNIT_OF_PAY\",\"WAGE_OFFER_FROM\",\"PW_WAGE\",\"WO_A\",\"PW_A\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73cc285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGING THE PW_A TO THE WAGE OFFER AGAIN LEAVING THE UNIT LIKE WO_A\n",
    "todrop1[\"PW_A\"]=todrop1[\"PW_WAGE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cad543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGING THE EXISTING APPLICANTS WITH UPDATED WAGE and UNIT OF PAY IN THE ORIGINAL DF\n",
    "# MATCH APPLICATIONS IN TODROP DF WITH ORIGINAL DF AND UPDATE WAGE DATA\n",
    "todrop1 = todrop1.set_index(\"ID\")\n",
    "df = df.set_index(\"ID\")\n",
    "df.loc[todrop1.index] = todrop1\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b097de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTINUE CLEANING OUTLIERS IF PW_UNIT IS NOT THE SAME AS WAGE_UNIT BUT WAGE_OFFER AND PW_WAGE IS THE SAME & PW UNIT IS YR:\n",
    "# UPDATE WO_A TO PW_A\n",
    "clean = df[(df[\"WAGE_OFFER_FROM\"]==df[\"PW_WAGE\"])&(df[\"PW_UNIT_OF_PAY\"]!=df[\"WAGE_OFFER_UNIT_OF_PAY\"])&(df[\"PW_UNIT_OF_PAY\"]==\"YR\")]\n",
    "clean[\"WO_A\"]=clean[\"PW_A\"]\n",
    "clean = clean.set_index(\"ID\")\n",
    "df = df.set_index(\"ID\")\n",
    "df.loc[clean.index] = clean\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64be4819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROPPING THE BTM 1% FROM EACH YEAR\n",
    "# SETTING UP THE FOR LOOP\n",
    "wagebtm = df.groupby(\"YEAR\")[\"WO_A\"].quantile(.01)\n",
    "pwbtm = df.groupby(\"YEAR\")[\"PW_A\"].quantile(.01)\n",
    "yr = wagebtm.index.tolist()\n",
    "wagebtm = wagebtm.tolist()\n",
    "pwbtm = pwbtm.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd70fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR LOOP TO GET ALL APPLICANTS WITH WO_A < CUTOFF\n",
    "emp = pd.DataFrame()\n",
    "for x in range(len(yr)):\n",
    "    j = df[(df[\"YEAR\"] == yr[x])&(df[\"WO_A\"]<wagebtm[x])]\n",
    "    emp = pd.concat([emp,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ade76a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW MANY ARE DROPPED FOR WO_A<CUTOFF?\n",
    "len(emp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafc59b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROPPING ALL APPLICANTS WO_A < CUTOFF\n",
    "emp = emp[\"ID\"].tolist()\n",
    "df = df[~df['ID'].isin(emp)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e39fbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR LOOP TO GET ALL APPLICANTS WITH PW_A < CUTOFF\n",
    "empt = pd.DataFrame()\n",
    "for x in range(len(yr)):\n",
    "    j = df[(df[\"YEAR\"] == yr[x])&(df[\"PW_A\"]<pwbtm[x])]\n",
    "    empt = pd.concat([empt,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e59df79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW MANY ARE DROPPED FOR PW_A < CUTOFF?\n",
    "len(empt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7fd496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROPPING ALL APPLICANTS PW < CUTOFF\n",
    "empt = empt[\"ID\"].tolist()\n",
    "df = df[~df['ID'].isin(empt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ae13a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEANING CITIES: EVENTUALLY WILL BE USED TO CREATE AVG WAGE TO REPLACE EXTREME OUTLIER WAGES\n",
    "# CLEAN UP JOB CITY: distribution of the length of the cities\n",
    "df[\"WORKSITE_CITY\"].str.len().plot(kind = \"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5aaa265",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TOO MANY TO MANUALLY CLEAN: BUT WE WILL TARGET CITIES UNDER 5 CHARECTERS TO SPELL OUT ANY ABR.\n",
    "df[df[\"WORKSITE_CITY\"].str.len()<5][\"WORKSITE_CITY\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4d57b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHAT ARE THE TOP CITIES?\n",
    "df[\"WORKSITE_CITY\"].value_counts()[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5cd301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW MANY VARIATIONS OF THE TOP CITIES?\n",
    "#df[df[\"WORKSITE_CITY\"].str.contains(\"edison\", case = False, na = False)][\"WORKSITE_CITY\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267dde6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TURNING ALL THE NYC VARATIONS INTO NEW YORK + SPELL OUT NEW YORK\n",
    "df[\"WORKSITE_CITY\"] =  df[\"WORKSITE_CITY\"].replace(\"NY\",\"NEW YORK\").replace(\"NYC\",\"NEW YORK\")\n",
    "df.loc[df[\"WORKSITE_CITY\"].str.contains(\"new york\", case = False, na = False), \"WORKSITE_CITY\"] = \"NEW YORK\"\n",
    "# MERGING THE TOP 15 cities (COLLEGE STATION, PLANO DOES NOT NEED CLEANING)\n",
    "df.loc[df[\"WORKSITE_CITY\"].str.contains(\"seattle\", case = False, na = False), \"WORKSITE_CITY\"] = \"SEATTLE\"\n",
    "df.loc[df[\"WORKSITE_CITY\"].str.contains(\"san francisco\", case = False, na = False), \"WORKSITE_CITY\"] = \"SAN FRANCISCO\"\n",
    "df.loc[df[\"WORKSITE_CITY\"].str.contains(\"REDMOND\", case = False, na = False), \"WORKSITE_CITY\"] = \"REDMOND\"\n",
    "df.loc[df[\"WORKSITE_CITY\"].str.contains(\"mountain view\", case = False, na = False), \"WORKSITE_CITY\"] = \"MOUNTAIN VIEW\"\n",
    "df.loc[df[\"WORKSITE_CITY\"].str.contains(\"san jose\", case = False, na = False), \"WORKSITE_CITY\"] = \"SAN JOSE\"\n",
    "df.loc[df[\"WORKSITE_CITY\"].str.contains(\"santa clara\", case = False, na = False), \"WORKSITE_CITY\"] = \"SANTA CLARA\"\n",
    "df.loc[df[\"WORKSITE_CITY\"].str.contains(\"chicago\", case = False, na = False), \"WORKSITE_CITY\"] = \"CHICAGO\"\n",
    "df.loc[df[\"WORKSITE_CITY\"].str.contains(\"houston\", case = False, na = False), \"WORKSITE_CITY\"] = \"HOUSTON\"\n",
    "df.loc[df[\"WORKSITE_CITY\"].str.contains(\"sunnyvale\", case = False, na = False), \"WORKSITE_CITY\"] = \"SUNNYVALE\"\n",
    "df.loc[df[\"WORKSITE_CITY\"].str.contains(\"austin/|austin,\", case = False, na = False), \"WORKSITE_CITY\"] = \"AUSTIN\"\n",
    "df.loc[df[\"WORKSITE_CITY\"].str.contains(\"los angeles\", case = False, na = False), \"WORKSITE_CITY\"] = \"LOS ANGELES\"\n",
    "df.loc[df[\"WORKSITE_CITY\"].str.contains(\"edison\", case = False, na = False), \"WORKSITE_CITY\"] = \"EDISON\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce968ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW MANY STATES ARE APPLICANTS TRYING TO WORK IN? SHOULD BE NO MORE THAN 60...\n",
    "df[\"WORKSITE_STATE\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfcef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING THE MAIN LIST TO TRY AND CLEAN MOST STATES INTO ABBREVIATIONS\n",
    "mainst= [\"Alabama\",\n",
    "\"Alaska\",\n",
    "\"Arizona\",\n",
    "\"Arkansas\",\n",
    "\"California\",\n",
    "\"Colorado\",\n",
    "\"Connecticut\",\n",
    "\"Delaware\",\n",
    "\"Florida\",\n",
    "\"Georgia\",\n",
    "\"Hawaii\",\n",
    "\"Idaho\",\n",
    "\"Illinois\",\n",
    "\"Indiana\",\n",
    "\"Iowa\",\n",
    "\"Kansas\",\n",
    "\"Kentucky\",\n",
    "\"Louisiana\",\n",
    "\"Maine\",\n",
    "\"Maryland\",\n",
    "\"Massachusetts\",\n",
    "\"Michigan\",\n",
    "\"Minnesota\",\n",
    "\"Mississippi\",\n",
    "\"Missouri\",\n",
    "\"Montana\",\n",
    "\"Nebraska\",\n",
    "\"Nevada\",\n",
    "\"New Hampshire\",\n",
    "\"New Jersey\",\n",
    "\"New Mexico\",\n",
    "\"New York\",\n",
    "\"North Carolina\",\n",
    "\"North Dakota\",\n",
    "\"Ohio\",\n",
    "\"Oklahoma\",\n",
    "\"Oregon\",\n",
    "\"Pennsylvania\",\n",
    "\"Rhode Island\",\n",
    "\"South Carolina\",\n",
    "\"South Dakota\",\n",
    "\"Tennessee\",\n",
    "\"Texas\",\n",
    "\"Utah\",\n",
    "\"Vermont\",\n",
    "\"Virginia\",\n",
    "\"Washington\",\n",
    "\"West Virginia\",\n",
    "\"Wisconsin\",\n",
    "\"Wyoming\",\n",
    "\"District of Columbia\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28efc771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING THE MAIN ABR TO TRY AND CLEAN MOST STATES INTO ABBREVIATIONS\n",
    "mainabr = [\"AL\",\n",
    "\"AK\",\n",
    "\"AZ\",\n",
    "\"AR\",\n",
    "\"CA\",\n",
    "\"CO\",\n",
    "\"CT\",\n",
    "\"DE\",\n",
    "\"FL\",\n",
    "\"GA\",\n",
    "\"HI\",\n",
    "\"ID\",\n",
    "\"IL\",\n",
    "\"IN\",\n",
    "\"IA\",\n",
    "\"KS\",\n",
    "\"KY\",\n",
    "\"LA\",\n",
    "\"ME\",\n",
    "\"MD\",\n",
    "\"MA\",\n",
    "\"MI\",\n",
    "\"MN\",\n",
    "\"MS\",\n",
    "\"MO\",\n",
    "\"MT\",\n",
    "\"NE\",\n",
    "\"NV\",\n",
    "\"NH\",\n",
    "\"NJ\",\n",
    "\"NM\",\n",
    "\"NY\",\n",
    "\"NC\",\n",
    "\"ND\",\n",
    "\"OH\",\n",
    "\"OK\",\n",
    "\"OR\",\n",
    "\"PA\",\n",
    "\"RI\",\n",
    "\"SC\",\n",
    "\"SD\",\n",
    "\"TN\",\n",
    "\"TX\",\n",
    "\"UT\",\n",
    "\"VT\",\n",
    "\"VA\",\n",
    "\"WA\",\n",
    "\"WV\",\n",
    "\"WI\",\n",
    "\"WY\",\n",
    "\"DC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba604a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUND 1 CLEANING STATES\n",
    "for j in range(len(mainst)):\n",
    "    df.loc[df[\"WORKSITE_STATE\"].str.contains(mainst[j], case = False, na = False), \"WORKSITE_STATE\"] = mainabr[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac4fbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW MANY STATES ARE THERE NOT ABR AFTER ROUND 1 CLEANING?\n",
    "df[\"WORKSITE_STATE\"].unique()[np.vectorize(len)(df[\"WORKSITE_STATE\"].unique())>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52cfc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING THE SECONDARY LIST TO FINISH CLEANING STATES\n",
    "secondst = ['PUERTO RICO', 'GUAM', 'NORTHERN MARIANA ISLANDS',\n",
    "       'VIRGIN ISLANDS', 'MARSHALL ISLANDS',\n",
    "       'FEDERATED STATES OF MICRONESIA']\n",
    "secondabr = [\"PR\",\"GU\",\"MP\",\"VI\",\"MH\",\"FSM\"]\n",
    "# ROUND 2 CLEANING STATES\n",
    "for j in range(len(secondst)):\n",
    "    df.loc[df[\"WORKSITE_STATE\"].str.contains(secondst[j], case = False, na = False), \"WORKSITE_STATE\"] = secondabr[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0130656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING US GEOGRAPHIC REGION: STATE LIST\n",
    "geostate = [\"AK\",\n",
    "\"AL\",\n",
    "\"AR\",\n",
    "\"AZ\",\n",
    "\"CA\",\n",
    "\"CO\",\n",
    "\"CT\",\n",
    "\"DC\",\n",
    "\"DE\",\n",
    "\"FL\",\n",
    "\"GA\",\n",
    "\"HI\",\n",
    "\"IA\",\n",
    "\"ID\",\n",
    "\"IL\",\n",
    "\"IN\",\n",
    "\"KS\",\n",
    "\"KY\",\n",
    "\"LA\",\n",
    "\"MA\",\n",
    "\"MD\",\n",
    "\"ME\",\n",
    "\"MI\",\n",
    "\"MN\",\n",
    "\"MO\",\n",
    "\"MS\",\n",
    "\"MT\",\n",
    "\"NC\",\n",
    "\"ND\",\n",
    "\"NE\",\n",
    "\"NH\",\n",
    "\"NJ\",\n",
    "\"NM\",\n",
    "\"NV\",\n",
    "\"NY\",\n",
    "\"OH\",\n",
    "\"OK\",\n",
    "\"OR\",\n",
    "\"PA\",\n",
    "\"RI\",\n",
    "\"SC\",\n",
    "\"SD\",\n",
    "\"TN\",\n",
    "\"TX\",\n",
    "\"UT\",\n",
    "\"VA\",\n",
    "\"VT\",\n",
    "\"WA\",\n",
    "\"WI\",\n",
    "\"WV\",\n",
    "\"WY\",\n",
    "\"PR\",\n",
    "\"GU\",\n",
    "\"MP\",\n",
    "\"VI\",\n",
    "\"MH\",\n",
    "\"FSM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a73d4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING US GEOGRAPHIC REGION: REGION LIST\n",
    "region= [\"WEST\",\n",
    "\"SOUTH\",\n",
    "\"SOUTH\",\n",
    "\"WEST\",\n",
    "\"WEST\",\n",
    "\"WEST\",\n",
    "\"NORTHEAST\",\n",
    "\"SOUTH\",\n",
    "\"SOUTH\",\n",
    "\"SOUTH\",\n",
    "\"SOUTH\",\n",
    "\"WEST\",\n",
    "\"MIDWEST\",\n",
    "\"WEST\",\n",
    "\"MIDWEST\",\n",
    "\"MIDWEST\",\n",
    "\"MIDWEST\",\n",
    "\"SOUTH\",\n",
    "\"SOUTH\",\n",
    "\"NORTHEAST\",\n",
    "\"SOUTH\",\n",
    "\"NORTHEAST\",\n",
    "\"MIDWEST\",\n",
    "\"MIDWEST\",\n",
    "\"MIDWEST\",\n",
    "\"SOUTH\",\n",
    "\"WEST\",\n",
    "\"SOUTH\",\n",
    "\"MIDWEST\",\n",
    "\"MIDWEST\",\n",
    "\"NORTHEAST\",\n",
    "\"NORTHEAST\",\n",
    "\"WEST\",\n",
    "\"WEST\",\n",
    "\"NORTHEAST\",\n",
    "\"MIDWEST\",\n",
    "\"SOUTH\",\n",
    "\"WEST\",\n",
    "\"NORTHEAST\",\n",
    "\"NORTHEAST\",\n",
    "\"SOUTH\",\n",
    "\"MIDWEST\",\n",
    "\"SOUTH\",\n",
    "\"SOUTH\",\n",
    "\"WEST\",\n",
    "\"SOUTH\",\n",
    "\"NORTHEAST\",\n",
    "\"WEST\",\n",
    "\"MIDWEST\",\n",
    "\"SOUTH\",\n",
    "\"WEST\",\n",
    "\"TERRITORY\",\n",
    "\"TERRITORY\",\n",
    "\"TERRITORY\",\n",
    "\"TERRITORY\",\n",
    "\"TERRITORY\",\n",
    "\"TERRITORY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ac2af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING A WORKSITE REGION COLUMN\n",
    "df[\"WORKSITE_REGION\"]=df[\"WORKSITE_STATE\"]\n",
    "for j in range(len(geostate)):\n",
    "    df.loc[df[\"WORKSITE_REGION\"].str.contains(geostate[j], case = False, na = False), \"WORKSITE_REGION\"] = region[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdf1c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNIQUE REGIONS (CHECK IF ALL STATES WERE TRANSLATED CORRECTLY)\n",
    "df[\"WORKSITE_REGION\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c493b128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTLIER WO_A USING IQR (EXTREME OUTLIER PER YEAR) TOP SIDE\n",
    "wagetop = df.groupby(\"YEAR\")[\"WO_A\"].quantile(.75) + (df.groupby(\"YEAR\")[\"WO_A\"].quantile(.75)-df.groupby(\"YEAR\")[\"WO_A\"].quantile(.25)) *3\n",
    "pwtop = df.groupby(\"YEAR\")[\"PW_A\"].quantile(.75) + (df.groupby(\"YEAR\")[\"PW_A\"].quantile(.75)-df.groupby(\"YEAR\")[\"PW_A\"].quantile(.25)) *3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78da70f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING LISTS TO CLEAN UP WAGE BASED ON OUTLIERS : TOP END ONLY... DROP BOTTOM 1% (LAWS PROTECTING THE BTM END... MORE LIKELY TOP END IS ERROR)\n",
    "yr = wagetop.index.tolist()\n",
    "wagetop = wagetop.tolist()\n",
    "pwtop = pwtop.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e2c69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR LOOP TO GET ALL APPLICANTS WITH WO_A > CUTOFF: BIGWAGE\n",
    "bigwage = pd.DataFrame()\n",
    "for x in range(len(yr)):\n",
    "    j = df[(df[\"YEAR\"] == yr[x])&(df[\"WO_A\"]>wagetop[x])]\n",
    "    bigwage = pd.concat([bigwage,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ddcf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING THE REST OF THE APPLICANTS WHO ARE NOT > CUTOFF: REGWAGE\n",
    "bwlist = bigwage[\"ID\"].tolist()\n",
    "regwage = df[~df['ID'].isin(bwlist)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab948dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING A UNIQUE IDENTIFIER TO GET AN AVG WAGE FOR EACH IDENTIFIER TO REPLACE OUTLIER SALARIES\n",
    "regwage[\"BID\"] = regwage[\"PW_SOC_CODE\"].astype(str)+regwage[\"WORKSITE_STATE\"].astype(str) + regwage[\"WORKSITE_CITY\"].astype(str) + regwage[\"PW_SKILL_LEVEL\"].astype(str)\n",
    "# IS THIS TOO GRANULAR OF AN AVG? LETS SEE AVG COUNT FOR EACH UNIQUE IDENTIFIER\n",
    "regwage.groupby(\"BID\")[\"WO_A\"].value_counts().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c72ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW IDENTIFIER : USING STATE ONLY\n",
    "regwage[\"BID\"] = regwage[\"PW_SOC_CODE\"].astype(str)+regwage[\"WORKSITE_STATE\"].astype(str)+ regwage[\"PW_SKILL_LEVEL\"].astype(str)\n",
    "regwage.groupby(\"BID\")[\"WO_A\"].value_counts().mean()\n",
    "# STILL ONLY 2.7  APPLICANTS A CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2585b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW IDENTIFIER : USING REGION ONLY\n",
    "regwage[\"BID\"] = regwage[\"PW_SOC_CODE\"].astype(str)+regwage[\"WORKSITE_REGION\"].astype(str)+ regwage[\"PW_SKILL_LEVEL\"].astype(str)\n",
    "regwage.groupby(\"BID\")[\"WO_A\"].value_counts().mean()\n",
    "# BETTER with 3.3... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176551fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALIZED AVG WAGE FOR EACH UNIQUE IDENTIFIER\n",
    "normwage = pd.DataFrame(regwage.groupby(\"BID\")[\"WO_A\"].mean())\n",
    "normpw = pd.DataFrame(regwage.groupby(\"BID\")[\"PW_A\"].mean())\n",
    "normwage.columns=[\"NORM_WAGE\"]\n",
    "normpw.columns = [\"NORM_PW\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece9102a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING THE UNIQUE IDENTIFIER FOR BIG WAGE TO MATCH UP WITH REG WAGE AVG\n",
    "bigwage[\"BID\"] = bigwage[\"PW_SOC_CODE\"].astype(str)+bigwage[\"WORKSITE_REGION\"].astype(str)+ bigwage[\"PW_SKILL_LEVEL\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ca241c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGING BASED ON BID AND BRINGING IN NORM WAGE\n",
    "matchbidwage = bigwage\n",
    "matchbidwage = matchbidwage.merge(normwage, how = \"inner\", on = \"BID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3834e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW MANY APPLICANTS WERE NOT MATCHED THROUGH BID\n",
    "len(bigwage) - len(matchbidwage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d01cce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIST THE BIDs THAT DO NOT MATCH IN NORM WAGE: THESE ARE THE BIDs NOT IN THE NORM WAGE...\n",
    "bwregion = bigwage[\"BID\"].unique().tolist()\n",
    "normwageregion = normwage.index.tolist()\n",
    "list(set(bwregion) - set(normwageregion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e96b2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING THE APPLICANTS THAT DID NOT MATCH IN BID for NORM WAGE\n",
    "nonmatchwage = pd.DataFrame()\n",
    "for x in range(len(list(set(bwregion) - set(normwageregion)))):\n",
    "    j = bigwage[bigwage[\"BID\"]==list(set(bwregion) - set(normwageregion))[x]]\n",
    "    nonmatchwage = pd.concat([nonmatchwage, j],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc7c9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOES THE NUMBER OF APPLICANTS THAT DO NOT MATCH = TO 35?\n",
    "len(nonmatchwage)\n",
    "# YES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adb3579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRELATION BETWEEN REGION OR LEVEL WITH WAGE: TO DETERMINE HOW TO ADJUST THE BID\n",
    "regiondummy = pd.get_dummies(df[\"WORKSITE_REGION\"])\n",
    "leveldummy = pd.get_dummies(df[\"PW_SKILL_LEVEL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eaed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORREL BETWEEN REGIONS AND WO_A\n",
    "pd.concat([regiondummy,df[\"WO_A\"]],axis = 1).corr()[\"WO_A\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c60646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORREL BETWEEN LEVEL AND WO_A\n",
    "pd.concat([leveldummy,df[\"WO_A\"]],axis = 1).corr()[\"WO_A\"]\n",
    "# LEVELS LOOK MUCH HIGHER OVERALL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0b3c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING THE ADJUSTED BID FOR NORM WAGE: ONLY KEEPING LEVELS SINCE IT IS MORE CORREL WITH WAGE THAN REGION\n",
    "adjregw = regwage\n",
    "adjregw[\"BID\"] = adjregw[\"PW_SOC_CODE\"].astype(str)+adjregw[\"PW_SKILL_LEVEL\"].astype(str)\n",
    "# NORMALIZED AVG WAGE FOR EACH UNIQUE IDENTIFIER\n",
    "adjnormwage = pd.DataFrame(adjregw.groupby(\"BID\")[\"WO_A\"].mean())\n",
    "adjnormwage.columns=[\"NORM_WAGE\"]\n",
    "\n",
    "# CREATING THE UNIQUE IDENTIFIER FOR MISSING BIG WAGE TO MATCH UP WITH REG WAGE AVG\n",
    "adjbigwage = nonmatchwage\n",
    "adjbigwage[\"BID\"] = adjbigwage[\"PW_SOC_CODE\"].astype(str)+ adjbigwage[\"PW_SKILL_LEVEL\"].astype(str)\n",
    "\n",
    "# MERGING BASED ON BID AND BRINGING IN NORM WAGE\n",
    "matchadjbidwage = adjbigwage\n",
    "matchadjbidwage = matchadjbidwage.merge(adjnormwage, how = \"inner\", on = \"BID\")\n",
    "\n",
    "# HOW MANY APPLICANTS WERE NOT MATCHED THROUGH AdjBID\n",
    "len(adjbigwage) - len(matchadjbidwage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c4e36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIST THE Adj BIDs THAT DO NOT MATCH IN NORM WAGE: THESE ARE THE Adj BIDs NOT IN THE NORM WAGE...\n",
    "adjbwregion = adjbigwage[\"BID\"].unique().tolist()\n",
    "adjnormwageregion = adjnormwage.index.tolist()\n",
    "list(set(adjbwregion) - set(adjnormwageregion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c6a66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIST THE APPLICANTS THAT ARE STILL NOT MATCHED THROUGH BID...\n",
    "nonmatchadjwage = pd.DataFrame()\n",
    "for x in range(len(list(set(adjbwregion) - set(adjnormwageregion)))):\n",
    "    j = adjbigwage[adjbigwage[\"BID\"]==list(set(adjbwregion) - set(adjnormwageregion))[x]]\n",
    "    nonmatchadjwage = pd.concat([nonmatchadjwage, j],axis = 0)\n",
    "\n",
    "# DOES THE NUMBER OF APPLICANTS THAT DO NOT MATCH = TO 2?\n",
    "len(nonmatchadjwage)\n",
    "# YES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e239dfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING THE WAGE FOR SOC CODE ONLY FOR NORM WAGE\n",
    "soconly = regwage\n",
    "soconly[\"BID\"] = soconly[\"PW_SOC_CODE\"].astype(str)\n",
    "# NORMALIZED AVG WAGE FOR EACH UNIQUE IDENTIFIER\n",
    "socnormwage = pd.DataFrame(soconly.groupby(\"BID\")[\"WO_A\"].mean())\n",
    "socnormwage.columns=[\"NORM_WAGE\"]\n",
    "\n",
    "# CREATING THE UNIQUE IDENTIFIER FOR MISSING BIG WAGE TO MATCH UP WITH REG WAGE AVG\n",
    "socbigwage = nonmatchadjwage\n",
    "socbigwage[\"BID\"] = socbigwage[\"PW_SOC_CODE\"].astype(str)\n",
    "\n",
    "# MERGING BASED ON BID AND BRINGING IN NORM WAGE\n",
    "matchsocbidwage = socbigwage\n",
    "matchsocbidwage = matchsocbidwage.merge(socnormwage, how = \"inner\", on = \"BID\")\n",
    "\n",
    "# HOW MANY APPLICANTS WERE NOT MATCHED THROUGH AdjBID\n",
    "len(socbigwage) - len(matchsocbidwage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d626b715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGING BACK TO MAKE THE FINAL BIG WAGE : USING NORM WAGE...\n",
    "bigwage = pd.concat([matchbidwage,\n",
    "matchadjbidwage,\n",
    "matchsocbidwage],axis = 0, ignore_index = True)\n",
    "bigwage[\"WO_A\"] = bigwage[\"NORM_WAGE\"]\n",
    "bigwage = bigwage.drop(\"NORM_WAGE\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6441be14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEANING UP THE PW_A\n",
    "# CREATING THE UNIQUE IDENTIFIER FOR BIG WAGE TO MATCH UP WITH REG WAGE AVG\n",
    "bigwage[\"BID\"] = bigwage[\"PW_SOC_CODE\"].astype(str)+bigwage[\"WORKSITE_REGION\"].astype(str)+ bigwage[\"PW_SKILL_LEVEL\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd6755f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGING BASED ON BID AND BRINGING IN NORM PW\n",
    "matchbidwage = bigwage\n",
    "matchbidwage = matchbidwage.merge(normpw, how = \"inner\", on = \"BID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2dca9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW MANY APPLICANTS WERE NOT MATCHED THROUGH BID\n",
    "len(bigwage) - len(matchbidwage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc58c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIST THE BIDs THAT DO NOT MATCH IN NORM WAGE: THESE ARE THE BIDs NOT IN THE NORM WAGE...\n",
    "bwregion = bigwage[\"BID\"].unique().tolist()\n",
    "normwageregion = normpw.index.tolist()\n",
    "list(set(bwregion) - set(normwageregion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c029043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING THE APPLICANTS THAT DID NOT MATCH IN BID for NORM PW\n",
    "nonmatchwage = pd.DataFrame()\n",
    "for x in range(len(list(set(bwregion) - set(normwageregion)))):\n",
    "    j = bigwage[bigwage[\"BID\"]==list(set(bwregion) - set(normwageregion))[x]]\n",
    "    nonmatchwage = pd.concat([nonmatchwage, j],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff824c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOES THE NUMBER OF APPLICANTS THAT DO NOT MATCH = TO 35?\n",
    "len(nonmatchwage)\n",
    "# YES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2666d5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING THE ADJUSTED BID FOR NORM PW: ONLY KEEPING LEVELS SINCE IT IS MORE CORREL WITH WAGE THAN REGION\n",
    "adjregw = regwage\n",
    "adjregw[\"BID\"] = adjregw[\"PW_SOC_CODE\"].astype(str)+adjregw[\"PW_SKILL_LEVEL\"].astype(str)\n",
    "# NORMALIZED AVG WAGE FOR EACH UNIQUE IDENTIFIER\n",
    "adjnormwage = pd.DataFrame(adjregw.groupby(\"BID\")[\"PW_A\"].mean())\n",
    "adjnormwage.columns=[\"NORM_PW\"]\n",
    "\n",
    "# CREATING THE UNIQUE IDENTIFIER FOR MISSING BIG WAGE TO MATCH UP WITH REG WAGE AVG\n",
    "adjbigwage = nonmatchwage\n",
    "adjbigwage[\"BID\"] = adjbigwage[\"PW_SOC_CODE\"].astype(str)+ adjbigwage[\"PW_SKILL_LEVEL\"].astype(str)\n",
    "\n",
    "# MERGING BASED ON BID AND BRINGING IN NORM WAGE\n",
    "matchadjbidwage = adjbigwage\n",
    "matchadjbidwage = matchadjbidwage.merge(adjnormwage, how = \"inner\", on = \"BID\")\n",
    "\n",
    "# HOW MANY APPLICANTS WERE NOT MATCHED THROUGH AdjBID\n",
    "len(adjbigwage) - len(matchadjbidwage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96161b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIST THE Adj BIDs THAT DO NOT MATCH IN NORM WAGE: THESE ARE THE Adj BIDs NOT IN THE NORM WAGE...\n",
    "adjbwregion = adjbigwage[\"BID\"].unique().tolist()\n",
    "adjnormwageregion = adjnormwage.index.tolist()\n",
    "list(set(adjbwregion) - set(adjnormwageregion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6c3bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIST THE APPLICANTS THAT ARE STILL NOT MATCHED THROUGH BID...\n",
    "nonmatchadjwage = pd.DataFrame()\n",
    "for x in range(len(list(set(adjbwregion) - set(adjnormwageregion)))):\n",
    "    j = adjbigwage[adjbigwage[\"BID\"]==list(set(adjbwregion) - set(adjnormwageregion))[x]]\n",
    "    nonmatchadjwage = pd.concat([nonmatchadjwage, j],axis = 0)\n",
    "\n",
    "# DOES THE NUMBER OF APPLICANTS THAT DO NOT MATCH = TO 2?\n",
    "len(nonmatchadjwage)\n",
    "# YES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb3e73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING THE WAGE FOR SOC CODE ONLY FOR NORM WAGE\n",
    "soconly = regwage\n",
    "soconly[\"BID\"] = soconly[\"PW_SOC_CODE\"].astype(str)\n",
    "# NORMALIZED AVG WAGE FOR EACH UNIQUE IDENTIFIER\n",
    "socnormwage = pd.DataFrame(soconly.groupby(\"BID\")[\"PW_A\"].mean())\n",
    "socnormwage.columns=[\"NORM_PW\"]\n",
    "\n",
    "# CREATING THE UNIQUE IDENTIFIER FOR MISSING BIG WAGE TO MATCH UP WITH REG WAGE AVG\n",
    "socbigwage = nonmatchadjwage\n",
    "socbigwage[\"BID\"] = socbigwage[\"PW_SOC_CODE\"].astype(str)\n",
    "\n",
    "# MERGING BASED ON BID AND BRINGING IN NORM WAGE\n",
    "matchsocbidwage = socbigwage\n",
    "matchsocbidwage = matchsocbidwage.merge(socnormwage, how = \"inner\", on = \"BID\")\n",
    "\n",
    "# HOW MANY APPLICANTS WERE NOT MATCHED THROUGH AdjBID\n",
    "len(socbigwage) - len(matchsocbidwage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf2e2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGING BACK TO MAKE THE FINAL BIG WAGE : USING NORM WAGE...\n",
    "bigwage = pd.concat([matchbidwage,\n",
    "matchadjbidwage,\n",
    "matchsocbidwage],axis = 0, ignore_index = True)\n",
    "bigwage[\"PW_A\"] = bigwage[\"NORM_PW\"]\n",
    "bigwage = bigwage.drop([\"BID\",\"NORM_PW\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fe52c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MATCH APPLICATIONS IN BIGWAGE DF WITH ORIGINAL DF AND UPDATE WAGE AND PW DATA\n",
    "bigwage = bigwage.set_index(\"ID\")\n",
    "df = df.set_index(\"ID\")\n",
    "df.loc[bigwage.index] = bigwage\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fa0fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANNUALIZED WAGE OFFER - ANNUALIZED PW: REDO, WITH CLEANED UP ANNUAL WAGE AND PW\n",
    "df[\"WO-PW\"] = df[\"WO_A\"] - df[\"PW_A\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fc6111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEANING UP DAYS TO DECIDE\n",
    "# IQR EXTREME OUTLIER... SINCE WE HAVE NO IDEA IF AN APPLICANT TOOK THAT LONG, WE CANNOT NORMALIZE IT\n",
    "# DROP THE OUTLIERS ABOVE 99.5%\n",
    "ddtop = df.groupby(\"YEAR\")[\"DAYS_TO_DECIDE\"].quantile(.995)\n",
    "ddtop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259ab2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW COLUMN TO DROP\n",
    "ddcutoff = ddtop.tolist()\n",
    "for x in range(len(yr)):\n",
    "    df[\"drop\"] = np.where((df[\"YEAR\"]==yr[x])&(df[\"DAYS_TO_DECIDE\"]>=ddcutoff[x]),\"DROP\",np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7517ccf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW MANY ARE WE DROPPING?\n",
    "(df[\"drop\"]==\"DROP\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b951f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROPPING ALL DAYS TO DECIDE > 99 PERCENTILE\n",
    "df = df[df[\"drop\"]!=\"DROP\"].drop(\"drop\",axis = 1).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b37fc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEANING EMPLOYER HQ CITIES\n",
    "# CLEAN UP JOB CITY: distribution of the length of the cities\n",
    "df[\"EMPLOYER_CITY\"].str.len().plot(kind = \"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c17991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOO MANY TO MANUALLY CLEAN: \n",
    "# WHAT ARE THE TOP CITIES?\n",
    "df[\"EMPLOYER_CITY\"].value_counts()[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5850cd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW MANY VARIATIONS OF THE TOP CITIES?\n",
    "#df[df[\"EMPLOYER_CITY\"].str.contains(\"mountain view\", case = False, na = False)][\"EMPLOYER_CITY\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686654ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TURNING ALL THE NYC VARATIONS INTO NEW YORK + SPELL OUT NEW YORK\n",
    "df[\"EMPLOYER_CITY\"] =  df[\"EMPLOYER_CITY\"].replace(\"NY\",\"NEW YORK\").replace(\"NYC\",\"NEW YORK\")\n",
    "df.loc[df[\"EMPLOYER_CITY\"].str.contains(\"new york\", case = False, na = False), \"EMPLOYER_CITY\"] = \"NEW YORK\"\n",
    "# MERGING THE TOP 15 cities (santa clara, COLLEGE STATION,redmond, cupertino, menlo park, and PLANO did NOT NEED CLEANING)\n",
    "df.loc[df[\"EMPLOYER_CITY\"].str.contains(\"seattle\", case = False, na = False), \"EMPLOYER_CITY\"] = \"SEATTLE\"\n",
    "df.loc[df[\"EMPLOYER_CITY\"].str.contains(\"san francisco\", case = False, na = False), \"EMPLOYER_CITY\"] = \"SAN FRANCISCO\"\n",
    "df.loc[df[\"EMPLOYER_CITY\"].str.contains(\"san jose\", case = False, na = False), \"EMPLOYER_CITY\"] = \"SAN JOSE\"\n",
    "df.loc[df[\"EMPLOYER_CITY\"].str.contains(\"chicago\", case = False, na = False), \"EMPLOYER_CITY\"] = \"CHICAGO\"\n",
    "df.loc[df[\"EMPLOYER_CITY\"].str.contains(\"houston\", case = False, na = False), \"EMPLOYER_CITY\"] = \"HOUSTON\"\n",
    "df.loc[df[\"EMPLOYER_CITY\"].str.contains(\"sunnyvale\", case = False, na = False), \"EMPLOYER_CITY\"] = \"SUNNYVALE\"\n",
    "df.loc[df[\"EMPLOYER_CITY\"].str.contains(\"los angeles\", case = False, na = False), \"EMPLOYER_CITY\"] = \"LOS ANGELES\"\n",
    "df.loc[df[\"EMPLOYER_CITY\"].str.contains(\"edison\", case = False, na = False), \"EMPLOYER_CITY\"] = \"EDISON\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4c3d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW MANY STATES ARE EMPLOYERS HQ IN? SHOULD BE NO MORE THAN 60...\n",
    "df[\"EMPLOYER_STATE_PROVINCE\"].nunique()\n",
    "# ... 1157"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae68af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUND 1 CLEANING STATES\n",
    "for j in range(len(mainst)):\n",
    "    df.loc[df[\"EMPLOYER_STATE_PROVINCE\"].str.contains(mainst[j], case = False, na = False), \"EMPLOYER_STATE_PROVINCE\"] = mainabr[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33959e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW MANY STATES ARE THERE NOT ABR AFTER ROUND 1 CLEANING?\n",
    "df[\"EMPLOYER_STATE_PROVINCE\"].unique()[np.vectorize(len)(df[\"EMPLOYER_STATE_PROVINCE\"].unique())>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a29613",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"EMPLOYER_STATE_PROVINCE\"].unique()[np.vectorize(len)(df[\"EMPLOYER_STATE_PROVINCE\"].unique())>2].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f722ee86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING THE SECONDARY LIST TO FINISH CLEANING STATES\n",
    "secondst = ['PUERTO RICO', 'GUAM', 'NORTHERN MARIANA ISLANDS',\n",
    "       'VIRGIN ISLANDS', 'MARSHALL ISLANDS',\n",
    "       'FEDERATED STATES OF MICRONESIA',\" PR\",\"FL 32773\",\"FL BUILDIN\",\"OR OR\",\"BRITISH COLUMBIA\"]\n",
    "secondabr = [\"PR\",\"GU\",\"MP\",\"VI\",\"MH\",\"FSM\",\"PR\",\"FL\",\"FL\",\"OR\",\"BC\"]\n",
    "# ROUND 2 CLEANING STATES\n",
    "for j in range(len(secondst)):\n",
    "    df.loc[df[\"EMPLOYER_STATE_PROVINCE\"].str.contains(secondst[j], case = False, na = False), \"EMPLOYER_STATE_PROVINCE\"] = secondabr[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77ce18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING A WORKSITE REGION COLUMN\n",
    "df[\"EMPLOYER_REGION\"]=df[\"EMPLOYER_STATE_PROVINCE\"]\n",
    "for j in range(len(geostate)):\n",
    "    df.loc[df[\"EMPLOYER_REGION\"].str.contains(geostate[j], case = False, na = False), \"EMPLOYER_REGION\"] = region[j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648adff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNIQUE REGIONS (CHECK IF ALL STATES WERE TRANSLATED CORRECTLY)\n",
    "df[\"EMPLOYER_REGION\"] = df[\"EMPLOYER_REGION\"].replace(\"BC\",\"OTHER\")\n",
    "df[\"EMPLOYER_REGION\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3e893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADDING A CATEGORICAL VARIABLE OF DISCREPANCY BETWEEN EMPLOYER HQ STATE , REGION, AND CITY WITH JOB WORK SITE. UNIT OF WAGE AS WELL\n",
    "df[\"DISC_ST\"] = np.where(df[\"EMPLOYER_STATE_PROVINCE\"]==df[\"WORKSITE_STATE\"], 0, 1)\n",
    "df[\"DISC_CTY\"] = np.where(df[\"EMPLOYER_CITY\"]==df[\"WORKSITE_CITY\"], 0, 1)\n",
    "df[\"DISC_RGION\"] = np.where(df[\"EMPLOYER_REGION\"]==df[\"WORKSITE_REGION\"], 0, 1)\n",
    "df[\"DISC_UNIT\"] = np.where(df[\"WAGE_OFFER_UNIT_OF_PAY\"]==df[\"PW_UNIT_OF_PAY\"], 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635768c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MANUAL CLEANING OF EMPLOYER NAMES: THIS CODE IS TAKEN FROM THE US IMMIGRANT EMPLOYEMENT MINIPROJECT IN MY GITHUB\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"GOOGLE\", case = False, na = False),\"EMPLOYER_NAME\"]=\"GOOGLE\"\n",
    "df.loc[(df[\"EMPLOYER_NAME\"].str.contains(\"amazon |amazon.com\", case = False, na = False))&(~df[\"EMPLOYER_NAME\"].str.contains(\"amazon conservation team|amazon stones\", case = False, na = False)), \"EMPLOYER_NAME\"] = \"AMAZON\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"MICROSOFT\", case = False, na = False), \"EMPLOYER_NAME\"] = \"MICROSOFT\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"Cognizant\", case = False, na = False), \"EMPLOYER_NAME\"] = \"COGNIZANT\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"intel corporation|intel mobile|intel massachusetts|intel america|intel federal\", case = False, na = False), \"EMPLOYER_NAME\"] = \"INTEL\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"apple inc.|apple computer|apple educational\", case = False, na = False), \"EMPLOYER_NAME\"] = \"APPLE\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"infosys ltd.|infosys technologies|infosys public|infosys solutions|infosys consulting|infosys international|infosys BPO\", case = False, na = False), \"EMPLOYER_NAME\"] = \"INFOSYS\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"facebook\", case = False, na = False), \"EMPLOYER_NAME\"] = \"FACEBOOK\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"tata consultancy|tata technologies|tata communications|tata industries|tata consltancy|tata enterprises|tata america international|tata autocomp|tata international metals|tata chemicals|tata elxsi\", case = False, na = False), \"EMPLOYER_NAME\"] = \"TATA\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"oracle america|oracle usa|oracle financial\", case = False, na = False), \"EMPLOYER_NAME\"] = \"FACEBOOK\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"hcl america\", case = False, na = False), \"EMPLOYER_NAME\"] = \"HCL AMERICA\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"deloitte\", case = False, na = False), \"EMPLOYER_NAME\"] = \"DELOITTE\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"qualcomm\", case = False, na = False), \"EMPLOYER_NAME\"] = \"QUALCOMM\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"IBM \", case = False, na = False), \"EMPLOYER_NAME\"] = \"IBM\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"Cisco systems|cisco consumer\", case = False, na = False), \"EMPLOYER_NAME\"] = \"CISCO\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"salesforce\", case = False, na = False), \"EMPLOYER_NAME\"] = \"SALESFORCE\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"morgan stanley\", case = False, na = False), \"EMPLOYER_NAME\"] = \"MORGAN STANLEY\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"JP Morgan\", case = False, na = False), \"EMPLOYER_NAME\"] = \"JP MORGAN\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"goldman sachs|goldman, sachs|goldman,sachs\", case = False, na = False), \"EMPLOYER_NAME\"] = \"GOLDMAN SACHS\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"MERRILL LYNCH\", case = False, na = False), \"EMPLOYER_NAME\"] = \"MERRILL LYNCH\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"Citibank|citigroup\", case = False, na = False), \"EMPLOYER_NAME\"] = \"CITI GROUP\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"CREDIT SUISSE\", case = False, na = False), \"EMPLOYER_NAME\"] = \"CREDIT SUISSE\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"pricewater|pwc\", case = False, na = False), \"EMPLOYER_NAME\"] = \"PRICEWATERHOUSECOOPERS\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"ernst  |ernst & Young\", case = False, na = False), \"EMPLOYER_NAME\"] = \"ERNST YOUNG\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"capgemini\", case = False, na = False), \"EMPLOYER_NAME\"] = \"CAPGEMINI\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"technip usa\", case = False, na = False), \"EMPLOYER_NAME\"] = \"TECHNIP USA\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"bechtel \", case = False, na = False), \"EMPLOYER_NAME\"] = \"BECHTEL\"\n",
    "df.loc[(df[\"EMPLOYER_NAME\"].str.contains(\"hp enterprise services|hp inc.|Hewlett-|Hewlett P|HewlettP\", case = False, na = False))&(~df[\"EMPLOYER_NAME\"].str.contains(\"queen nails hp inc|backer ehp inc\", case = False, na = False)), \"EMPLOYER_NAME\"] = \"AMAZON\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"CGGVERITAS\", case = False, na = False), \"EMPLOYER_NAME\"] = \"CGGVERITAS\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"national oilwell\", case = False, na = False), \"EMPLOYER_NAME\"] = \"NATIONAL OILWELL VARCO\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"IH SERVICES\", case = False, na = False), \"EMPLOYER_NAME\"] = \"IH SERVICES\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"paypal\", case = False, na = False), \"EMPLOYER_NAME\"] = \"PAYPAL\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"EBAY INC.|EBAY INC|EBAY ENTERPRISE|EBAY MOBILE LABS| EBAY PAYMENTS\", case = False, na = False), \"EMPLOYER_NAME\"] = \"EBAY\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"brocade\", case = False, na = False), \"EMPLOYER_NAME\"] = \"BROCADE\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"ADOBE INC.|ADOBE SYSTEMS\", case = False, na = False), \"EMPLOYER_NAME\"] = \"ADOBE\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"University of Chicago\", case = False, na = False), \"EMPLOYER_NAME\"] = \"THE UNIVERSITY OF CHICAGO\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"mckinsey\", case = False, na = False), \"EMPLOYER_NAME\"] = \"MCKINSEY COMPANY\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"motorola\", case = False, na = False), \"EMPLOYER_NAME\"] = \"MOTOROLA\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"citadel LLC|citadel americas|citadel Securities|citadel investment|citadel enterprise\", case = False, na = False), \"EMPLOYER_NAME\"] = \"CITADEL LLC\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"yahoo\", case = False, na = False), \"EMPLOYER_NAME\"] = \"YAHOO\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"Juniper networks\", case = False, na = False), \"EMPLOYER_NAME\"] = \"JUNIPER NETWORKS\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"a2z development\", case = False, na = False), \"EMPLOYER_NAME\"] = \"A2Z DEVELOPMENT\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"walmart|wal-mart\", case = False, na = False), \"EMPLOYER_NAME\"] = \"WALMART\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"fortinet\", case = False, na = False), \"EMPLOYER_NAME\"] = \"FORTINET\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"larsen &|larsen  \", case = False, na = False), \"EMPLOYER_NAME\"] = \"LARSEN TOUBRO\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"mindtree\", case = False, na = False), \"EMPLOYER_NAME\"] = \"MINDTREE LIMITED\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"igate tech|igate mastech|igate global\", case = False, na = False), \"EMPLOYER_NAME\"] = \"IGATE TECHNOLOGIES\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"3i info\", case = False, na = False), \"EMPLOYER_NAME\"] = \"3I INFOTECH\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"NTT Data\", case = False, na = False), \"EMPLOYER_NAME\"] = \"NTT DATA\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"Texas instrument\", case = False, na = False), \"EMPLOYER_NAME\"] = \"TEXAS INSTRUMENTS\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"tech mahindra\", case = False, na = False), \"EMPLOYER_NAME\"] = \"TECH MAHINDRA\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"DELL USA LP|DELL PRODUCTS LP|DELL MARKETING LP|DELL SOFTWARE INC|DELL FINANCIAL\", case = False, na = False), \"EMPLOYER_NAME\"] = \"DELL\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"uber technologies\", case = False, na = False), \"EMPLOYER_NAME\"] = \"UBER TECHNOLOGIES\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"case farms\", case = False, na = False), \"EMPLOYER_NAME\"] = \"CASE FARMS\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"house of raeford\", case = False, na = False), \"EMPLOYER_NAME\"] = \"HOUSE OF RAEFORD FARMS\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"george's processing\", case = False, na = False), \"EMPLOYER_NAME\"] = \"GEORGE'S PROCESSING\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"abbyland\", case = False, na = False), \"EMPLOYER_NAME\"] = \"ABBYLAND FOODS\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"koch foods of cinc\", case = False, na = False), \"EMPLOYER_NAME\"] = \"KOCH FOODS OF CINCINNATI\"\n",
    "df.loc[df[\"EMPLOYER_NAME\"].str.contains(\"koch foods of al\", case = False, na = False), \"EMPLOYER_NAME\"] = \"KOCH FOODS OF ALABAMA\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c570f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING THE LIST OF COLUMNS THAT NEED CLEANING UP: \"Y\" OR \"N\" INTO 1 or 0\n",
    "Y_N_cols = [\"REFILE\",\n",
    "\"REQUIRED_TRAINING\",\n",
    "\"REQUIRED_EXPERIENCE\",\n",
    "\"ACCEPT_FOREIGN_EDUCATION\",\n",
    "\"ACCEPT_ALT_OCCUPATION\",\n",
    "\"JOB_OPP_REQUIREMENTS_NORMAL\",\n",
    "\"FOREIGN_LANGUAGE_REQUIRED\",\n",
    "\"PROFESSIONAL_OCCUPATION\",\n",
    "\"APP_FOR_COLLEGE_U_TEACHER\",\n",
    "\"SUNDAY_EDITION_NEWSPAPER\",\n",
    "\"SECOND_ADVERTISEMENT\",\n",
    "\"JOB_FAIR\",\n",
    "\"ON_CAMPUS_RECRUITING\",\n",
    "\"EMPLOYER_WEBSITE\",\n",
    "\"PRO_ORG_AD\",\n",
    "\"JOB_SEARCH_WEBSITE\",\n",
    "\"PVT_EMPLOYMENT_FIRM\",\n",
    "\"EMPLOYEE_REF_PROG\",\n",
    "\"CAMPUS_PLACEMENT\",\n",
    "\"LOCAL_ETHNIC_PAPER\",\n",
    "\"RADIO_TV_AD\",\n",
    "\"EMP_RECEIVED_PAYMENT\",\n",
    "\"BARGAINING_REP_NOTIFIED\",\n",
    "\"POSTED_NOTICE_AT_WORKSITE\",\n",
    "\"LAYOFF_IN_PAST_SIX_MONTHS\",\n",
    "\"US_WORKERS_CONSIDERED\",\n",
    "\"FOREIGN_WORKER_TRAINING_COMP\",\n",
    "\"FOREIGN_WORKER_REQ_EXPERIENCE\",\n",
    "\"FOREIGN_WORKER_ALT_ED_EXP\",\n",
    "\"FOREIGN_WORKER_ALT_OCC_EXP\",\n",
    "\"EMPLOYER_COMPLETED_APPLICATION\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9607e14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CHECKING WHAT THE MAX UNIQUE VALUES ARE FOR THE FIELDS WITH \"Y\" AND \"N\": THIS SHOULD BE NO MORE THAN 4 (Y,N,NA,Blank)\n",
    "exlist = []\n",
    "edf = pd.DataFrame()\n",
    "for i in range(len(Y_N_cols)):\n",
    "    j = df[Y_N_cols[i]].nunique()\n",
    "    x = pd.DataFrame([Y_N_cols[i],j]).transpose()\n",
    "    edf = pd.concat([edf,x])\n",
    "edf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fead012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOOKING INTO ACCEPT_ALT_OCCUPATION\n",
    "df[\"ACCEPT_ALT_OCCUPATION\"].value_counts()\n",
    "# LOOKS LIKE SOME APPLICANTS HAVE Y AND N WHILE OTHERS WRITE THE ACCEPTED OCCUPATION..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2e4306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING A LIST TO RUN A LOOP AND FIND ALL THE UNIQUES FOR EACH COLUMN, EXCEPT FOR ALT_OCCUPATION\n",
    "new_Y_N_cols = [\"REFILE\",\n",
    "\"REQUIRED_TRAINING\",\n",
    "\"REQUIRED_EXPERIENCE\",\n",
    "\"ACCEPT_FOREIGN_EDUCATION\",\n",
    "\"JOB_OPP_REQUIREMENTS_NORMAL\",\n",
    "\"FOREIGN_LANGUAGE_REQUIRED\",\n",
    "\"PROFESSIONAL_OCCUPATION\",\n",
    "\"APP_FOR_COLLEGE_U_TEACHER\",\n",
    "\"SUNDAY_EDITION_NEWSPAPER\",\n",
    "\"SECOND_ADVERTISEMENT\",\n",
    "\"JOB_FAIR\",\n",
    "\"ON_CAMPUS_RECRUITING\",\n",
    "\"EMPLOYER_WEBSITE\",\n",
    "\"PRO_ORG_AD\",\n",
    "\"JOB_SEARCH_WEBSITE\",\n",
    "\"PVT_EMPLOYMENT_FIRM\",\n",
    "\"EMPLOYEE_REF_PROG\",\n",
    "\"CAMPUS_PLACEMENT\",\n",
    "\"LOCAL_ETHNIC_PAPER\",\n",
    "\"RADIO_TV_AD\",\n",
    "\"EMP_RECEIVED_PAYMENT\",\n",
    "\"BARGAINING_REP_NOTIFIED\",\n",
    "\"POSTED_NOTICE_AT_WORKSITE\",\n",
    "\"LAYOFF_IN_PAST_SIX_MONTHS\",\n",
    "\"US_WORKERS_CONSIDERED\",\n",
    "\"FOREIGN_WORKER_TRAINING_COMP\",\n",
    "\"FOREIGN_WORKER_REQ_EXPERIENCE\",\n",
    "\"FOREIGN_WORKER_ALT_ED_EXP\",\n",
    "\"FOREIGN_WORKER_ALT_OCC_EXP\",\n",
    "\"EMPLOYER_COMPLETED_APPLICATION\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490a39be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CHECKING THE UNIQUE VALUES ARE FOR THE FIELDS WITH \"Y\" AND \"N\": THIS SHOULD BE NO MORE THAN 4 (Y,N,NA,Blank)\n",
    "exlist = []\n",
    "edf = pd.DataFrame()\n",
    "for i in range(len(new_Y_N_cols)):\n",
    "    j = df[new_Y_N_cols[i]].nunique()\n",
    "    a = df[new_Y_N_cols[i]].unique()\n",
    "    x = pd.DataFrame([new_Y_N_cols[i],j,a]).transpose()\n",
    "    edf = pd.concat([edf,x])\n",
    "edf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a66170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGING \"Y\" TO 1, N to 0, A to 0, and \"\" to 0\n",
    "for i in range(len(new_Y_N_cols)):\n",
    "    df[new_Y_N_cols[i]] = np.where(df[new_Y_N_cols[i]]==\"Y\", 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0004fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGING ALT OCCUPATION Y to 1, N to 0, \"\" to 0, anything else will be a 1\n",
    "df[\"ACCEPT_ALT_OCCUPATION\"] = np.where(df[\"ACCEPT_ALT_OCCUPATION\"]==\"Y\", 1, df[\"ACCEPT_ALT_OCCUPATION\"])\n",
    "df[\"ACCEPT_ALT_OCCUPATION\"] = np.where((df[\"ACCEPT_ALT_OCCUPATION\"]==\"N\")|(df[\"ACCEPT_ALT_OCCUPATION\"]==\"\"), 0, df[\"ACCEPT_ALT_OCCUPATION\"])\n",
    "df[\"ACCEPT_ALT_OCCUPATION\"] = np.where(df[\"ACCEPT_ALT_OCCUPATION\"]!=0, 1, df[\"ACCEPT_ALT_OCCUPATION\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195c42c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROP THE COLUMNS NOT IN USE\n",
    "df = df.drop([\"ID\",\"CASE_NUMBER\",\"DECISION_DATE\",\"RECEIVED_DATE\"], axis = 1).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61066619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TURN ALL \"\" TO NP.NAN\n",
    "df = df.replace(\"\",np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bbdc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a551f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## EDA ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df2d0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EMPLOYER RELATED DATA ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf21b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TURNING EMPLOYER EMPLOYEE NUMBER, \"\" INTO 0, THIS WILL ALLOW FOR EASIER CLEANING\n",
    "df[\"EMPLOYER_NUM_EMPLOYEES\"] = df[\"EMPLOYER_NUM_EMPLOYEES\"].replace(np.nan,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229a6354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEANING UP APPLICANT DATA BASED ON FREQUENCY OF EMPLOYER SPONSORSHIP...\n",
    "# IF AN EMPLOYER ONLY HAS 1 APPLICANT EVER, PROBABLY WILL NOT SPONSOR AGAIN - COMPARED TO COMPANIES THAT SPONSOR FREQUENTLY\n",
    "# HOW MANY EMPLOYERS HAVE SPONSORED ONLY ONCE? NOT COMPLETELY ACCURATE DUE TO LACK OF STANDARDIZATION RULES\n",
    "print(\"There are around\",(df[\"EMPLOYER_NAME\"].value_counts()==1).sum(),\"companies that have only sponsored one applicant.\")\n",
    "print(\"There are a total of\",len(df[\"EMPLOYER_NAME\"].value_counts()),\"unique companies in the entire dataset.\")\n",
    "print(\"Dropping companies with only 1 applicant would be\",round(((df[\"EMPLOYER_NAME\"].value_counts()==1).sum()/len(df[\"EMPLOYER_NAME\"].value_counts()))*100,2),\"% of all companies.\")\n",
    "print(\"There would be\",df[\"EMPLOYER_NAME\"].value_counts()[df[\"EMPLOYER_NAME\"].value_counts()>1].sum(),\"applicants left out of\",len(df),\"in the entire dataset, or dropping\",round((len(df)-df[\"EMPLOYER_NAME\"].value_counts()[df[\"EMPLOYER_NAME\"].value_counts()>1].sum())/len(df)*100,2),\"%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107234e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEFORE WE DROP, WHATS THE SUCCESS % OF THESE APPLICANTS?\n",
    "# CREATE A DF WITH ONLY APPLICANTS TO COMPANIES THAT HAVE SPONSORED ONLY 1 APPLICANT\n",
    "onlyone = df[\"EMPLOYER_NAME\"].value_counts()[df[\"EMPLOYER_NAME\"].value_counts()<=1].index.tolist()\n",
    "onlyonedf = df[df[\"EMPLOYER_NAME\"].isin(onlyone)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa162fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUCCESS RATE OF APPLICANTS SPONSORED BY COMPANIES THAT HAVE ONLY SPONSORED ONE APPLICANT\n",
    "# COMPARE THIS TO THE REST OF THE DATA SET\n",
    "print(\"Applicants sponsored by companies that have only sponsored one applicant have a,\",round((onlyonedf[\"CASE_STATUS\"].sum())/len(onlyonedf)*100,2),\"% success rate.\")\n",
    "rest = df[\"EMPLOYER_NAME\"].value_counts()[df[\"EMPLOYER_NAME\"].value_counts()>1].index.tolist()\n",
    "df = df[df[\"EMPLOYER_NAME\"].isin(rest)]\n",
    "print(\"The rest of the applicants have a,\",round((df[\"CASE_STATUS\"].sum())/len(df)*100,2),\"% success rate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26d1f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEANING UP APPLICANT DATA BASED ON EMPLOYEE NUM FIELD\n",
    "# IF AN EMPLOYER HAS 0 or DID NOT FILL OUT THE EMPLOYEE NUM FIELD, WE WILL DROP: INACCURATE DATA\n",
    "# HOW MANY EMPLOYERS HAVE 0 EMPLOYEES?\n",
    "print(\"There are around\",(df[\"EMPLOYER_NUM_EMPLOYEES\"]==0).sum(),\"applications from companies that have no employees or did not fill out the field.\")\n",
    "print(\"Dropping applicants with 0 employees or who did not fill out would be\",round((df[\"EMPLOYER_NUM_EMPLOYEES\"]==0).sum()/len(df)*100,2),\"% of all applications.\")\n",
    "print(\"There would be\",len(df[df[\"EMPLOYER_NUM_EMPLOYEES\"]>0]),\"applications left out of\",len(df),\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdd9d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEFORE WE DROP, WHATS THE SUCCESS % OF THESE APPLICANTS?\n",
    "# CREATE A DF WITH ONLY APPLICANTS TO COMPANIES THAT HAVE 0 EMPLOYEES\n",
    "none = df[df[\"EMPLOYER_NUM_EMPLOYEES\"]==0]\n",
    "# SUCCESS RATE OF APPLICANTS SPONSORED BY COMPANIES HAVE 0 EMPLOYEES / DID NOT FILL OUT THE FIELD\n",
    "# COMPARE THIS TO THE REST OF THE DATA SET\n",
    "print(\"Applicants sponsored by companies with 0 employees have a,\",round((none[\"CASE_STATUS\"].sum())/len(none)*100,2),\"% success rate.\")\n",
    "df = df[df[\"EMPLOYER_NUM_EMPLOYEES\"]>0]\n",
    "print(\"The rest of the applicants have a,\",round((df[\"CASE_STATUS\"].sum())/len(df)*100,2),\"% success rate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb8fb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEANING UP APPLICANT DATA BASED ON EMPLOYER YR COMMENCED BUSINESS\n",
    "# IF AN EMPLOYER COMMENCED BUSINESS IN YEAR 1600, DROP\n",
    "print(\"There are around\",(df[\"EMPLOYER_YEAR_COMMENCED_BUSINESS\"]<=1600).sum(),\"applications from companies that operated before the year 1600.\")\n",
    "print(\"Dropping applicants from companies operating before 1600 would be\",round((df[\"EMPLOYER_YEAR_COMMENCED_BUSINESS\"]<=1600).sum()/len(df)*100,2),\"% of all applications.\")\n",
    "print(\"There would be\",len(df[df[\"EMPLOYER_YEAR_COMMENCED_BUSINESS\"]>1600]),\"applications left out of\",len(df),\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60de60ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEFORE WE DROP, WHATS THE SUCCESS % OF THESE APPLICANTS?\n",
    "# CREATE A DF WITH ONLY APPLICANTS TO COMPANIES THAT WERE OPERATING BEFORE 1600\n",
    "none = df[df[\"EMPLOYER_YEAR_COMMENCED_BUSINESS\"]<=1600]\n",
    "# SUCCESS RATE OF APPLICANTS SPONSORED BY COMPANIES OPERATING BEFORE 1600\n",
    "# COMPARE THIS TO THE REST OF THE DATA SET\n",
    "print(\"Applicants sponsored by companies operating before 1600,\",round((none[\"CASE_STATUS\"].sum())/len(none)*100,2),\"% success rate.\")\n",
    "df = df[df[\"EMPLOYER_YEAR_COMMENCED_BUSINESS\"]>1600]\n",
    "print(\"The rest of the applicants have a,\",round((df[\"CASE_STATUS\"].sum())/len(df)*100,2),\"% success rate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b07adf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 15 Employers of all time 2014-2021\n",
    "allemp = pd.DataFrame(df[\"EMPLOYER_NAME\"].value_counts()[:15]).reset_index()\n",
    "allemp.columns = [\"AEMP\",\"COUNT AEMP\"]\n",
    "sucemp = pd.DataFrame(df[df[\"CASE_STATUS\"]==1][\"EMPLOYER_NAME\"].value_counts()[:15]).reset_index()\n",
    "sucemp.columns = [\"SEMP\",\"COUNT SEMP\"]\n",
    "pd.concat([allemp,sucemp], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9ab7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing the top 15 for each year employers: only succesful applications\n",
    "top15semp = pd.DataFrame()\n",
    "def grab (year):\n",
    "    return df[(df[\"YEAR\"]==year)&(df[\"CASE_STATUS\"]==1)][\"EMPLOYER_NAME\"].value_counts()[:15]\n",
    "for x in range(len(yr)):\n",
    "    j = pd.DataFrame(grab(yr[x]))\n",
    "    top15semp = pd.concat([top15semp,j], axis = 1)\n",
    "top15semp.columns= yr\n",
    "# King of Consistency: HOW MANY YEARS IS THIS COMPANY NOT IN THE TOP 15?: TOP 10 CONSISTENT COMPANIES...\n",
    "kingcons = top15semp.isnull().replace(False,0).replace(True,1)\n",
    "kingcons[\"Yrs not in Top 15\"] = top15semp.isnull().replace(False,0).replace(True,1).sum(axis = 1)\n",
    "kingcons[\"Yrs not in Top 15\"].sort_values()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129aff06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing the top 15 for each year employers: total applications\n",
    "top15aemp = pd.DataFrame()\n",
    "def grab (year):\n",
    "    return df[df[\"YEAR\"]==year][\"EMPLOYER_NAME\"].value_counts()[:15]\n",
    "for x in range(len(yr)):\n",
    "    j = pd.DataFrame(grab(yr[x]))\n",
    "    top15aemp = pd.concat([top15aemp,j], axis = 1)\n",
    "top15aemp.columns= yr\n",
    "# King of Consistency: HOW MANY YEARS IS THIS COMPANY NOT IN THE TOP 15?: TOP 10 CONSISTENT COMPANIES...\n",
    "kingconsA = top15aemp.isnull().replace(False,0).replace(True,1)\n",
    "kingconsA[\"Yrs not in Top 15\"] = top15aemp.isnull().replace(False,0).replace(True,1).sum(axis = 1)\n",
    "kingconsA[\"Yrs not in Top 15\"].sort_values()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56576012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW COLUMN: 1 is in the top 15 of the YEAR, 0 is not: Applications\n",
    "filler = pd.DataFrame()\n",
    "for x in range(len(yr)):\n",
    "    listy = top15aemp[yr[x]].dropna().index.tolist()\n",
    "    filler = pd.concat([filler,pd.DataFrame((df[df[\"YEAR\"]==yr[x]][\"EMPLOYER_NAME\"].isin(listy)).replace(True,1).replace(False,0))])\n",
    "\n",
    "filler = filler.reset_index().sort_values(by = \"index\").set_index(\"index\")\n",
    "filler.columns = [\"TOP15COMPANY\"]\n",
    "df[\"TOP15COMPANY\"] = filler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d723a154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRELATION ANALYSIS: NUMBER OF EMPLOYEEs WITH CASE STATUS: CASE BY CASE\n",
    "st.linregress(x = df.dropna()[\"TOP15COMPANY\"], y = df.dropna()[\"CASE_STATUS\"])\n",
    "# DOES NOT SHOW MUCH, AGGREGATE MAY SHOW A DIFFERENT STORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa74ef64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING THE DIFFERENT SUCCESS RATES OVER TIME BY TOP 15 COMPANY (IN TERMS OF ANNUAL APPLICATIONS)\n",
    "\n",
    "top15 = df[df[\"TOP15COMPANY\"]==1]\n",
    "not15 = df[df[\"TOP15COMPANY\"]==0]\n",
    "\n",
    "topdf = pd.DataFrame()\n",
    "notdf = pd.DataFrame()\n",
    "analysisdf = pd.DataFrame()\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    j = pd.DataFrame([len(top15[(top15[\"YEAR\"]==yr[x])&(top15[\"CASE_STATUS\"]==1)])/len(top15[top15[\"YEAR\"]==yr[x]])])\n",
    "    b = pd.DataFrame([len(not15[(not15[\"YEAR\"]==yr[x])&(not15[\"CASE_STATUS\"]==1)])/len(not15[not15[\"YEAR\"]==yr[x]])])\n",
    "    topdf = pd.concat([topdf,j])\n",
    "    notdf = pd.concat([notdf,b])\n",
    "    analysisdf = pd.concat([topdf,notdf], axis = 1)\n",
    "\n",
    "analysisdf.index = yr\n",
    "analysisdf.columns = [\"Top 15 Success %\", \"Not 15 Success %\"]\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "analysisdf.plot(kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Success Rates by Companies in the Top 15 and not in the Top 15, Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463402c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARING DATA FOR LIN REGRESS AND GETTING P VALUE\n",
    "top15apps = []\n",
    "not15apps = []\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    t = (top15[\"YEAR\"]==yr[x]).sum()\n",
    "    n = (not15[\"YEAR\"]==yr[x]).sum()\n",
    "    top15apps.append(t)\n",
    "    not15apps.append(n)\n",
    "analysisdf = analysisdf.reset_index()\n",
    "analysisdf[\"Top 15 Apps\"] = pd.DataFrame(top15apps)\n",
    "analysisdf[\"Not 15 Apps\"] = pd.DataFrame(not15apps)\n",
    "analysisdf = analysisdf.set_index(\"index\")\n",
    "# PLOTTING APPLICATION RATES OVER TIME BY TOP 15 or NOT TOP 15\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "analysisdf.reset_index().plot(x = \"index\", y = [\"Top 15 Apps\",\"Not 15 Apps\"], kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Application Rates by Companies in the Top 15 and not in the Top 15, Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e906cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P VALUE PER YEAR: ON THE SUCCESS RATE\n",
    "top_not = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = analysisdf.loc[yr[x]][\"Top 15 Success %\"]\n",
    "    sample2_phat = analysisdf.loc[yr[x]][\"Not 15 Success %\"]\n",
    "    sample1_size = analysisdf.loc[yr[x]][\"Top 15 Apps\"]\n",
    "    sample2_size = analysisdf.loc[yr[x]][\"Not 15 Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    top_not.append(pval)\n",
    "\n",
    "pvaldf = pd.DataFrame(top_not)\n",
    "pvaldf.columns = [\"Top 15 Vs. Not Top 15\"]\n",
    "pvaldf.index = yr\n",
    "analysisdf[\"PVAL Success %\"] = pvaldf[\"Top 15 Vs. Not Top 15\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195aaf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION BETWEEN COMPANY SIZE AND SUCCESS RATE OVER TIME: TOP 15 COMPANY\n",
    "print(\"Top 15 Company Success Rate Over Time Slope:\",round(st.linregress(y = analysisdf[\"Top 15 Success %\"], x = yr).slope,6),\", P Val:\",round(st.linregress(y = analysisdf[\"Top 15 Success %\"], x = yr).pvalue,4),\", R Val:\",round(st.linregress(y = analysisdf[\"Top 15 Success %\"], x = yr).rvalue,4),\"\\nModerate growth, .68% growth in success rate a year, but not a statistically significant slope.\")\n",
    "# LINEAR REGRESSION BETWEEN COMPANY SIZE AND APPLICATIONS OVER TIME: TOP 15 COMPANY\n",
    "print(\"\\nTop 15 Company Application Rate Over Time Slope:\",round(st.linregress(y = analysisdf[\"Top 15 Apps\"], x = yr).slope,3),\", P Val:\",round(st.linregress(y = analysisdf[\"Top 15 Apps\"], x = yr).pvalue,4),\", R Val:\",round(st.linregress(y = analysisdf[\"Top 15 Apps\"], x = yr).rvalue,4),\"\\nModerate growth, 1379 applications a year, but not a statistically significant slope.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f1a8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION BETWEEN COMPANY SIZE AND SUCCESS RATE OVER TIME: NOT TOP 15 COMPANY\n",
    "print(\"Not Top 15 Company Success Rate Over Time Slope:\",round(st.linregress(y = analysisdf[\"Not 15 Success %\"], x = yr).slope,6),\", P Val:\",round(st.linregress(y = analysisdf[\"Not 15 Success %\"], x = yr).pvalue,4),\", R Val:\",round(st.linregress(y = analysisdf[\"Not 15 Success %\"], x = yr).rvalue,4),\"\\nStrong growth, .54% growth in success rate a year, and a statistically significant slope.\")\n",
    "# LINEAR REGRESSION BETWEEN COMPANY SIZE AND APPLICATIONS OVER TIME: NOT TOP 15 COMPANY\n",
    "print(\"\\nNot Top 15 Company Application Rate Over Time Slope:\",round(st.linregress(y = analysisdf[\"Not 15 Apps\"], x = yr).slope,3),\", P Val:\",round(st.linregress(y = analysisdf[\"Not 15 Apps\"], x = yr).pvalue,4),\", R Val:\",round(st.linregress(y = analysisdf[\"Not 15 Apps\"], x = yr).rvalue,4),\"\\nWeak growth, 3516 applications a year, but not a statistically significant slope.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0041bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING THE TOTAL PVAL\n",
    "analysisdf.loc[\"ALL YRS\"] = analysisdf.sum()\n",
    "top15suc = []\n",
    "not15suc = []\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    t15 = analysisdf.loc[yr[x]].iloc[0]*analysisdf.loc[yr[x]].iloc[2]\n",
    "    n15 = analysisdf.loc[yr[x]].iloc[1]*analysisdf.loc[yr[x]].iloc[3]\n",
    "    top15suc.append(t15)\n",
    "    not15suc.append(n15)\n",
    "analysisdf.loc[\"ALL YRS\"].iloc[0] = sum(top15suc)/analysisdf.loc[\"ALL YRS\"].iloc[2]\n",
    "analysisdf.loc[\"ALL YRS\"].iloc[1] = sum(not15suc)/analysisdf.loc[\"ALL YRS\"].iloc[3]\n",
    "\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = analysisdf.loc[\"ALL YRS\"].iloc[0]\n",
    "    sample2_phat = analysisdf.loc[\"ALL YRS\"].iloc[1]\n",
    "    sample1_size = analysisdf.loc[\"ALL YRS\"].iloc[2]\n",
    "    sample2_size = analysisdf.loc[\"ALL YRS\"].iloc[3]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "analysisdf.loc[\"ALL YRS\"].iloc[4] = pval\n",
    "\n",
    "analysisdf = analysisdf.head(9).style.format(\"{:,.2f}\")\n",
    "analysisdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cb87bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A NEW COLUMN: YRS EDUCATION - DECISION YEAR, HOW MANY YEARS AFTER GRADUATION: POSTGRADYRS\n",
    "# THIS IS FOR A DEEPER DIVE INTO THE TOP EMPLOYERS... \n",
    "df[\"YRSPOSTGRAD\"] = df[\"YEAR\"]-df[\"FOREIGN_WORKER_YRS_ED_COMP\"]\n",
    "# CLEANING UP WORKER EDUCATION AND EDUCATION REQUIREMENTS\n",
    "df[\"FOREIGN_WORKER_EDUCATION\"]= df[\"FOREIGN_WORKER_EDUCATION\"].replace(\"NONE\",np.nan)\n",
    "df[\"MINIMUM_EDUCATION\"] = df[\"MINIMUM_EDUCATION\"].replace(\"NONE\",np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fb5c84",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# DIVING DEEPER INTO MICROSOFT, AMAZON, GOOGLE, APPLE (COMPANY VARIABLE CAN BE CHANGED FOR ANY COMPANY)\n",
    "#ANCHORED BY STATE: PW SKILL,APPLICATIONS, MEDIAN WAGE, YRS POST GRADE, MEDIAN DAYS\n",
    "\n",
    "company = \"MICROSOFT\"\n",
    "# CHANGE THIS FOR ANY COMPANY\n",
    "\n",
    "# MEDIAN WAGE, MEDIAN YEARS AFTER GRADUATING, MEDIAN DAYS TO DECIDE : FOR EACH STATE IN THE TOP 15 OF THE COMPANY AND BY PW LEVEL\n",
    "topstate = df[df[\"EMPLOYER_NAME\"] == company][\"WORKSITE_STATE\"].value_counts()[:15].index.tolist()\n",
    "pwskill = df[\"PW_SKILL_LEVEL\"].dropna().unique().tolist()\n",
    "pwskill.sort()\n",
    "\n",
    "\n",
    "statelist = []\n",
    "pwlist = pwskill*len(topstate)\n",
    "apps = []\n",
    "medwage = []\n",
    "yrsgrad = []\n",
    "meddays = []\n",
    "\n",
    "\n",
    "for x in range(len(topstate)):\n",
    "    s = (topstate[x]+\",\")*len(pwskill)\n",
    "    s = \"\".join(s).split(\",\")\n",
    "    s = list(filter((\"\").__ne__, s))\n",
    "    statelist.append(s) \n",
    "for x in range(len(topstate)):\n",
    "    for j in range(len(pwskill)):\n",
    "        a = len(df[(df[\"EMPLOYER_NAME\"] == company)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"EMPLOYER_NAME\"] == company)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]])\n",
    "        apps.append(a)\n",
    "        b = df[(df[\"EMPLOYER_NAME\"] == company)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"EMPLOYER_NAME\"] == company)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]][\"WO_A\"].median()\n",
    "        medwage.append(b)\n",
    "        c = df[(df[\"EMPLOYER_NAME\"] == company)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"EMPLOYER_NAME\"] == company)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]][\"YRSPOSTGRAD\"].median()\n",
    "        yrsgrad.append(c)\n",
    "        d = df[(df[\"EMPLOYER_NAME\"] == company)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"EMPLOYER_NAME\"] == company)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]][\"DAYS_TO_DECIDE\"].median()\n",
    "        meddays.append(d)\n",
    "empstatedf = pd.DataFrame([sum(statelist, []),pwlist,apps,medwage,yrsgrad,meddays]).transpose()\n",
    "empstatedf.columns = [\"STATE\",\"PW LEVEL\", \"TOTAL APPLICATIONS\",\"MEDIAN WAGE\",\"MEDIAN YRS POST GRAD\",\"MEDIAN DAYS TO DECIDE\"]\n",
    "empstatedf = empstatedf.set_index([\"STATE\",\"PW LEVEL\"])\n",
    "empstatedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5109f715",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ANCHORED BY STATE: EDU LEVEL,APPLICATIONS, MEDIAN WAGE, YRS POST GRADE, MEDIAN DAYS\n",
    "# MEDIAN WAGE, MEDIAN YEARS AFTER GRADUATING, MEDIAN DAYS TO DECIDE : FOR EACH STATE IN THE TOP 15 OF THE COMPANY AND BY EDU LEVEL\n",
    "edu = df[\"FOREIGN_WORKER_EDUCATION\"].dropna().unique().tolist()\n",
    "edu[0],edu[1],edu[2],edu[3],edu[4],edu[5] = edu[3],edu[5],edu[0],edu[2],edu[1],edu[4]\n",
    "\n",
    "statelist = []\n",
    "edulist = edu*len(topstate)\n",
    "apps = []\n",
    "medwage = []\n",
    "yrsgrad = []\n",
    "meddays = []\n",
    "\n",
    "\n",
    "for x in range(len(topstate)):\n",
    "    s = (topstate[x]+\",\")*len(edu)\n",
    "    s = \"\".join(s).split(\",\")\n",
    "    s = list(filter((\"\").__ne__, s))\n",
    "    statelist.append(s) \n",
    "for x in range(len(topstate)):\n",
    "    for j in range(len(edu)):\n",
    "        a = len(df[(df[\"EMPLOYER_NAME\"] == company)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"EMPLOYER_NAME\"] == company)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]])\n",
    "        apps.append(a)\n",
    "        b = df[(df[\"EMPLOYER_NAME\"] == company)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"EMPLOYER_NAME\"] == company)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]][\"WO_A\"].median()\n",
    "        medwage.append(b)\n",
    "        c = df[(df[\"EMPLOYER_NAME\"] == company)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"EMPLOYER_NAME\"] == company)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]][\"YRSPOSTGRAD\"].median()\n",
    "        yrsgrad.append(c)\n",
    "        d = df[(df[\"EMPLOYER_NAME\"] == company)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"EMPLOYER_NAME\"] == company)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]][\"DAYS_TO_DECIDE\"].median()\n",
    "        meddays.append(d)\n",
    "empstatedf1 = pd.DataFrame([sum(statelist, []),edulist,apps,medwage,yrsgrad,meddays]).transpose()\n",
    "empstatedf1.columns = [\"STATE\",\"EDU LEVEL\", \"TOTAL APPLICATIONS\",\"MEDIAN WAGE\",\"MEDIAN YRS POST GRAD\",\"MEDIAN DAYS TO DECIDE\"]\n",
    "empstatedf1 = empstatedf1.set_index([\"STATE\",\"EDU LEVEL\"])\n",
    "empstatedf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8278f5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHERE ARE EMPLOYERS HEADQUARTED? \n",
    "# TOP 15 STATES WHERE EMPLOYERS HQ AND SPONSOR GREEN CARDS\n",
    "allHQ = pd.DataFrame(df[\"EMPLOYER_STATE_PROVINCE\"].value_counts()[:15]).reset_index()\n",
    "allHQ.columns = [\"ASTATE\",\"COUNT ASTATE\"]\n",
    "sucHQ = pd.DataFrame(df[df[\"CASE_STATUS\"]==1][\"EMPLOYER_STATE_PROVINCE\"].value_counts()[:15]).reset_index()\n",
    "sucHQ.columns = [\"SSTATE\",\"COUNT SSTATE\"]\n",
    "pd.concat([allHQ,sucHQ], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b188f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHOROPLETH MAP OF THE TOP STATES BY SUCCESS\n",
    "# ACCEPT % BASED ON STATE HQ IN\n",
    "empy = df.groupby(\"EMPLOYER_STATE_PROVINCE\")[\"CASE_STATUS\"].value_counts()\n",
    "empyer = pd.DataFrame(empy.groupby(level=0).transform(lambda x: (x / x.sum()).round(2)))\n",
    "empyer.columns = [\"Percent\"]\n",
    "f1 = empyer.reset_index()[\"EMPLOYER_STATE_PROVINCE\"].tolist()\n",
    "t1 = empyer.reset_index()[\"CASE_STATUS\"].replace(1,\"Accept\").replace(0,\"Reject\").tolist()\n",
    "idx1 = []\n",
    "idx1.extend([tuple(a) for a in zip (f1,t1)])\n",
    "empyer.index = pd.MultiIndex.from_tuples(idx1)\n",
    "empyer = empyer.reset_index()\n",
    "empyer = empyer[empyer[\"level_1\"]==\"Accept\"]\n",
    "empyer = empyer.drop(\"level_1\",axis = 1)\n",
    "\n",
    "empycount = pd.DataFrame(df.groupby(\"EMPLOYER_STATE_PROVINCE\")[\"CASE_STATUS\"].value_counts())\n",
    "empycount.columns = [\"Count\"]\n",
    "empycount = empycount.reset_index()\n",
    "empycount = empycount[empycount[\"CASE_STATUS\"]==1]\n",
    "empycount = empycount[\"Count\"]\n",
    "\n",
    "empyer = pd.concat([empyer, empycount], axis = 1)\n",
    "empyer.columns = [\"State\",\"Percent\",\"Count\"]\n",
    "empyer[\"Count\"] = empyer[\"State\"].astype(str)+\": \"+empyer[\"Count\"].astype(str)+\" Successful Applications\"\n",
    "# Choropleth Map of Employer State Accept %\n",
    "data = dict(type = \"choropleth\",\n",
    "           locations = empyer[\"State\"],\n",
    "           locationmode = \"USA-states\",\n",
    "           z = empyer[\"Percent\"].astype(float),\n",
    "           text = empyer[\"Count\"],\n",
    "           colorbar={\"title\":\"Percent Accepted\"},\n",
    "           colorscale = \"delta\",\n",
    "           marker = dict(line = dict(color = \"rgb(255,255,255)\")) )\n",
    "layout = dict(title = \"Percent Accepted in Employer Headquartered States\",\n",
    "              geo = dict(scope = \"usa\",showlakes = True, lakecolor = \"rgb(255,255,255)\"))\n",
    "choro = go.Figure(data, layout)\n",
    "iplot(choro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc736ab9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# HOW MANY OF THE TOP COMPANIES, IDENTIFIED ABOVE, ARE HQ IN THE TOP 5 STATES\n",
    "companies = kingconsA[\"Yrs not in Top 15\"].sort_values()[:7].index.tolist()\n",
    "states = allHQ[\"ASTATE\"][:5].tolist()\n",
    "apps = allHQ[\"COUNT ASTATE\"][:5].tolist()\n",
    "companystates = pd.DataFrame()\n",
    "for x in range(len(companies)):\n",
    "    for i in range(len(states)):\n",
    "        c = (df[df[\"EMPLOYER_NAME\"]==companies[x]][\"EMPLOYER_STATE_PROVINCE\"]==states[i]).sum()\n",
    "        n = pd.DataFrame([companies[x],states[i],c]).transpose()\n",
    "        companystates = pd.concat([companystates,n])\n",
    "companystates.columns=[\"EMPLOYER\",\"STATE\",\"APPLICATIONS\"]\n",
    "companystates[\"TOTALAPP\"]=np.nan\n",
    "for x in range(len(states)):\n",
    "    companystates[\"TOTALAPP\"] = np.where(companystates[\"STATE\"]==states[x],apps[x],companystates[\"TOTALAPP\"])\n",
    "companystates = companystates.set_index([\"EMPLOYER\",\"STATE\"])\n",
    "companystates[\"% OF TOTAL APPLICATIONS\"] = (companystates[\"APPLICATIONS\"]/companystates[\"TOTALAPP\"])*100\n",
    "companystates = companystates.drop(\"TOTALAPP\", axis = 1)\n",
    "companystates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f257f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHERE ARE EMPLOYERS HEADQUARTED? \n",
    "# TOP REGIONS WHERE EMPLOYERS HQ AND SPONSOR GREEN CARDS\n",
    "allRHQ = pd.DataFrame(df[\"EMPLOYER_REGION\"].value_counts()[:15]).reset_index()\n",
    "allRHQ.columns = [\"AREGION\",\"COUNT AREGION\"]\n",
    "sucRHQ = pd.DataFrame(df[df[\"CASE_STATUS\"]==1][\"EMPLOYER_REGION\"].value_counts()[:15]).reset_index()\n",
    "sucRHQ.columns = [\"SREGION\",\"COUNT SREGION\"]\n",
    "pd.concat([allRHQ,sucRHQ], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f71abf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# HOW MANY OF THE TOP COMPANIES, IDENTIFIED ABOVE, ARE HQ IN THE TOP 4 REGION\n",
    "companies = kingconsA[\"Yrs not in Top 15\"].sort_values()[:7].index.tolist()\n",
    "region = allRHQ[\"AREGION\"][:4].tolist()\n",
    "apps = allRHQ[\"COUNT AREGION\"][:4].tolist()\n",
    "companyregions = pd.DataFrame()\n",
    "for x in range(len(companies)):\n",
    "    for i in range(len(region)):\n",
    "        c = (df[df[\"EMPLOYER_NAME\"]==companies[x]][\"EMPLOYER_REGION\"]==region[i]).sum()\n",
    "        n = pd.DataFrame([companies[x],region[i],c]).transpose()\n",
    "        companyregions = pd.concat([companyregions,n])\n",
    "companyregions.columns=[\"EMPLOYER\",\"REGION\",\"APPLICATIONS\"]\n",
    "companyregions[\"TOTALAPP\"]=np.nan\n",
    "for x in range(len(region)):\n",
    "    companyregions[\"TOTALAPP\"] = np.where(companyregions[\"REGION\"]==region[x],apps[x],companyregions[\"TOTALAPP\"])\n",
    "companyregions = companyregions.set_index([\"EMPLOYER\",\"REGION\"])\n",
    "companyregions[\"% OF TOTAL APPLICATIONS\"] = (companyregions[\"APPLICATIONS\"]/companyregions[\"TOTALAPP\"])*100\n",
    "companyregions = companyregions.drop(\"TOTALAPP\", axis = 1)\n",
    "companyregions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e77f60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW MANY EMPLOYERS ARE NOT HQ IN USA?\n",
    "print(\"Only\",df[\"EMPLOYER_COUNTRY\"].value_counts()[1:].sum(),\"countries are not headquartered in the U.S.\")\n",
    "print(df[\"EMPLOYER_COUNTRY\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d575cdca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EXPLORING THE DISTRIBUTION OF NUMBER OF EMPLOYEES PER EMPLOYER\n",
    "avgempnum = df.groupby(\"EMPLOYER_NAME\")[\"EMPLOYER_NUM_EMPLOYEES\"].agg(lambda x: st.mode(x)[0][0])\n",
    "# WHAT IS THE MOST EMPLOYEES PER EMPLOYER\n",
    "avgempnum.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5947d94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE MODE LOOKS FISHY, LETS BRING IN THE TOTAL APPLICANTS\n",
    "countemp = pd.DataFrame(df.groupby(\"EMPLOYER_NAME\")[\"CASE_STATUS\"].count()).reset_index().sort_values(by = \"EMPLOYER_NAME\").reset_index(drop = True)\n",
    "empdf = pd.DataFrame(avgempnum).reset_index().sort_values(by = \"EMPLOYER_NAME\").reset_index(drop = True)\n",
    "empdf[\"COUNT\"] = np.where(empdf[\"EMPLOYER_NAME\"]==countemp[\"EMPLOYER_NAME\"],countemp[\"CASE_STATUS\"],np.nan)\n",
    "empdf = empdf.sort_values(by = \"EMPLOYER_NUM_EMPLOYEES\", ascending = False)\n",
    "empdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eab3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT THE DIST OF EMPLOYEE COUNT\n",
    "btmemp = avgempnum[avgempnum<=avgempnum.median()]\n",
    "topemp = avgempnum[avgempnum>avgempnum.median()]\n",
    "fig, ax = plt.subplots(1,2, figsize = (12,10))\n",
    "sns.histplot(x = btmemp, ax = ax[0])\n",
    "topend = sns.histplot(x = topemp, ax = ax[1])\n",
    "topend.set_xlim([50, 10000])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d74f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRELATION ANALYSIS: NUMBER OF EMPLOYEEs WITH CASE STATUS: CASE BY CASE\n",
    "st.linregress(x = df.dropna()[\"EMPLOYER_NUM_EMPLOYEES\"], y = df.dropna()[\"CASE_STATUS\"])\n",
    "# DOES NOT SHOW MUCH, AGGREGATE MAY SHOW A DIFFERENT STORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c9e689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRELATION ANALYSIS: AGGREGATE BY SIZE BRACKETS AND YEARLY\n",
    "# PLOTTING SUCCESS RATES OVER TIME BY SIZE OF COMPANY\n",
    "\n",
    "small = df[df[\"EMPLOYER_NUM_EMPLOYEES\"]<1000]\n",
    "medium = df[(df[\"EMPLOYER_NUM_EMPLOYEES\"]>=1000)&(df[\"EMPLOYER_NUM_EMPLOYEES\"]<10000)]\n",
    "large = df[df[\"EMPLOYER_NUM_EMPLOYEES\"]>=10000]\n",
    "\n",
    "\n",
    "smalldf = pd.DataFrame()\n",
    "meddf = pd.DataFrame()\n",
    "largedf = pd.DataFrame()\n",
    "regressdf = pd.DataFrame()\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    j = pd.DataFrame([len(small[(small[\"YEAR\"]==yr[x])&(small[\"CASE_STATUS\"]==1)])/len(small[small[\"YEAR\"]==yr[x]])])\n",
    "    med = pd.DataFrame([len(medium[(medium[\"YEAR\"]==yr[x])&(medium[\"CASE_STATUS\"]==1)])/len(medium[medium[\"YEAR\"]==yr[x]])])\n",
    "    lar = pd.DataFrame([len(large[(large[\"YEAR\"]==yr[x])&(large[\"CASE_STATUS\"]==1)])/len(large[large[\"YEAR\"]==yr[x]])])\n",
    "    smalldf = pd.concat([smalldf,j])\n",
    "    meddf = pd.concat([meddf,med])\n",
    "    largedf = pd.concat([largedf,lar])\n",
    "    regressdf = pd.concat([smalldf,meddf,largedf], axis = 1)\n",
    "    \n",
    "\n",
    "regressdf.index = yr\n",
    "regressdf.columns = [\"Small Success %\", \"Medium Success %\", \"Large Success %\"]\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "regressdf.plot(kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Success Rates by Company Size, Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb3133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARING DATA FOR LIN REGRESS AND GETTING P VALUE\n",
    "smallapps = []\n",
    "mediumapps = []\n",
    "largeapps = []\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    s = (small[\"YEAR\"]==yr[x]).sum()\n",
    "    ssd = st.tstd(small[small[\"YEAR\"]==yr[x]][\"CASE_STATUS\"])\n",
    "    m = (medium[\"YEAR\"]==yr[x]).sum()\n",
    "    msd = st.tstd(medium[medium[\"YEAR\"]==yr[x]][\"CASE_STATUS\"])\n",
    "    l = (large[\"YEAR\"]==yr[x]).sum()\n",
    "    lsd = st.tstd(large[large[\"YEAR\"]==yr[x]][\"CASE_STATUS\"])\n",
    "    smallapps.append(s)\n",
    "    mediumapps.append(m)\n",
    "    largeapps.append(l)\n",
    "regressdf = regressdf.reset_index()\n",
    "regressdf[\"Small Apps\"] = pd.DataFrame(smallapps)\n",
    "regressdf[\"Medium Apps\"] = pd.DataFrame(mediumapps)\n",
    "regressdf[\"Large Apps\"] = pd.DataFrame(largeapps)\n",
    "regressdf = regressdf.set_index(\"index\")\n",
    "# PLOTTING APPLICATION RATES OVER TIME BY SIZE OF COMPANY\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "regressdf.reset_index().plot(x = \"index\", y = [\"Small Apps\",\"Medium Apps\",\"Large Apps\"], kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Application Rates by Company Size, Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1064d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P VALUE PER YEAR\n",
    "small_med = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = regressdf.loc[yr[x]][\"Small Success %\"]\n",
    "    sample2_phat = regressdf.loc[yr[x]][\"Medium Success %\"]\n",
    "    sample1_size = regressdf.loc[yr[x]][\"Small Apps\"]\n",
    "    sample2_size = regressdf.loc[yr[x]][\"Medium Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    small_med.append(pval)\n",
    "\n",
    "pvaldf = pd.DataFrame(small_med)\n",
    "\n",
    "small_large = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = regressdf.loc[yr[x]][\"Small Success %\"]\n",
    "    sample2_phat = regressdf.loc[yr[x]][\"Large Success %\"]\n",
    "    sample1_size = regressdf.loc[yr[x]][\"Small Apps\"]\n",
    "    sample2_size = regressdf.loc[yr[x]][\"Large Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    small_large.append(pval)\n",
    "pvaldf = pd.concat([pvaldf,pd.DataFrame(small_large)], axis = 1)\n",
    "\n",
    "med_large = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = regressdf.loc[yr[x]][\"Medium Success %\"]\n",
    "    sample2_phat = regressdf.loc[yr[x]][\"Large Success %\"]\n",
    "    sample1_size = regressdf.loc[yr[x]][\"Medium Apps\"]\n",
    "    sample2_size = regressdf.loc[yr[x]][\"Large Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    med_large.append(pval)\n",
    "pvaldf = pd.concat([pvaldf,pd.DataFrame(med_large)], axis = 1)\n",
    "pvaldf.columns = [\"Small_Med\",\"Small_Large\",\"Med_Large\"]\n",
    "pvaldf.index = yr\n",
    "regressdf[[\"Small_Med\",\"Small_Large\",\"Med_Large\"]] = pvaldf[[\"Small_Med\",\"Small_Large\",\"Med_Large\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ffcfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION BETWEEN COMPANY SIZE AND SUCCESS RATE OVER TIME: SMALL COMPANY\n",
    "print(\"Small Company Success Rate Over Time Slope:\",round(st.linregress(y = regressdf[\"Small Success %\"], x = yr).slope,6),\", P Val:\",round(st.linregress(y = regressdf[\"Small Success %\"], x = yr).pvalue,4),\", R Val:\",round(st.linregress(y = regressdf[\"Small Success %\"], x = yr).rvalue,4),\"\\nWeak growth, .19% growth in success rate a year, and not a statistically significant slope.\")\n",
    "# LINEAR REGRESSION BETWEEN COMPANY SIZE AND APPLICATIONS OVER TIME: SMALL COMPANY\n",
    "print(\"\\nSmall Company Application Rate Over Time Slope:\",round(st.linregress(y = regressdf[\"Small Apps\"], x = yr).slope,3),\", P Val:\",round(st.linregress(y = regressdf[\"Small Apps\"], x = yr).pvalue,4),\", R Val:\",round(st.linregress(y = regressdf[\"Small Apps\"], x = yr).rvalue,4),\"\\nWeak growth, 1204 applications a year, and not a statistically significant slope.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fda4148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION BETWEEN COMPANY SIZE AND SUCCESS RATE OVER TIME: MEDIUM COMPANY\n",
    "print(\"Medium Company Success Rate Over Time Slope:\",round(st.linregress(y = regressdf[\"Medium Success %\"], x = yr).slope,6),\", P Val:\",round(st.linregress(y = regressdf[\"Medium Success %\"], x = yr).pvalue,4),\", R Val:\",round(st.linregress(y = regressdf[\"Medium Success %\"], x = yr).rvalue,4),\"\\nStrong growth, .57% growth in success rate a year, with a statistically significant slope.\")\n",
    "# LINEAR REGRESSION BETWEEN COMPANY SIZE AND APPLICATIONS OVER TIME: MEDIUM COMPANY\n",
    "print(\"\\nMedium Company Application Rate Over Time Slope:\",round(st.linregress(y = regressdf[\"Medium Apps\"], x = yr).slope,3),\", P Val:\",round(st.linregress(y = regressdf[\"Medium Apps\"], x = yr).pvalue,4),\", R Val:\",round(st.linregress(y = regressdf[\"Medium Apps\"], x = yr).rvalue,4),\"\\nModerate growth, 1122 applications a year, and not a statistically significant slope.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e97b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION BETWEEN COMPANY SIZE AND SUCCESS RATE OVER TIME: LARGE COMPANY\n",
    "print(\"Large Company Success Rate Over Time Slope:\",round(st.linregress(y = regressdf[\"Large Success %\"], x = yr).slope,6),\", P Val:\",round(st.linregress(y = regressdf[\"Large Success %\"], x = yr).pvalue,4),\", R Val:\",round(st.linregress(y = regressdf[\"Large Success %\"], x = yr).rvalue,4),\"\\nStrong growth, .83% growth in success rate a year, and a statistically significant slope.\")\n",
    "# LINEAR REGRESSION BETWEEN COMPANY SIZE AND APPLICATIONS OVER TIME: LARGE COMPANY\n",
    "print(\"\\nLarge Company Application Rate Over Time Slope:\",round(st.linregress(y = regressdf[\"Large Apps\"], x = yr).slope,3),\", P Val:\",round(st.linregress(y = regressdf[\"Large Apps\"], x = yr).pvalue,4),\", R Val:\",round(st.linregress(y = regressdf[\"Large Apps\"], x = yr).rvalue,4),\"\\nModerate growth, 2569 applications a year, and not a statistically significant slope.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4273bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING THE TOTAL PVAL\n",
    "regressdf.loc[\"ALL YRS\"] = regressdf.sum()\n",
    "smallsuc = []\n",
    "medsuc = []\n",
    "largesuc = []\n",
    "for x in range(len(yr)):\n",
    "    s = regressdf.loc[yr[x]].iloc[0]*regressdf.loc[yr[x]].iloc[3]\n",
    "    m = regressdf.loc[yr[x]].iloc[1]*regressdf.loc[yr[x]].iloc[4]\n",
    "    l = regressdf.loc[yr[x]].iloc[2]*regressdf.loc[yr[x]].iloc[5]\n",
    "    smallsuc.append(s)\n",
    "    medsuc.append(m)\n",
    "    largesuc.append(l)\n",
    "regressdf.loc[\"ALL YRS\"].iloc[0] = sum(smallsuc)/regressdf.loc[\"ALL YRS\"].iloc[3]\n",
    "regressdf.loc[\"ALL YRS\"].iloc[1] = sum(medsuc)/regressdf.loc[\"ALL YRS\"].iloc[4]\n",
    "regressdf.loc[\"ALL YRS\"].iloc[2] = sum(largesuc)/regressdf.loc[\"ALL YRS\"].iloc[5]\n",
    "\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = regressdf.loc[\"ALL YRS\"].iloc[0]\n",
    "    sample2_phat = regressdf.loc[\"ALL YRS\"].iloc[1]\n",
    "    sample1_size = regressdf.loc[\"ALL YRS\"].iloc[3]\n",
    "    sample2_size = regressdf.loc[\"ALL YRS\"].iloc[4]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "regressdf.loc[\"ALL YRS\"].iloc[6] = pval\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = regressdf.loc[\"ALL YRS\"].iloc[0]\n",
    "    sample2_phat = regressdf.loc[\"ALL YRS\"].iloc[2]\n",
    "    sample1_size = regressdf.loc[\"ALL YRS\"].iloc[3]\n",
    "    sample2_size = regressdf.loc[\"ALL YRS\"].iloc[5]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "regressdf.loc[\"ALL YRS\"].iloc[7] = pval\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = regressdf.loc[\"ALL YRS\"].iloc[1]\n",
    "    sample2_phat = regressdf.loc[\"ALL YRS\"].iloc[2]\n",
    "    sample1_size = regressdf.loc[\"ALL YRS\"].iloc[4]\n",
    "    sample2_size = regressdf.loc[\"ALL YRS\"].iloc[5]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "regressdf.loc[\"ALL YRS\"].iloc[8] = pval\n",
    "regressdf = regressdf.head(9).style.format(\"{:,.2f}\")\n",
    "regressdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1ce441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING A NEW COLUMN: SIZE OF COMPANY\n",
    "df[\"EMPLOYER_SIZE\"] = np.nan\n",
    "df[\"EMPLOYER_SIZE\"] = np.where(df[\"EMPLOYER_NUM_EMPLOYEES\"]<1000,\"SMALL\",df[\"EMPLOYER_SIZE\"])\n",
    "df[\"EMPLOYER_SIZE\"] = np.where((df[\"EMPLOYER_NUM_EMPLOYEES\"]>=1000)&(df[\"EMPLOYER_NUM_EMPLOYEES\"]<10000),\"MEDIUM\",df[\"EMPLOYER_SIZE\"])\n",
    "df[\"EMPLOYER_SIZE\"] = np.where(df[\"EMPLOYER_NUM_EMPLOYEES\"]>=10000,\"LARGE\",df[\"EMPLOYER_SIZE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ad71b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISTRIBUTION OF YEAR COMMENCED\n",
    "# PLOT THE DIST OF YEAR COMMENCED\n",
    "plt.figure(figsize = (12,10))\n",
    "empcount = sns.histplot(x = df[\"EMPLOYER_YEAR_COMMENCED_BUSINESS\"], bins = 100)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f5919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRELATION ANALYSIS: EMPLOYER AGE WITH CASE STATUS: CASE BY CASE\n",
    "st.linregress(x = df.dropna()[\"EMPLOYER_YEAR_COMMENCED_BUSINESS\"], y = df.dropna()[\"CASE_STATUS\"])\n",
    "# DOES NOT SHOW MUCH, AGGREGATE MAY SHOW A DIFFERENT STORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f544d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW COLUMN: BREAKING UP THE COMPANIES BY AGE\n",
    "df[\"EMPLOYER_AGE\"] = np.nan\n",
    "df[\"EMPLOYER_AGE\"] = np.where((df[\"YEAR\"]-df[\"EMPLOYER_YEAR_COMMENCED_BUSINESS\"])<15,\"YOUNG\",df[\"EMPLOYER_AGE\"])\n",
    "df[\"EMPLOYER_AGE\"] = np.where(((df[\"YEAR\"]-df[\"EMPLOYER_YEAR_COMMENCED_BUSINESS\"])>=15)&((df[\"YEAR\"]-df[\"EMPLOYER_YEAR_COMMENCED_BUSINESS\"])<30),\"MEDIUM\",df[\"EMPLOYER_AGE\"])\n",
    "df[\"EMPLOYER_AGE\"] = np.where((df[\"YEAR\"]-df[\"EMPLOYER_YEAR_COMMENCED_BUSINESS\"])>=30,\"OLD\",df[\"EMPLOYER_AGE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd496d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRELATION ANALYSIS: AGGREGATE BY SIZE BRACKETS AND YEARLY\n",
    "# PLOTTING SUCCESS RATES OVER TIME BY SIZE OF COMPANY\n",
    "\n",
    "young = df[df[\"EMPLOYER_AGE\"]==\"YOUNG\"]\n",
    "mid = df[df[\"EMPLOYER_AGE\"]==\"MEDIUM\"]\n",
    "old = df[df[\"EMPLOYER_AGE\"]==\"OLD\"]\n",
    "\n",
    "\n",
    "youndf = pd.DataFrame()\n",
    "middf = pd.DataFrame()\n",
    "olddf = pd.DataFrame()\n",
    "linregdf = pd.DataFrame()\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    j = pd.DataFrame([len(young[(young[\"YEAR\"]==yr[x])&(young[\"CASE_STATUS\"]==1)])/len(young[young[\"YEAR\"]==yr[x]])])\n",
    "    med = pd.DataFrame([len(mid[(mid[\"YEAR\"]==yr[x])&(mid[\"CASE_STATUS\"]==1)])/len(mid[mid[\"YEAR\"]==yr[x]])])\n",
    "    oldy = pd.DataFrame([len(old[(old[\"YEAR\"]==yr[x])&(old[\"CASE_STATUS\"]==1)])/len(old[old[\"YEAR\"]==yr[x]])])\n",
    "    youndf = pd.concat([youndf,j])\n",
    "    middf = pd.concat([middf,med])\n",
    "    olddf = pd.concat([olddf,oldy])\n",
    "    linregdf = pd.concat([youndf,middf,olddf], axis = 1)\n",
    "    \n",
    "\n",
    "linregdf.index = yr\n",
    "linregdf.columns = [\"Young Success %\", \"Medium Success %\", \"Old Success %\"]\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "linregdf.plot(kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Success Rates by Company Age, Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47191817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARING DATA FOR LIN REGRESS AND GETTING P VALUE\n",
    "youngapps = []\n",
    "midapps = []\n",
    "oldapps = []\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    y = (young[\"YEAR\"]==yr[x]).sum()\n",
    "    m = (mid[\"YEAR\"]==yr[x]).sum()\n",
    "    o = (old[\"YEAR\"]==yr[x]).sum()\n",
    "    youngapps.append(y)\n",
    "    midapps.append(m)\n",
    "    oldapps.append(o)\n",
    "linregdf = linregdf.reset_index()\n",
    "linregdf[\"Young Apps\"] = pd.DataFrame(youngapps)\n",
    "linregdf[\"Medium Apps\"] = pd.DataFrame(midapps)\n",
    "linregdf[\"Old Apps\"] = pd.DataFrame(oldapps)\n",
    "linregdf = linregdf.set_index(\"index\")\n",
    "# PLOTTING APPLICATION RATES OVER TIME BY SIZE OF COMPANY\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "linregdf.reset_index().plot(x = \"index\", y = [\"Young Apps\",\"Medium Apps\",\"Old Apps\"], kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Application Rates by Company Age, Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadb899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P VALUE PER YEAR\n",
    "young_med = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[yr[x]][\"Young Success %\"]\n",
    "    sample2_phat = linregdf.loc[yr[x]][\"Medium Success %\"]\n",
    "    sample1_size = linregdf.loc[yr[x]][\"Young Apps\"]\n",
    "    sample2_size = linregdf.loc[yr[x]][\"Medium Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    young_med.append(pval)\n",
    "\n",
    "pvaldf = pd.DataFrame(young_med)\n",
    "\n",
    "young_old = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[yr[x]][\"Young Success %\"]\n",
    "    sample2_phat = linregdf.loc[yr[x]][\"Old Success %\"]\n",
    "    sample1_size = linregdf.loc[yr[x]][\"Young Apps\"]\n",
    "    sample2_size = linregdf.loc[yr[x]][\"Old Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    young_old.append(pval)\n",
    "pvaldf = pd.concat([pvaldf,pd.DataFrame(young_old)], axis = 1)\n",
    "\n",
    "med_old = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[yr[x]][\"Medium Success %\"]\n",
    "    sample2_phat = linregdf.loc[yr[x]][\"Old Success %\"]\n",
    "    sample1_size = linregdf.loc[yr[x]][\"Medium Apps\"]\n",
    "    sample2_size = linregdf.loc[yr[x]][\"Old Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    med_old.append(pval)\n",
    "pvaldf = pd.concat([pvaldf,pd.DataFrame(med_old)], axis = 1)\n",
    "pvaldf.columns = [\"Young_Med\",\"Young_Old\",\"Med_Old\"]\n",
    "pvaldf.index = yr\n",
    "linregdf[[\"Young_Med\",\"Young_Old\",\"Med_Old\"]] = pvaldf[[\"Young_Med\",\"Young_Old\",\"Med_Old\"]]\n",
    "linregdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bae43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION BETWEEN COMPANY AGE AND SUCCESS RATE OVER TIME: YOUNG COMPANY\n",
    "print(\"Young Company Success Rate Over Time Slope:\",round(st.linregress(y = linregdf[\"Young Success %\"], x = yr).slope,6),\", P Val:\",round(st.linregress(y = linregdf[\"Young Success %\"], x = yr).pvalue,4),\", R Val:\",round(st.linregress(y = linregdf[\"Young Success %\"], x = yr).rvalue,4),\"\\nWeak growth, .25% growth in success rate a year, and not a statistically significant slope.\")\n",
    "# LINEAR REGRESSION BETWEEN COMPANY AGE AND APPLICATIONS OVER TIME: YOUNG COMPANY\n",
    "print(\"\\nYoung Company Application Rate Over Time Slope:\",round(st.linregress(y = linregdf[\"Young Apps\"], x = yr).slope,3),\", P Val:\",round(st.linregress(y = linregdf[\"Young Apps\"], x = yr).pvalue,4),\", R Val:\",round(st.linregress(y = linregdf[\"Young Apps\"], x = yr).rvalue,4),\"\\nWeak growth, 531 applications a year, and not a statistically significant slope.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f263d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION BETWEEN COMPANY AGE AND SUCCESS RATE OVER TIME: MEDIUM COMPANY\n",
    "print(\"Medium Company Success Rate Over Time Slope:\",round(st.linregress(y = linregdf[\"Medium Success %\"], x = yr).slope,6),\", P Val:\",round(st.linregress(y = linregdf[\"Medium Success %\"], x = yr).pvalue,4),\", R Val:\",round(st.linregress(y = linregdf[\"Medium Success %\"], x = yr).rvalue,4),\"\\Moderate growth, .29% growth in success rate a year, and not a statistically significant slope.\")\n",
    "# LINEAR REGRESSION BETWEEN COMPANY AGE AND APPLICATIONS OVER TIME: MEDIUM COMPANY\n",
    "print(\"\\nMedium Company Application Rate Over Time Slope:\",round(st.linregress(y = linregdf[\"Medium Apps\"], x = yr).slope,3),\", P Val:\",round(st.linregress(y = linregdf[\"Medium Apps\"], x = yr).pvalue,4),\", R Val:\",round(st.linregress(y = linregdf[\"Medium Apps\"], x = yr).rvalue,4),\"\\Moderate growth, 1786 applications a year, and not a statistically significant slope.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509f641f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION BETWEEN COMPANY AGE AND SUCCESS RATE OVER TIME: OLD COMPANY\n",
    "print(\"Old Company Success Rate Over Time Slope:\",round(st.linregress(y = linregdf[\"Old Success %\"], x = yr).slope,6),\", P Val:\",round(st.linregress(y = linregdf[\"Old Success %\"], x = yr).pvalue,4),\", R Val:\",round(st.linregress(y = linregdf[\"Old Success %\"], x = yr).rvalue,4),\"\\nVery strong growth, 1% growth in success rate a year, and a statistically significant slope.\")\n",
    "# LINEAR REGRESSION BETWEEN COMPANY AGE AND APPLICATIONS OVER TIME: OLD COMPANY\n",
    "print(\"\\nOld Company Application Rate Over Time Slope:\",round(st.linregress(y = linregdf[\"Old Apps\"], x = yr).slope,3),\", P Val:\",round(st.linregress(y = linregdf[\"Old Apps\"], x = yr).pvalue,4),\", R Val:\",round(st.linregress(y = linregdf[\"Old Apps\"], x = yr).rvalue,4),\"\\nModerate growth, 2578 applications a year, and not a statistically significant slope.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e839a3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING THE TOTAL PVAL\n",
    "linregdf.loc[\"ALL YRS\"] = linregdf.sum()\n",
    "youngsuc = []\n",
    "medsuc = []\n",
    "oldsuc = []\n",
    "for x in range(len(yr)):\n",
    "    y = linregdf.loc[yr[x]].iloc[0]*linregdf.loc[yr[x]].iloc[3]\n",
    "    m = linregdf.loc[yr[x]].iloc[1]*linregdf.loc[yr[x]].iloc[4]\n",
    "    o = linregdf.loc[yr[x]].iloc[2]*linregdf.loc[yr[x]].iloc[5]\n",
    "    youngsuc.append(y)\n",
    "    medsuc.append(m)\n",
    "    oldsuc.append(o)\n",
    "linregdf.loc[\"ALL YRS\"].iloc[0] = sum(youngsuc)/linregdf.loc[\"ALL YRS\"].iloc[3]\n",
    "linregdf.loc[\"ALL YRS\"].iloc[1] = sum(medsuc)/linregdf.loc[\"ALL YRS\"].iloc[4]\n",
    "linregdf.loc[\"ALL YRS\"].iloc[2] = sum(oldsuc)/linregdf.loc[\"ALL YRS\"].iloc[5]\n",
    "\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[\"ALL YRS\"].iloc[0]\n",
    "    sample2_phat = linregdf.loc[\"ALL YRS\"].iloc[1]\n",
    "    sample1_size = linregdf.loc[\"ALL YRS\"].iloc[3]\n",
    "    sample2_size = linregdf.loc[\"ALL YRS\"].iloc[4]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "linregdf.loc[\"ALL YRS\"].iloc[6] = pval\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[\"ALL YRS\"].iloc[0]\n",
    "    sample2_phat = linregdf.loc[\"ALL YRS\"].iloc[2]\n",
    "    sample1_size = linregdf.loc[\"ALL YRS\"].iloc[3]\n",
    "    sample2_size = linregdf.loc[\"ALL YRS\"].iloc[5]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "linregdf.loc[\"ALL YRS\"].iloc[7] = pval\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[\"ALL YRS\"].iloc[1]\n",
    "    sample2_phat = linregdf.loc[\"ALL YRS\"].iloc[2]\n",
    "    sample1_size = linregdf.loc[\"ALL YRS\"].iloc[4]\n",
    "    sample2_size = linregdf.loc[\"ALL YRS\"].iloc[5]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "linregdf.loc[\"ALL YRS\"].iloc[8] = pval\n",
    "linregdf = linregdf.head(9).style.format(\"{:,.2f}\")\n",
    "linregdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f2081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 15 NAICS of all time 2014-2021\n",
    "allcode = pd.DataFrame(df[\"NAICS_CODE\"].value_counts()[:15]).reset_index()\n",
    "allcode.columns = [\"ACode\",\"COUNT ACode\"]\n",
    "succode = pd.DataFrame(df[df[\"CASE_STATUS\"]==1][\"NAICS_CODE\"].value_counts()[:15]).reset_index()\n",
    "succode.columns = [\"SCode\",\"COUNT SCode\"]\n",
    "pd.concat([allcode,succode], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181f5d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing the top 15 for each year codes: only succesful applications\n",
    "top15scode = pd.DataFrame()\n",
    "def grab (year):\n",
    "    return df[(df[\"YEAR\"]==year)&(df[\"CASE_STATUS\"]==1)][\"NAICS_CODE\"].value_counts()[:15]\n",
    "for x in range(len(yr)):\n",
    "    j = pd.DataFrame(grab(yr[x]))\n",
    "    top15scode = pd.concat([top15scode,j], axis = 1)\n",
    "top15scode.columns= yr\n",
    "# King of Consistency: HOW MANY YEARS IS THIS CODE NOT IN THE TOP 15?: TOP 10 CONSISTENT CODES...\n",
    "kingcons = top15scode.isnull().replace(False,0).replace(True,1)\n",
    "kingcons[\"Yrs not in Top 15\"] = top15scode.isnull().replace(False,0).replace(True,1).sum(axis = 1)\n",
    "kingcons[\"Yrs not in Top 15\"].sort_values()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078ab04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing the top 15 for each year codes: total applications\n",
    "top15acode = pd.DataFrame()\n",
    "def grab (year):\n",
    "    return df[df[\"YEAR\"]==year][\"NAICS_CODE\"].value_counts()[:15]\n",
    "for x in range(len(yr)):\n",
    "    j = pd.DataFrame(grab(yr[x]))\n",
    "    top15acode = pd.concat([top15acode,j], axis = 1)\n",
    "top15acode.columns= yr\n",
    "# King of Consistency: HOW MANY YEARS IS THIS CODE NOT IN THE TOP 15?: TOP 10 CONSISTENT CODES...\n",
    "kingconsA = top15acode.isnull().replace(False,0).replace(True,1)\n",
    "kingconsA[\"Yrs not in Top 15\"] = top15acode.isnull().replace(False,0).replace(True,1).sum(axis = 1)\n",
    "kingconsA[\"Yrs not in Top 15\"].sort_values()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b548f63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DIVING DEEPER INTO THE TOP CODES!\n",
    "#ANCHORED BY STATE: PW SKILL,APPLICATIONS, MEDIAN WAGE, YRS POST GRADE, MEDIAN DAYS\n",
    "\n",
    "code = \"541511\"\n",
    "# CHANGE THIS FOR ANY CODE\n",
    "\n",
    "# MEDIAN WAGE, MEDIAN YEARS AFTER GRADUATING, MEDIAN DAYS TO DECIDE : FOR EACH STATE IN THE TOP 15 OF THE COMPANY AND BY PW LEVEL\n",
    "topstate = df[df[\"NAICS_CODE\"] == code][\"WORKSITE_STATE\"].value_counts()[:15].index.tolist()\n",
    "pwskill = df[\"PW_SKILL_LEVEL\"].dropna().unique().tolist()\n",
    "pwskill.sort()\n",
    "\n",
    "\n",
    "statelist = []\n",
    "pwlist = pwskill*len(topstate)\n",
    "apps = []\n",
    "medwage = []\n",
    "yrsgrad = []\n",
    "meddays = []\n",
    "\n",
    "\n",
    "for x in range(len(topstate)):\n",
    "    s = (topstate[x]+\",\")*len(pwskill)\n",
    "    s = \"\".join(s).split(\",\")\n",
    "    s = list(filter((\"\").__ne__, s))\n",
    "    statelist.append(s) \n",
    "for x in range(len(topstate)):\n",
    "    for j in range(len(pwskill)):\n",
    "        a = len(df[(df[\"NAICS_CODE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"NAICS_CODE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]])\n",
    "        apps.append(a)\n",
    "        b = df[(df[\"NAICS_CODE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"NAICS_CODE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]][\"WO_A\"].median()\n",
    "        medwage.append(b)\n",
    "        c = df[(df[\"NAICS_CODE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"NAICS_CODE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]][\"YRSPOSTGRAD\"].median()\n",
    "        yrsgrad.append(c)\n",
    "        d = df[(df[\"NAICS_CODE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"NAICS_CODE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]][\"DAYS_TO_DECIDE\"].median()\n",
    "        meddays.append(d)\n",
    "codestatedf = pd.DataFrame([sum(statelist, []),pwlist,apps,medwage,yrsgrad,meddays]).transpose()\n",
    "codestatedf.columns = [\"STATE\",\"PW LEVEL\", \"TOTAL APPLICATIONS\",\"MEDIAN WAGE\",\"MEDIAN YRS POST GRAD\",\"MEDIAN DAYS TO DECIDE\"]\n",
    "codestatedf = codestatedf.set_index([\"STATE\",\"PW LEVEL\"])\n",
    "codestatedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2eff606",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ANCHORED BY STATE: EDU LEVEL,APPLICATIONS, MEDIAN WAGE, YRS POST GRADE, MEDIAN DAYS\n",
    "# MEDIAN WAGE, MEDIAN YEARS AFTER GRADUATING, MEDIAN DAYS TO DECIDE : FOR EACH STATE IN THE TOP 15 OF THE COMPANY AND BY EDU LEVEL\n",
    "edu = df[\"FOREIGN_WORKER_EDUCATION\"].dropna().unique().tolist()\n",
    "edu[0],edu[1],edu[2],edu[3],edu[4],edu[5] = edu[3],edu[5],edu[0],edu[2],edu[1],edu[4]\n",
    "\n",
    "statelist = []\n",
    "edulist = edu*len(topstate)\n",
    "apps = []\n",
    "medwage = []\n",
    "yrsgrad = []\n",
    "meddays = []\n",
    "\n",
    "\n",
    "for x in range(len(topstate)):\n",
    "    s = (topstate[x]+\",\")*len(edu)\n",
    "    s = \"\".join(s).split(\",\")\n",
    "    s = list(filter((\"\").__ne__, s))\n",
    "    statelist.append(s) \n",
    "for x in range(len(topstate)):\n",
    "    for j in range(len(edu)):\n",
    "        a = len(df[(df[\"NAICS_CODE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"NAICS_CODE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]])\n",
    "        apps.append(a)\n",
    "        b = df[(df[\"NAICS_CODE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"NAICS_CODE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]][\"WO_A\"].median()\n",
    "        medwage.append(b)\n",
    "        c = df[(df[\"NAICS_CODE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"NAICS_CODE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]][\"YRSPOSTGRAD\"].median()\n",
    "        yrsgrad.append(c)\n",
    "        d = df[(df[\"NAICS_CODE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"NAICS_CODE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]][\"DAYS_TO_DECIDE\"].median()\n",
    "        meddays.append(d)\n",
    "codestatedf1 = pd.DataFrame([sum(statelist, []),edulist,apps,medwage,yrsgrad,meddays]).transpose()\n",
    "codestatedf1.columns = [\"STATE\",\"EDU LEVEL\", \"TOTAL APPLICATIONS\",\"MEDIAN WAGE\",\"MEDIAN YRS POST GRAD\",\"MEDIAN DAYS TO DECIDE\"]\n",
    "codestatedf1 = codestatedf1.set_index([\"STATE\",\"EDU LEVEL\"])\n",
    "codestatedf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4d01a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW COLUMN: 1 is in the top 15 of the YEAR, 0 is not: Applications\n",
    "filler = pd.DataFrame()\n",
    "for x in range(len(yr)):\n",
    "    listy = top15acode[yr[x]].dropna().index.tolist()\n",
    "    filler = pd.concat([filler,pd.DataFrame((df[df[\"YEAR\"]==yr[x]][\"NAICS_CODE\"].isin(listy)).replace(True,1).replace(False,0))])\n",
    "\n",
    "filler = filler.reset_index().sort_values(by = \"index\").set_index(\"index\")\n",
    "filler.columns = [\"TOP15CODE\"]\n",
    "df[\"TOP15CODE\"] = filler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec9b244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRELATION ANALYSIS: NUMBER OF EMPLOYEEs WITH CASE STATUS: CASE BY CASE\n",
    "st.linregress(x = df.dropna()[\"TOP15CODE\"], y = df.dropna()[\"CASE_STATUS\"])\n",
    "# DOES NOT SHOW MUCH, AGGREGATE MAY SHOW A DIFFERENT STORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac970d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING THE DIFFERENT SUCCESS RATES OVER TIME BY TOP 15 COMPANY (IN TERMS OF ANNUAL APPLICATIONS)\n",
    "top15 = df[df[\"TOP15CODE\"]==1]\n",
    "not15 = df[df[\"TOP15CODE\"]==0]\n",
    "\n",
    "topdf = pd.DataFrame()\n",
    "notdf = pd.DataFrame()\n",
    "lanalysisdf = pd.DataFrame()\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    j = pd.DataFrame([len(top15[(top15[\"YEAR\"]==yr[x])&(top15[\"CASE_STATUS\"]==1)])/len(top15[top15[\"YEAR\"]==yr[x]])])\n",
    "    b = pd.DataFrame([len(not15[(not15[\"YEAR\"]==yr[x])&(not15[\"CASE_STATUS\"]==1)])/len(not15[not15[\"YEAR\"]==yr[x]])])\n",
    "    topdf = pd.concat([topdf,j])\n",
    "    notdf = pd.concat([notdf,b])\n",
    "    lanalysisdf = pd.concat([topdf,notdf], axis = 1)\n",
    "\n",
    "lanalysisdf.index = yr\n",
    "lanalysisdf.columns = [\"Top 15 Success %\", \"Not 15 Success %\"]\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "lanalysisdf.plot(kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Success Rates by Companies Codes in the Top 15 and not in the Top 15, Over Time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e773999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARING DATA FOR LIN REGRESS AND GETTING P VALUE\n",
    "top15apps = []\n",
    "not15apps = []\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    t = (top15[\"YEAR\"]==yr[x]).sum()\n",
    "    n = (not15[\"YEAR\"]==yr[x]).sum()\n",
    "    top15apps.append(t)\n",
    "    not15apps.append(n)\n",
    "lanalysisdf = lanalysisdf.reset_index()\n",
    "lanalysisdf[\"Top 15 Apps\"] = pd.DataFrame(top15apps)\n",
    "lanalysisdf[\"Not 15 Apps\"] = pd.DataFrame(not15apps)\n",
    "lanalysisdf = lanalysisdf.set_index(\"index\")\n",
    "# PLOTTING APPLICATION RATES OVER TIME BY TOP 15 or NOT TOP 15\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "lanalysisdf.reset_index().plot(x = \"index\", y = [\"Top 15 Apps\",\"Not 15 Apps\"], kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Application Rates by Companies Codes in the Top 15 and not in the Top 15, Over Time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60006482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P VALUE PER YEAR: ON THE SUCCESS RATE\n",
    "top_not = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = lanalysisdf.loc[yr[x]][\"Top 15 Success %\"]\n",
    "    sample2_phat = lanalysisdf.loc[yr[x]][\"Not 15 Success %\"]\n",
    "    sample1_size = lanalysisdf.loc[yr[x]][\"Top 15 Apps\"]\n",
    "    sample2_size = lanalysisdf.loc[yr[x]][\"Not 15 Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    top_not.append(pval)\n",
    "\n",
    "pvaldf = pd.DataFrame(top_not)\n",
    "pvaldf.columns = [\"Top 15 Vs. Not Top 15\"]\n",
    "pvaldf.index = yr\n",
    "lanalysisdf[\"PVAL Success %\"] = pvaldf[\"Top 15 Vs. Not Top 15\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185d1022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION BETWEEN COMPANY SIZE AND SUCCESS RATE OVER TIME: TOP 15 COMPANY\n",
    "print(\"Top 15 Company Codes Success Rate Over Time Slope:\",round(st.linregress(y = lanalysisdf[\"Top 15 Success %\"], x = yr).slope,6),\", P Val:\",round(st.linregress(y = lanalysisdf[\"Top 15 Success %\"], x = yr).pvalue,4),\", R Val:\",round(st.linregress(y = lanalysisdf[\"Top 15 Success %\"], x = yr).rvalue,4),\"\\nStrong growth, .75% growth in success rate a year, and a statistically significant slope.\")\n",
    "# LINEAR REGRESSION BETWEEN COMPANY SIZE AND APPLICATIONS OVER TIME: TOP 15 COMPANY\n",
    "print(\"\\nTop 15 Company Codes Application Rate Over Time Slope:\",round(st.linregress(y = lanalysisdf[\"Top 15 Apps\"], x = yr).slope,3),\", P Val:\",round(st.linregress(y = lanalysisdf[\"Top 15 Apps\"], x = yr).pvalue,4),\", R Val:\",round(st.linregress(y = lanalysisdf[\"Top 15 Apps\"], x = yr).rvalue,4),\"\\nModerate growth, 2068 applications a year, and not a statistically significant slope.\")\n",
    "\n",
    "# LINEAR REGRESSION BETWEEN COMPANY SIZE AND SUCCESS RATE OVER TIME: NOT TOP 15 COMPANY\n",
    "print(\"\\n\\nNot Top 15 Company Codes Success Rate Over Time Slope:\",round(st.linregress(y = lanalysisdf[\"Not 15 Success %\"], x = yr).slope,6),\", P Val:\",round(st.linregress(y = lanalysisdf[\"Not 15 Success %\"], x = yr).pvalue,4),\", R Val:\",round(st.linregress(y = lanalysisdf[\"Not 15 Success %\"], x = yr).rvalue,4),\"\\nStrong growth, .47% growth in success rate a year, and a statistically significant slope.\")\n",
    "# LINEAR REGRESSION BETWEEN COMPANY SIZE AND APPLICATIONS OVER TIME: NOT TOP 15 COMPANY\n",
    "print(\"\\nNot Top 15 Company Codes Application Rate Over Time Slope:\",round(st.linregress(y = lanalysisdf[\"Not 15 Apps\"], x = yr).slope,3),\", P Val:\",round(st.linregress(y = lanalysisdf[\"Not 15 Apps\"], x = yr).pvalue,4),\", R Val:\",round(st.linregress(y = lanalysisdf[\"Not 15 Apps\"], x = yr).rvalue,4),\"\\nWeak growth, 2828 applications a year, but not a statistically significant slope.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f637e076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING THE TOTAL PVAL\n",
    "lanalysisdf.loc[\"ALL YRS\"] = lanalysisdf.sum()\n",
    "top15suc = []\n",
    "not15suc = []\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    t15 = lanalysisdf.loc[yr[x]].iloc[0]*lanalysisdf.loc[yr[x]].iloc[2]\n",
    "    n15 = lanalysisdf.loc[yr[x]].iloc[1]*lanalysisdf.loc[yr[x]].iloc[3]\n",
    "    top15suc.append(t15)\n",
    "    not15suc.append(n15)\n",
    "lanalysisdf.loc[\"ALL YRS\"].iloc[0] = sum(top15suc)/lanalysisdf.loc[\"ALL YRS\"].iloc[2]\n",
    "lanalysisdf.loc[\"ALL YRS\"].iloc[1] = sum(not15suc)/lanalysisdf.loc[\"ALL YRS\"].iloc[3]\n",
    "\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = lanalysisdf.loc[\"ALL YRS\"].iloc[0]\n",
    "    sample2_phat = lanalysisdf.loc[\"ALL YRS\"].iloc[1]\n",
    "    sample1_size = lanalysisdf.loc[\"ALL YRS\"].iloc[2]\n",
    "    sample2_size = lanalysisdf.loc[\"ALL YRS\"].iloc[3]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "lanalysisdf.loc[\"ALL YRS\"].iloc[4] = pval\n",
    "\n",
    "lanalysisdf = lanalysisdf.head(9).style.format(\"{:,.2f}\")\n",
    "lanalysisdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129fea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### APPLICATION / JOB RELATED #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2999136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROUPBY YEAR AND PLOT THE .25, .5, and .75 WO_A\n",
    "wages = pd.DataFrame([df.groupby(\"YEAR\")[\"WO_A\"].quantile(.25),df.groupby(\"YEAR\")[\"WO_A\"].quantile(.5),\n",
    " df.groupby(\"YEAR\")[\"WO_A\"].quantile(.75)]).transpose()\n",
    "wages.columns = [\".25\",\".5\",\".75\"]\n",
    "wages.plot(kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Annual Wages by Percentiles, Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888a4de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRELATION ANALYSIS: Wage offered and success\n",
    "st.linregress(x = df.dropna()[\"WO_A\"], y = df.dropna()[\"CASE_STATUS\"])\n",
    "# DOES NOT SHOW MUCH, AGGREGATE MAY SHOW A DIFFERENT STORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cb87d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING A NEW COLUMN: WAGE GROUP, SPLITS WAGES INTO THIRDS\n",
    "high = df.groupby(\"YEAR\")[\"WO_A\"].quantile(2/3).tolist()\n",
    "low = df.groupby(\"YEAR\")[\"WO_A\"].quantile(1/3).tolist()\n",
    "df[\"WAGE_GROUP\"] = np.nan\n",
    "for x in range(len(yr)):\n",
    "    df[\"WAGE_GROUP\"] = np.where((df[\"YEAR\"]==yr[x])&(df[\"WO_A\"]<low[x]),\"LOW\",df[\"WAGE_GROUP\"])\n",
    "    df[\"WAGE_GROUP\"] = np.where((df[\"YEAR\"]==yr[x])&(df[\"WO_A\"]>=low[x])&(df[\"WO_A\"]<high[x]),\"MED\",df[\"WAGE_GROUP\"])\n",
    "    df[\"WAGE_GROUP\"] = np.where((df[\"YEAR\"]==yr[x])&(df[\"WO_A\"]>=high[x]),\"HIGH\",df[\"WAGE_GROUP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4b04ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRELATION ANALYSIS: AGGREGATE BY WAGE GROUP\n",
    "# PLOTTING SUCCESS RATES OVER TIME BY WAGE GROUP\n",
    "\n",
    "young = df[df[\"WAGE_GROUP\"]==\"LOW\"]\n",
    "mid = df[df[\"WAGE_GROUP\"]==\"MED\"]\n",
    "old = df[df[\"WAGE_GROUP\"]==\"HIGH\"]\n",
    "\n",
    "\n",
    "youndf = pd.DataFrame()\n",
    "middf = pd.DataFrame()\n",
    "olddf = pd.DataFrame()\n",
    "linregdf = pd.DataFrame()\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    j = pd.DataFrame([len(young[(young[\"YEAR\"]==yr[x])&(young[\"CASE_STATUS\"]==1)])/len(young[young[\"YEAR\"]==yr[x]])])\n",
    "    med = pd.DataFrame([len(mid[(mid[\"YEAR\"]==yr[x])&(mid[\"CASE_STATUS\"]==1)])/len(mid[mid[\"YEAR\"]==yr[x]])])\n",
    "    oldy = pd.DataFrame([len(old[(old[\"YEAR\"]==yr[x])&(old[\"CASE_STATUS\"]==1)])/len(old[old[\"YEAR\"]==yr[x]])])\n",
    "    youndf = pd.concat([youndf,j])\n",
    "    middf = pd.concat([middf,med])\n",
    "    olddf = pd.concat([olddf,oldy])\n",
    "    linregdf = pd.concat([youndf,middf,olddf], axis = 1)\n",
    "    \n",
    "\n",
    "linregdf.index = yr\n",
    "linregdf.columns = [\"Low Success %\", \"Medium Success %\", \"High Success %\"]\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "linregdf.plot(kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Success Rates by Wage Bracket, Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401b7aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARING DATA FOR LIN REGRESS AND GETTING P VALUE\n",
    "youngapps = []\n",
    "midapps = []\n",
    "oldapps = []\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    y = (young[\"YEAR\"]==yr[x]).sum()\n",
    "    m = (mid[\"YEAR\"]==yr[x]).sum()\n",
    "    o = (old[\"YEAR\"]==yr[x]).sum()\n",
    "    youngapps.append(y)\n",
    "    midapps.append(m)\n",
    "    oldapps.append(o)\n",
    "linregdf = linregdf.reset_index()\n",
    "linregdf[\"Low Apps\"] = pd.DataFrame(youngapps)\n",
    "linregdf[\"Medium Apps\"] = pd.DataFrame(midapps)\n",
    "linregdf[\"High Apps\"] = pd.DataFrame(oldapps)\n",
    "linregdf = linregdf.set_index(\"index\")\n",
    "\n",
    "# P VALUE PER YEAR\n",
    "young_med = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[yr[x]][\"Low Success %\"]\n",
    "    sample2_phat = linregdf.loc[yr[x]][\"Medium Success %\"]\n",
    "    sample1_size = linregdf.loc[yr[x]][\"Low Apps\"]\n",
    "    sample2_size = linregdf.loc[yr[x]][\"Medium Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    young_med.append(pval)\n",
    "\n",
    "pvaldf = pd.DataFrame(young_med)\n",
    "\n",
    "young_old = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[yr[x]][\"Low Success %\"]\n",
    "    sample2_phat = linregdf.loc[yr[x]][\"High Success %\"]\n",
    "    sample1_size = linregdf.loc[yr[x]][\"Low Apps\"]\n",
    "    sample2_size = linregdf.loc[yr[x]][\"High Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    young_old.append(pval)\n",
    "pvaldf = pd.concat([pvaldf,pd.DataFrame(young_old)], axis = 1)\n",
    "\n",
    "med_old = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[yr[x]][\"Medium Success %\"]\n",
    "    sample2_phat = linregdf.loc[yr[x]][\"High Success %\"]\n",
    "    sample1_size = linregdf.loc[yr[x]][\"Medium Apps\"]\n",
    "    sample2_size = linregdf.loc[yr[x]][\"High Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    med_old.append(pval)\n",
    "pvaldf = pd.concat([pvaldf,pd.DataFrame(med_old)], axis = 1)\n",
    "pvaldf.columns = [\"Low_Med\",\"Low_High\",\"Med_High\"]\n",
    "pvaldf.index = yr\n",
    "linregdf[[\"Low_Med\",\"Low_High\",\"Med_High\"]] = pvaldf[[\"Low_Med\",\"Low_High\",\"Med_High\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab2e54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION BETWEEN COMPANY AGE AND SUCCESS RATE OVER TIME: Low Wage\n",
    "print(\"Low Wage Success Rate Over Time Slope:\",round(st.linregress(y = linregdf[\"Low Success %\"], x = yr).slope,6),\", P Val:\",round(st.linregress(y = linregdf[\"Low Success %\"], x = yr).pvalue,4),\", R Val:\",round(st.linregress(y = linregdf[\"Low Success %\"], x = yr).rvalue,4),\"\\nModerate growth, .56% growth in success rate a year, and not a statistically significant slope.\")\n",
    "# LINEAR REGRESSION BETWEEN COMPANY AGE AND APPLICATIONS OVER TIME: Low Wage\n",
    "print(\"\\nLow Wage Application Rate Over Time Slope:\",round(st.linregress(y = linregdf[\"Low Apps\"], x = yr).slope,3),\", P Val:\",round(st.linregress(y = linregdf[\"Low Apps\"], x = yr).pvalue,4),\", R Val:\",round(st.linregress(y = linregdf[\"Low Apps\"], x = yr).rvalue,4),\"\\nWeak growth, 1633 applications a year, and not a statistically significant slope.\")\n",
    "\n",
    "# LINEAR REGRESSION BETWEEN COMPANY AGE AND SUCCESS RATE OVER TIME: MEDIUM Wage\n",
    "print(\"\\n\\nMedium Wage Success Rate Over Time Slope:\",round(st.linregress(y = linregdf[\"Medium Success %\"], x = yr).slope,6),\", P Val:\",round(st.linregress(y = linregdf[\"Medium Success %\"], x = yr).pvalue,4),\", R Val:\",round(st.linregress(y = linregdf[\"Medium Success %\"], x = yr).rvalue,4),\"\\nStrong growth, .64% growth in success rate a year, and a statistically significant slope.\")\n",
    "# LINEAR REGRESSION BETWEEN COMPANY AGE AND APPLICATIONS OVER TIME: MEDIUM Wage\n",
    "print(\"\\nMedium Wage Application Rate Over Time Slope:\",round(st.linregress(y = linregdf[\"Medium Apps\"], x = yr).slope,3),\", P Val:\",round(st.linregress(y = linregdf[\"Medium Apps\"], x = yr).pvalue,4),\", R Val:\",round(st.linregress(y = linregdf[\"Medium Apps\"], x = yr).rvalue,4),\"\\nModerate growth, 1688 applications a year, and not a statistically significant slope.\")\n",
    "\n",
    "# LINEAR REGRESSION BETWEEN COMPANY AGE AND SUCCESS RATE OVER TIME: OLD COMPANY\n",
    "print(\"\\n\\nHigh Wage Success Rate Over Time Slope:\",round(st.linregress(y = linregdf[\"High Success %\"], x = yr).slope,6),\", P Val:\",round(st.linregress(y = linregdf[\"High Success %\"], x = yr).pvalue,4),\", R Val:\",round(st.linregress(y = linregdf[\"High Success %\"], x = yr).rvalue,4),\"\\nStrong growth, .67% growth in success rate a year, and a statistically significant slope.\")\n",
    "# LINEAR REGRESSION BETWEEN COMPANY AGE AND APPLICATIONS OVER TIME: OLD COMPANY\n",
    "print(\"\\nHigh Wage Application Rate Over Time Slope:\",round(st.linregress(y = linregdf[\"High Apps\"], x = yr).slope,3),\", P Val:\",round(st.linregress(y = linregdf[\"High Apps\"], x = yr).pvalue,4),\", R Val:\",round(st.linregress(y = linregdf[\"High Apps\"], x = yr).rvalue,4),\"\\nModerate growth, 1575 applications a year, and not a statistically significant slope.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae3df3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING THE TOTAL PVAL\n",
    "linregdf.loc[\"ALL YRS\"] = linregdf.sum()\n",
    "youngsuc = []\n",
    "medsuc = []\n",
    "oldsuc = []\n",
    "for x in range(len(yr)):\n",
    "    y = linregdf.loc[yr[x]].iloc[0]*linregdf.loc[yr[x]].iloc[3]\n",
    "    m = linregdf.loc[yr[x]].iloc[1]*linregdf.loc[yr[x]].iloc[4]\n",
    "    o = linregdf.loc[yr[x]].iloc[2]*linregdf.loc[yr[x]].iloc[5]\n",
    "    youngsuc.append(y)\n",
    "    medsuc.append(m)\n",
    "    oldsuc.append(o)\n",
    "linregdf.loc[\"ALL YRS\"].iloc[0] = sum(youngsuc)/linregdf.loc[\"ALL YRS\"].iloc[3]\n",
    "linregdf.loc[\"ALL YRS\"].iloc[1] = sum(medsuc)/linregdf.loc[\"ALL YRS\"].iloc[4]\n",
    "linregdf.loc[\"ALL YRS\"].iloc[2] = sum(oldsuc)/linregdf.loc[\"ALL YRS\"].iloc[5]\n",
    "\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[\"ALL YRS\"].iloc[0]\n",
    "    sample2_phat = linregdf.loc[\"ALL YRS\"].iloc[1]\n",
    "    sample1_size = linregdf.loc[\"ALL YRS\"].iloc[3]\n",
    "    sample2_size = linregdf.loc[\"ALL YRS\"].iloc[4]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "linregdf.loc[\"ALL YRS\"].iloc[6] = pval\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[\"ALL YRS\"].iloc[0]\n",
    "    sample2_phat = linregdf.loc[\"ALL YRS\"].iloc[2]\n",
    "    sample1_size = linregdf.loc[\"ALL YRS\"].iloc[3]\n",
    "    sample2_size = linregdf.loc[\"ALL YRS\"].iloc[5]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "linregdf.loc[\"ALL YRS\"].iloc[7] = pval\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[\"ALL YRS\"].iloc[1]\n",
    "    sample2_phat = linregdf.loc[\"ALL YRS\"].iloc[2]\n",
    "    sample1_size = linregdf.loc[\"ALL YRS\"].iloc[4]\n",
    "    sample2_size = linregdf.loc[\"ALL YRS\"].iloc[5]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "linregdf.loc[\"ALL YRS\"].iloc[8] = pval\n",
    "linregdf = linregdf.head(9).style.format(\"{:,.2f}\")\n",
    "linregdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a90f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of people in the 66th percentile and above in Wages\n",
    "\n",
    "joby = df[df[\"WAGE_GROUP\"]==\"HIGH\"].groupby(\"WORKSITE_STATE\")[\"CASE_STATUS\"].value_counts()\n",
    "jobyer = pd.DataFrame(joby.groupby(level=0).transform(lambda x: (x / x.sum()).round(2)))\n",
    "jobyer.columns = [\"Percent\"]\n",
    "f1 = jobyer.reset_index()[\"WORKSITE_STATE\"].tolist()\n",
    "t1 = jobyer.reset_index()[\"CASE_STATUS\"].replace(1,\"Accept\").replace(0,\"Reject\").tolist()\n",
    "idx1 = []\n",
    "idx1.extend([tuple(a) for a in zip (f1,t1)])\n",
    "jobyer.index = pd.MultiIndex.from_tuples(idx1)\n",
    "jobyer = jobyer.reset_index()\n",
    "jobyer = jobyer[jobyer[\"level_1\"]==\"Accept\"]\n",
    "jobyer = jobyer.drop(\"level_1\",axis = 1)\n",
    "\n",
    "jobycount = pd.DataFrame(df[df[\"WAGE_GROUP\"]==\"HIGH\"].groupby(\"WORKSITE_STATE\")[\"CASE_STATUS\"].value_counts())\n",
    "jobycount.columns = [\"Count\"]\n",
    "jobycount = jobycount.reset_index()\n",
    "jobycount = jobycount[jobycount[\"CASE_STATUS\"]==1]\n",
    "jobycount = jobycount[\"Count\"]\n",
    "\n",
    "jobyer = pd.concat([jobyer, jobycount], axis = 1)\n",
    "jobyer.columns = [\"State\",\"Text\",\"Count\"]\n",
    "jobyer[\"Text\"] = jobyer[\"State\"].astype(str)+\": \"+jobyer[\"Text\"].astype(str)+\" Percent Successful\"\n",
    "# Choropleth Map of JOB State Accept %\n",
    "data = dict(type = \"choropleth\",\n",
    "           locations = jobyer[\"State\"],\n",
    "           locationmode = \"USA-states\",\n",
    "           z = jobyer[\"Count\"].astype(float),\n",
    "           text = jobyer[\"Text\"],\n",
    "           colorbar={\"title\":\"Count\"},\n",
    "           colorscale = \"sunset\",\n",
    "           marker = dict(line = dict(color = \"rgb(255,255,255)\")) )\n",
    "layout = dict(title = \"Distribution of Top 66 Percentile Wages\",\n",
    "              geo = dict(scope = \"usa\",showlakes = True, lakecolor = \"rgb(255,255,255)\"))\n",
    "choro = go.Figure(data, layout)\n",
    "iplot(choro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761e0c43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PW_SOC_CODE ANALYSIS\n",
    "# Top 5 PW_SOC of all time 2014-2021\n",
    "allsoc = pd.DataFrame(df[\"PW_SOC_CODE\"].value_counts()[:5]).reset_index()\n",
    "allsoc.columns = [\"ASoc\",\"COUNT ASoc\"]\n",
    "sucsoc = pd.DataFrame(df[df[\"CASE_STATUS\"]==1][\"PW_SOC_CODE\"].value_counts()[:5]).reset_index()\n",
    "sucsoc.columns = [\"SSoc\",\"COUNT SSoc\"]\n",
    "pd.concat([allsoc,sucsoc], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b113cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing the top 5 for each year PW CODE: only succesful applications\n",
    "top5semp = pd.DataFrame()\n",
    "def grab (year):\n",
    "    return df[(df[\"YEAR\"]==year)&(df[\"CASE_STATUS\"]==1)][\"PW_SOC_CODE\"].value_counts()[:5]\n",
    "for x in range(len(yr)):\n",
    "    j = pd.DataFrame(grab(yr[x]))\n",
    "    top5semp = pd.concat([top5semp,j], axis = 1)\n",
    "top5semp.columns= yr\n",
    "# King of Consistency: HOW MANY YEARS IS THIS PW CODE NOT IN THE TOP 5?: TOP CONSISTENT COMPANIES...\n",
    "kingcons = top5semp.isnull().replace(False,0).replace(True,1)\n",
    "kingcons[\"Yrs not in Top 5\"] = top5semp.isnull().replace(False,0).replace(True,1).sum(axis = 1)\n",
    "kingcons[\"Yrs not in Top 5\"].sort_values()[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785e1cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing the top 5 for each year PW CODE: total applications\n",
    "top5aemp = pd.DataFrame()\n",
    "def grab (year):\n",
    "    return df[df[\"YEAR\"]==year][\"PW_SOC_CODE\"].value_counts()[:5]\n",
    "for x in range(len(yr)):\n",
    "    j = pd.DataFrame(grab(yr[x]))\n",
    "    top5aemp = pd.concat([top5aemp,j], axis = 1)\n",
    "top5aemp.columns= yr\n",
    "# King of Consistency: HOW MANY YEARS IS THIS PW CODE NOT IN THE TOP 5?: TOP CONSISTENT COMPANIES...\n",
    "kingconsA = top5aemp.isnull().replace(False,0).replace(True,1)\n",
    "kingconsA[\"Yrs not in Top 5\"] = top5aemp.isnull().replace(False,0).replace(True,1).sum(axis = 1)\n",
    "kingconsA[\"Yrs not in Top 5\"].sort_values()[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2690e4fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DIVING DEEPER INTO THE TOP CODES!\n",
    "#ANCHORED BY STATE: PW SKILL,APPLICATIONS, MEDIAN WAGE, YRS POST GRADE, MEDIAN DAYS\n",
    "\n",
    "code = \"15-1132\"\n",
    "# CHANGE THIS FOR ANY CODE\n",
    "\n",
    "# MEDIAN WAGE, MEDIAN YEARS AFTER GRADUATING, MEDIAN DAYS TO DECIDE : FOR EACH STATE IN THE TOP 15 OF THE COMPANY AND BY PW LEVEL\n",
    "topstate = df[df[\"PW_SOC_CODE\"] == code][\"WORKSITE_STATE\"].value_counts()[:15].index.tolist()\n",
    "pwskill = df[\"PW_SKILL_LEVEL\"].dropna().unique().tolist()\n",
    "pwskill.sort()\n",
    "\n",
    "\n",
    "statelist = []\n",
    "pwlist = pwskill*len(topstate)\n",
    "apps = []\n",
    "medwage = []\n",
    "yrsgrad = []\n",
    "meddays = []\n",
    "\n",
    "\n",
    "for x in range(len(topstate)):\n",
    "    s = (topstate[x]+\",\")*len(pwskill)\n",
    "    s = \"\".join(s).split(\",\")\n",
    "    s = list(filter((\"\").__ne__, s))\n",
    "    statelist.append(s) \n",
    "for x in range(len(topstate)):\n",
    "    for j in range(len(pwskill)):\n",
    "        a = len(df[(df[\"PW_SOC_CODE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"PW_SOC_CODE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]])\n",
    "        apps.append(a)\n",
    "        b = df[(df[\"PW_SOC_CODE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"PW_SOC_CODE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]][\"WO_A\"].median()\n",
    "        medwage.append(b)\n",
    "        c = df[(df[\"PW_SOC_CODE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"PW_SOC_CODE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]][\"YRSPOSTGRAD\"].median()\n",
    "        yrsgrad.append(c)\n",
    "        d = df[(df[\"PW_SOC_CODE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"PW_SOC_CODE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]][\"DAYS_TO_DECIDE\"].median()\n",
    "        meddays.append(d)\n",
    "codestatedf = pd.DataFrame([sum(statelist, []),pwlist,apps,medwage,yrsgrad,meddays]).transpose()\n",
    "codestatedf.columns = [\"STATE\",\"PW LEVEL\", \"TOTAL APPLICATIONS\",\"MEDIAN WAGE\",\"MEDIAN YRS POST GRAD\",\"MEDIAN DAYS TO DECIDE\"]\n",
    "codestatedf = codestatedf.set_index([\"STATE\",\"PW LEVEL\"])\n",
    "codestatedf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3304360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANCHORED BY STATE: EDU LEVEL,APPLICATIONS, MEDIAN WAGE, YRS POST GRADE, MEDIAN DAYS\n",
    "# MEDIAN WAGE, MEDIAN YEARS AFTER GRADUATING, MEDIAN DAYS TO DECIDE : FOR EACH STATE IN THE TOP 15 OF THE COMPANY AND BY EDU LEVEL\n",
    "edu = df[\"FOREIGN_WORKER_EDUCATION\"].dropna().unique().tolist()\n",
    "edu[0],edu[1],edu[2],edu[3],edu[4],edu[5] = edu[3],edu[5],edu[0],edu[2],edu[1],edu[4]\n",
    "\n",
    "statelist = []\n",
    "edulist = edu*len(topstate)\n",
    "apps = []\n",
    "medwage = []\n",
    "yrsgrad = []\n",
    "meddays = []\n",
    "\n",
    "\n",
    "for x in range(len(topstate)):\n",
    "    s = (topstate[x]+\",\")*len(edu)\n",
    "    s = \"\".join(s).split(\",\")\n",
    "    s = list(filter((\"\").__ne__, s))\n",
    "    statelist.append(s) \n",
    "for x in range(len(topstate)):\n",
    "    for j in range(len(edu)):\n",
    "        a = len(df[(df[\"PW_SOC_CODE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"PW_SOC_CODE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]])\n",
    "        apps.append(a)\n",
    "        b = df[(df[\"PW_SOC_CODE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"PW_SOC_CODE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]][\"WO_A\"].median()\n",
    "        medwage.append(b)\n",
    "        c = df[(df[\"PW_SOC_CODE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"PW_SOC_CODE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]][\"YRSPOSTGRAD\"].median()\n",
    "        yrsgrad.append(c)\n",
    "        d = df[(df[\"PW_SOC_CODE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"PW_SOC_CODE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]][\"DAYS_TO_DECIDE\"].median()\n",
    "        meddays.append(d)\n",
    "codestatedf1 = pd.DataFrame([sum(statelist, []),edulist,apps,medwage,yrsgrad,meddays]).transpose()\n",
    "codestatedf1.columns = [\"STATE\",\"EDU LEVEL\", \"TOTAL APPLICATIONS\",\"MEDIAN WAGE\",\"MEDIAN YRS POST GRAD\",\"MEDIAN DAYS TO DECIDE\"]\n",
    "codestatedf1 = codestatedf1.set_index([\"STATE\",\"EDU LEVEL\"])\n",
    "codestatedf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ee6ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW COLUMN: 1 is in the top 5 of the YEAR, 0 is not: Applications\n",
    "filler = pd.DataFrame()\n",
    "for x in range(len(yr)):\n",
    "    listy = top5aemp[yr[x]].dropna().index.tolist()\n",
    "    filler = pd.concat([filler,pd.DataFrame((df[df[\"YEAR\"]==yr[x]][\"PW_SOC_CODE\"].isin(listy)).replace(True,1).replace(False,0))])\n",
    "\n",
    "filler = filler.reset_index().sort_values(by = \"index\").set_index(\"index\")\n",
    "filler.columns = [\"TOP5SOC\"]\n",
    "df[\"TOP5SOC\"] = filler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33f274e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of the states worked in if within top 5 pw code\n",
    "\n",
    "joby = df[df[\"TOP5SOC\"]==1].groupby(\"WORKSITE_STATE\")[\"CASE_STATUS\"].value_counts()\n",
    "jobyer = pd.DataFrame(joby.groupby(level=0).transform(lambda x: (x / x.sum()).round(2)))\n",
    "jobyer.columns = [\"Percent\"]\n",
    "f1 = jobyer.reset_index()[\"WORKSITE_STATE\"].tolist()\n",
    "t1 = jobyer.reset_index()[\"CASE_STATUS\"].replace(1,\"Accept\").replace(0,\"Reject\").tolist()\n",
    "idx1 = []\n",
    "idx1.extend([tuple(a) for a in zip (f1,t1)])\n",
    "jobyer.index = pd.MultiIndex.from_tuples(idx1)\n",
    "jobyer = jobyer.reset_index()\n",
    "jobyer = jobyer[jobyer[\"level_1\"]==\"Accept\"]\n",
    "jobyer = jobyer.drop(\"level_1\",axis = 1)\n",
    "\n",
    "jobycount = pd.DataFrame(df[df[\"TOP5SOC\"]==1].groupby(\"WORKSITE_STATE\")[\"CASE_STATUS\"].value_counts())\n",
    "jobycount.columns = [\"Count\"]\n",
    "jobycount = jobycount.reset_index()\n",
    "jobycount = jobycount[jobycount[\"CASE_STATUS\"]==1]\n",
    "jobycount = jobycount[\"Count\"]\n",
    "\n",
    "jobyer = pd.concat([jobyer, jobycount], axis = 1)\n",
    "jobyer.columns = [\"State\",\"Text\",\"Count\"]\n",
    "jobyer[\"Text\"] = jobyer[\"State\"].astype(str)+\": \"+jobyer[\"Text\"].astype(str)+\" Percent Successful\"\n",
    "# Choropleth Map of JOB State Accept %\n",
    "data = dict(type = \"choropleth\",\n",
    "           locations = jobyer[\"State\"],\n",
    "           locationmode = \"USA-states\",\n",
    "           z = jobyer[\"Count\"].astype(float),\n",
    "           text = jobyer[\"Text\"],\n",
    "           colorbar={\"title\":\"Count\"},\n",
    "           colorscale = \"jet\",\n",
    "           marker = dict(line = dict(color = \"rgb(255,255,255)\")) )\n",
    "layout = dict(title = \"Distribution of Top 5 SOC Code\",\n",
    "              geo = dict(scope = \"usa\",showlakes = True, lakecolor = \"rgb(255,255,255)\"))\n",
    "choro = go.Figure(data, layout)\n",
    "iplot(choro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d69b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRELATION ANALYSIS: TOP 5 PW CODE WITH CASE STATUS: CASE BY CASE\n",
    "st.linregress(x = df.dropna()[\"TOP5SOC\"], y = df.dropna()[\"CASE_STATUS\"])\n",
    "# DOES NOT SHOW MUCH, AGGREGATE MAY SHOW A DIFFERENT STORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e80a387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING THE DIFFERENT SUCCESS RATES OVER TIME BY TOP 15 COMPANY (IN TERMS OF ANNUAL APPLICATIONS)\n",
    "top15 = df[df[\"TOP5SOC\"]==1]\n",
    "not15 = df[df[\"TOP5SOC\"]==0]\n",
    "\n",
    "topdf = pd.DataFrame()\n",
    "notdf = pd.DataFrame()\n",
    "analysisdf = pd.DataFrame()\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    j = pd.DataFrame([len(top15[(top15[\"YEAR\"]==yr[x])&(top15[\"CASE_STATUS\"]==1)])/len(top15[top15[\"YEAR\"]==yr[x]])])\n",
    "    b = pd.DataFrame([len(not15[(not15[\"YEAR\"]==yr[x])&(not15[\"CASE_STATUS\"]==1)])/len(not15[not15[\"YEAR\"]==yr[x]])])\n",
    "    topdf = pd.concat([topdf,j])\n",
    "    notdf = pd.concat([notdf,b])\n",
    "    analysisdf = pd.concat([topdf,notdf], axis = 1)\n",
    "\n",
    "analysisdf.index = yr\n",
    "analysisdf.columns = [\"Top 5 Success %\", \"Not 5 Success %\"]\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "analysisdf.plot(kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Success Rates by Applicants in the Top 5 and not in the Top 5 Job Codes, Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6751bae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARING DATA FOR LIN REGRESS AND GETTING P VALUE\n",
    "top15apps = []\n",
    "not15apps = []\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    t = (top15[\"YEAR\"]==yr[x]).sum()\n",
    "    n = (not15[\"YEAR\"]==yr[x]).sum()\n",
    "    top15apps.append(t)\n",
    "    not15apps.append(n)\n",
    "analysisdf = analysisdf.reset_index()\n",
    "analysisdf[\"Top 5 Apps\"] = pd.DataFrame(top15apps)\n",
    "analysisdf[\"Not 5 Apps\"] = pd.DataFrame(not15apps)\n",
    "analysisdf = analysisdf.set_index(\"index\")\n",
    "# PLOTTING APPLICATION RATES OVER TIME BY TOP 15 or NOT TOP 15\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "analysisdf.reset_index().plot(x = \"index\", y = [\"Top 5 Apps\",\"Not 5 Apps\"], kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Application Rates by Companies in the Top 5 and not in the Top 5 Job Codes, Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73523789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P VALUE PER YEAR: ON THE SUCCESS RATE\n",
    "top_not = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = analysisdf.loc[yr[x]][\"Top 5 Success %\"]\n",
    "    sample2_phat = analysisdf.loc[yr[x]][\"Not 5 Success %\"]\n",
    "    sample1_size = analysisdf.loc[yr[x]][\"Top 5 Apps\"]\n",
    "    sample2_size = analysisdf.loc[yr[x]][\"Not 5 Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    top_not.append(pval)\n",
    "\n",
    "pvaldf = pd.DataFrame(top_not)\n",
    "pvaldf.columns = [\"Top 5 Vs. Not Top 5\"]\n",
    "pvaldf.index = yr\n",
    "analysisdf[\"PVAL Success %\"] = pvaldf[\"Top 5 Vs. Not Top 5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fd3363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION BETWEEN Code AND SUCCESS RATE OVER TIME: TOP 5 Code\n",
    "print(\"Top 5 Code Success Rate Over Time Slope:\",round(st.linregress(y = analysisdf[\"Top 5 Success %\"], x = yr).slope,6),\", P Val:\",round(st.linregress(y = analysisdf[\"Top 5 Success %\"], x = yr).pvalue,4),\", R Val:\",round(st.linregress(y = analysisdf[\"Top 5 Success %\"], x = yr).rvalue,4),\"\\nStrong growth, .56% growth in success rate a year, and a statistically significant slope.\")\n",
    "# LINEAR REGRESSION BETWEEN Code AND APPLICATIONS OVER TIME: TOP 5 Code\n",
    "print(\"\\nTop 5 Code Application Rate Over Time Slope:\",round(st.linregress(y = analysisdf[\"Top 5 Apps\"], x = yr).slope,3),\", P Val:\",round(st.linregress(y = analysisdf[\"Top 5 Apps\"], x = yr).pvalue,4),\", R Val:\",round(st.linregress(y = analysisdf[\"Top 5 Apps\"], x = yr).rvalue,4),\"\\nModerate growth, 2326 applications a year, but not a statistically significant slope.\")\n",
    "\n",
    "# LINEAR REGRESSION BETWEEN Code AND SUCCESS RATE OVER TIME: NOT TOP 5 Code\n",
    "print(\"\\n\\nNot Top 5 Code Success Rate Over Time Slope:\",round(st.linregress(y = analysisdf[\"Not 5 Success %\"], x = yr).slope,6),\", P Val:\",round(st.linregress(y = analysisdf[\"Not 5 Success %\"], x = yr).pvalue,4),\", R Val:\",round(st.linregress(y = analysisdf[\"Not 5 Success %\"], x = yr).rvalue,4),\"\\nStrong growth, .79% growth in success rate a year, and a statistically significant slope.\")\n",
    "# LINEAR REGRESSION BETWEEN Code AND APPLICATIONS OVER TIME: NOT TOP 5 Code\n",
    "print(\"\\nNot Top 5 Code Application Rate Over Time Slope:\",round(st.linregress(y = analysisdf[\"Not 5 Apps\"], x = yr).slope,3),\", P Val:\",round(st.linregress(y = analysisdf[\"Not 5 Apps\"], x = yr).pvalue,4),\", R Val:\",round(st.linregress(y = analysisdf[\"Not 5 Apps\"], x = yr).rvalue,4),\"\\nModerate growth, 2570 applications a year, but not a statistically significant slope.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430e2400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING THE TOTAL PVAL\n",
    "analysisdf.loc[\"ALL YRS\"] = analysisdf.sum()\n",
    "top15suc = []\n",
    "not15suc = []\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    t15 = analysisdf.loc[yr[x]].iloc[0]*analysisdf.loc[yr[x]].iloc[2]\n",
    "    n15 = analysisdf.loc[yr[x]].iloc[1]*analysisdf.loc[yr[x]].iloc[3]\n",
    "    top15suc.append(t15)\n",
    "    not15suc.append(n15)\n",
    "analysisdf.loc[\"ALL YRS\"].iloc[0] = sum(top15suc)/analysisdf.loc[\"ALL YRS\"].iloc[2]\n",
    "analysisdf.loc[\"ALL YRS\"].iloc[1] = sum(not15suc)/analysisdf.loc[\"ALL YRS\"].iloc[3]\n",
    "\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = analysisdf.loc[\"ALL YRS\"].iloc[0]\n",
    "    sample2_phat = analysisdf.loc[\"ALL YRS\"].iloc[1]\n",
    "    sample1_size = analysisdf.loc[\"ALL YRS\"].iloc[2]\n",
    "    sample2_size = analysisdf.loc[\"ALL YRS\"].iloc[3]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "analysisdf.loc[\"ALL YRS\"].iloc[4] = pval\n",
    "\n",
    "analysisdf = analysisdf.head(9).style.format(\"{:,.2f}\")\n",
    "analysisdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395da0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 WORK STATE of all time 2014-2021\n",
    "allsoc = pd.DataFrame(df[\"WORKSITE_STATE\"].value_counts()[:10]).reset_index()\n",
    "allsoc.columns = [\"AState\",\"COUNT AState\"]\n",
    "sucsoc = pd.DataFrame(df[df[\"CASE_STATUS\"]==1][\"WORKSITE_STATE\"].value_counts()[:10]).reset_index()\n",
    "sucsoc.columns = [\"SState\",\"COUNT SState\"]\n",
    "pd.concat([allsoc,sucsoc], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567e77fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACCEPT % BASED ON STATE WORKED IN\n",
    "\n",
    "joby = df.groupby(\"WORKSITE_STATE\")[\"CASE_STATUS\"].value_counts()\n",
    "jobyer = pd.DataFrame(joby.groupby(level=0).transform(lambda x: (x / x.sum()).round(2)))\n",
    "jobyer.columns = [\"Percent\"]\n",
    "f1 = jobyer.reset_index()[\"WORKSITE_STATE\"].tolist()\n",
    "t1 = jobyer.reset_index()[\"CASE_STATUS\"].replace(1,\"Accept\").replace(0,\"Reject\").tolist()\n",
    "idx1 = []\n",
    "idx1.extend([tuple(a) for a in zip (f1,t1)])\n",
    "jobyer.index = pd.MultiIndex.from_tuples(idx1)\n",
    "jobyer = jobyer.reset_index()\n",
    "jobyer = jobyer[jobyer[\"level_1\"]==\"Accept\"]\n",
    "jobyer = jobyer.drop(\"level_1\",axis = 1)\n",
    "\n",
    "jobycount = pd.DataFrame(df.groupby(\"WORKSITE_STATE\")[\"CASE_STATUS\"].value_counts())\n",
    "jobycount.columns = [\"Count\"]\n",
    "jobycount = jobycount.reset_index()\n",
    "jobycount = jobycount[jobycount[\"CASE_STATUS\"]==1]\n",
    "jobycount = jobycount[\"Count\"]\n",
    "\n",
    "jobyer = pd.concat([jobyer, jobycount], axis = 1)\n",
    "jobyer.columns = [\"State\",\"Percent\",\"Count\"]\n",
    "jobyer[\"Count\"] = jobyer[\"State\"].astype(str)+\": \"+jobyer[\"Count\"].astype(str)+\" Successful Applications\"\n",
    "# Choropleth Map of JOB State Accept %\n",
    "data = dict(type = \"choropleth\",\n",
    "           locations = jobyer[\"State\"],\n",
    "           locationmode = \"USA-states\",\n",
    "           z = jobyer[\"Percent\"].astype(float),\n",
    "           text = jobyer[\"Count\"],\n",
    "           colorbar={\"title\":\"Percent Accepted\"},\n",
    "           colorscale = \"earth\",\n",
    "           marker = dict(line = dict(color = \"rgb(255,255,255)\")) )\n",
    "layout = dict(title = \"Percent Accepted in States Worked In\",\n",
    "              geo = dict(scope = \"usa\",showlakes = True, lakecolor = \"rgb(255,255,255)\"))\n",
    "choro = go.Figure(data, layout)\n",
    "iplot(choro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822398a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing the top 5 for each year state worked in: total applications\n",
    "top5aemp = pd.DataFrame()\n",
    "def grab (year):\n",
    "    return df[df[\"YEAR\"]==year][\"WORKSITE_STATE\"].value_counts()[:5]\n",
    "for x in range(len(yr)):\n",
    "    j = pd.DataFrame(grab(yr[x]))\n",
    "    top5aemp = pd.concat([top5aemp,j], axis = 1)\n",
    "top5aemp.columns= yr\n",
    "# King of Consistency: HOW MANY YEARS IS THIS state NOT IN THE TOP 5?: TOP CONSISTENT states...\n",
    "kingconsA = top5aemp.isnull().replace(False,0).replace(True,1)\n",
    "kingconsA[\"Yrs not in Top 5\"] = top5aemp.isnull().replace(False,0).replace(True,1).sum(axis = 1)\n",
    "kingconsA[\"Yrs not in Top 5\"].sort_values()[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d593f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW COLUMN: 1 is in the top 5 of the YEAR, 0 is not: Applications\n",
    "filler = pd.DataFrame()\n",
    "for x in range(len(yr)):\n",
    "    listy = top5aemp[yr[x]].dropna().index.tolist()\n",
    "    filler = pd.concat([filler,pd.DataFrame((df[df[\"YEAR\"]==yr[x]][\"WORKSITE_STATE\"].isin(listy)).replace(True,1).replace(False,0))])\n",
    "\n",
    "filler = filler.reset_index().sort_values(by = \"index\").set_index(\"index\")\n",
    "filler.columns = [\"TOP5WORKSTATE\"]\n",
    "df[\"TOP5WORKSTATE\"] = filler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f301a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRELATION ANALYSIS: TOP 5 WORK STATE WITH CASE STATUS: CASE BY CASE\n",
    "st.linregress(x = df.dropna()[\"TOP5WORKSTATE\"], y = df.dropna()[\"CASE_STATUS\"])\n",
    "# DOES NOT SHOW MUCH, AGGREGATE MAY SHOW A DIFFERENT STORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89778b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING THE DIFFERENT SUCCESS RATES OVER TIME BY TOP 5 COMPANY (IN TERMS OF ANNUAL APPLICATIONS)\n",
    "top15 = df[df[\"TOP5WORKSTATE\"]==1]\n",
    "not15 = df[df[\"TOP5WORKSTATE\"]==0]\n",
    "\n",
    "topdf = pd.DataFrame()\n",
    "notdf = pd.DataFrame()\n",
    "analysisdf = pd.DataFrame()\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    j = pd.DataFrame([len(top15[(top15[\"YEAR\"]==yr[x])&(top15[\"CASE_STATUS\"]==1)])/len(top15[top15[\"YEAR\"]==yr[x]])])\n",
    "    b = pd.DataFrame([len(not15[(not15[\"YEAR\"]==yr[x])&(not15[\"CASE_STATUS\"]==1)])/len(not15[not15[\"YEAR\"]==yr[x]])])\n",
    "    topdf = pd.concat([topdf,j])\n",
    "    notdf = pd.concat([notdf,b])\n",
    "    analysisdf = pd.concat([topdf,notdf], axis = 1)\n",
    "\n",
    "analysisdf.index = yr\n",
    "analysisdf.columns = [\"Top 5 Success %\", \"Not 5 Success %\"]\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "analysisdf.plot(kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Success Rates by Applicants in the Top 5 and not in the Top 5 States, Over Time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29d11b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARING DATA FOR LIN REGRESS AND GETTING P VALUE\n",
    "top15apps = []\n",
    "not15apps = []\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    t = (top15[\"YEAR\"]==yr[x]).sum()\n",
    "    n = (not15[\"YEAR\"]==yr[x]).sum()\n",
    "    top15apps.append(t)\n",
    "    not15apps.append(n)\n",
    "analysisdf = analysisdf.reset_index()\n",
    "analysisdf[\"Top 5 Apps\"] = pd.DataFrame(top15apps)\n",
    "analysisdf[\"Not 5 Apps\"] = pd.DataFrame(not15apps)\n",
    "analysisdf = analysisdf.set_index(\"index\")\n",
    "# PLOTTING APPLICATION RATES OVER TIME BY TOP 5 or NOT TOP 5\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "analysisdf.reset_index().plot(x = \"index\", y = [\"Top 5 Apps\",\"Not 5 Apps\"], kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Application Rates by Companies in the Top 5 and not in the Top 5 States, Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980a966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P VALUE PER YEAR: ON THE SUCCESS RATE\n",
    "top_not = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = analysisdf.loc[yr[x]][\"Top 5 Success %\"]\n",
    "    sample2_phat = analysisdf.loc[yr[x]][\"Not 5 Success %\"]\n",
    "    sample1_size = analysisdf.loc[yr[x]][\"Top 5 Apps\"]\n",
    "    sample2_size = analysisdf.loc[yr[x]][\"Not 5 Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    top_not.append(pval)\n",
    "\n",
    "pvaldf = pd.DataFrame(top_not)\n",
    "pvaldf.columns = [\"Top 5 Vs. Not Top 5\"]\n",
    "pvaldf.index = yr\n",
    "analysisdf[\"PVAL Success %\"] = pvaldf[\"Top 5 Vs. Not Top 5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4781fd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION BETWEEN TOP STATES AND SUCCESS RATE OVER TIME: TOP 5 STATES\n",
    "print(\"top 5 success rate over time:\",st.linregress(y = analysisdf[\"Top 5 Success %\"], x = yr))\n",
    "print(\"\\ntop 5 applications over time:\",st.linregress(y = analysisdf[\"Top 5 Apps\"], x = yr))\n",
    "\n",
    "print(\"\\n\\nnottop 5 success rate over time:\",st.linregress(y = analysisdf[\"Not 5 Success %\"], x = yr))\n",
    "print(\"\\nnot top 5 applications over time:\",st.linregress(y = analysisdf[\"Not 5 Apps\"], x = yr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e403ce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING THE TOTAL PVAL\n",
    "analysisdf.loc[\"ALL YRS\"] = analysisdf.sum()\n",
    "top15suc = []\n",
    "not15suc = []\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    t15 = analysisdf.loc[yr[x]].iloc[0]*analysisdf.loc[yr[x]].iloc[2]\n",
    "    n15 = analysisdf.loc[yr[x]].iloc[1]*analysisdf.loc[yr[x]].iloc[3]\n",
    "    top15suc.append(t15)\n",
    "    not15suc.append(n15)\n",
    "analysisdf.loc[\"ALL YRS\"].iloc[0] = sum(top15suc)/analysisdf.loc[\"ALL YRS\"].iloc[2]\n",
    "analysisdf.loc[\"ALL YRS\"].iloc[1] = sum(not15suc)/analysisdf.loc[\"ALL YRS\"].iloc[3]\n",
    "\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = analysisdf.loc[\"ALL YRS\"].iloc[0]\n",
    "    sample2_phat = analysisdf.loc[\"ALL YRS\"].iloc[1]\n",
    "    sample1_size = analysisdf.loc[\"ALL YRS\"].iloc[2]\n",
    "    sample2_size = analysisdf.loc[\"ALL YRS\"].iloc[3]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "analysisdf.loc[\"ALL YRS\"].iloc[4] = pval\n",
    "\n",
    "analysisdf = analysisdf.head(9).style.format(\"{:,.2f}\")\n",
    "analysisdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b63082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 WORK Cities of all time 2014-2021\n",
    "allsoc = pd.DataFrame(df[\"WORKSITE_CITY\"].value_counts()[:10]).reset_index()\n",
    "allsoc.columns = [\"ACity\",\"COUNT ACity\"]\n",
    "sucsoc = pd.DataFrame(df[df[\"CASE_STATUS\"]==1][\"WORKSITE_CITY\"].value_counts()[:10]).reset_index()\n",
    "sucsoc.columns = [\"SCity\",\"COUNT SCity\"]\n",
    "pd.concat([allsoc,sucsoc], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81b7943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing the top 5 for each year state worked in: total applications\n",
    "top5aemp = pd.DataFrame()\n",
    "def grab (year):\n",
    "    return df[df[\"YEAR\"]==year][\"WORKSITE_CITY\"].value_counts()[:10]\n",
    "for x in range(len(yr)):\n",
    "    j = pd.DataFrame(grab(yr[x]))\n",
    "    top5aemp = pd.concat([top5aemp,j], axis = 1)\n",
    "top5aemp.columns= yr\n",
    "# King of Consistency: HOW MANY YEARS IS THIS state NOT IN THE TOP 5?: TOP CONSISTENT states...\n",
    "kingconsA = top5aemp.isnull().replace(False,0).replace(True,1)\n",
    "kingconsA[\"Yrs not in Top 10\"] = top5aemp.isnull().replace(False,0).replace(True,1).sum(axis = 1)\n",
    "kingconsA[\"Yrs not in Top 10\"].sort_values()[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7798a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW COLUMN: 1 is in the top 5 of the YEAR, 0 is not: Applications\n",
    "filler = pd.DataFrame()\n",
    "for x in range(len(yr)):\n",
    "    listy = top5aemp[yr[x]].dropna().index.tolist()\n",
    "    filler = pd.concat([filler,pd.DataFrame((df[df[\"YEAR\"]==yr[x]][\"WORKSITE_CITY\"].isin(listy)).replace(True,1).replace(False,0))])\n",
    "\n",
    "filler = filler.reset_index().sort_values(by = \"index\").set_index(\"index\")\n",
    "filler.columns = [\"TOP10WORKCITY\"]\n",
    "df[\"TOP10WORKCITY\"] = filler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c539d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRELATION ANALYSIS: TOP 10 WORK CITY WITH CASE STATUS: CASE BY CASE\n",
    "st.linregress(x = df.dropna()[\"TOP10WORKCITY\"], y = df.dropna()[\"CASE_STATUS\"])\n",
    "# DOES NOT SHOW MUCH, AGGREGATE MAY SHOW A DIFFERENT STORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91efb6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING THE DIFFERENT SUCCESS RATES OVER TIME BY TOP 10 COMPANY (IN TERMS OF ANNUAL APPLICATIONS)\n",
    "top15 = df[df[\"TOP10WORKCITY\"]==1]\n",
    "not15 = df[df[\"TOP10WORKCITY\"]==0]\n",
    "\n",
    "topdf = pd.DataFrame()\n",
    "notdf = pd.DataFrame()\n",
    "analysisdf = pd.DataFrame()\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    j = pd.DataFrame([len(top15[(top15[\"YEAR\"]==yr[x])&(top15[\"CASE_STATUS\"]==1)])/len(top15[top15[\"YEAR\"]==yr[x]])])\n",
    "    b = pd.DataFrame([len(not15[(not15[\"YEAR\"]==yr[x])&(not15[\"CASE_STATUS\"]==1)])/len(not15[not15[\"YEAR\"]==yr[x]])])\n",
    "    topdf = pd.concat([topdf,j])\n",
    "    notdf = pd.concat([notdf,b])\n",
    "    analysisdf = pd.concat([topdf,notdf], axis = 1)\n",
    "\n",
    "analysisdf.index = yr\n",
    "analysisdf.columns = [\"Top 10 Success %\", \"Not 10 Success %\"]\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "analysisdf.plot(kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Success Rates by Applicants in the Top 10 and not in the Top 10 Cities, Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7aea43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARING DATA FOR LIN REGRESS AND GETTING P VALUE\n",
    "top15apps = []\n",
    "not15apps = []\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    t = (top15[\"YEAR\"]==yr[x]).sum()\n",
    "    n = (not15[\"YEAR\"]==yr[x]).sum()\n",
    "    top15apps.append(t)\n",
    "    not15apps.append(n)\n",
    "analysisdf = analysisdf.reset_index()\n",
    "analysisdf[\"Top 10 Apps\"] = pd.DataFrame(top15apps)\n",
    "analysisdf[\"Not 10 Apps\"] = pd.DataFrame(not15apps)\n",
    "analysisdf = analysisdf.set_index(\"index\")\n",
    "# PLOTTING APPLICATION RATES OVER TIME BY TOP 10 or NOT TOP 10\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "analysisdf.reset_index().plot(x = \"index\", y = [\"Top 10 Apps\",\"Not 10 Apps\"], kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Application Rates by Companies in the Top 10 and not in the Top 10 Cities, Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c215ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P VALUE PER YEAR: ON THE SUCCESS RATE\n",
    "top_not = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = analysisdf.loc[yr[x]][\"Top 10 Success %\"]\n",
    "    sample2_phat = analysisdf.loc[yr[x]][\"Not 10 Success %\"]\n",
    "    sample1_size = analysisdf.loc[yr[x]][\"Top 10 Apps\"]\n",
    "    sample2_size = analysisdf.loc[yr[x]][\"Not 10 Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    top_not.append(pval)\n",
    "\n",
    "pvaldf = pd.DataFrame(top_not)\n",
    "pvaldf.columns = [\"Top 10 Vs. Not Top 10\"]\n",
    "pvaldf.index = yr\n",
    "analysisdf[\"PVAL Success %\"] = pvaldf[\"Top 10 Vs. Not Top 10\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3b4dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION BETWEEN TOP CITIES AND SUCCESS RATE OVER TIME: TOP 10 CITIES\n",
    "print(\"top 10 success rate over time:\",st.linregress(y = analysisdf[\"Top 10 Success %\"], x = yr))\n",
    "print(\"\\ntop 10 applications over time:\",st.linregress(y = analysisdf[\"Top 10 Apps\"], x = yr))\n",
    "\n",
    "print(\"\\n\\nnot top 10 success rate over time:\",st.linregress(y = analysisdf[\"Not 10 Success %\"], x = yr))\n",
    "print(\"\\nnot top 10 applications over time:\",st.linregress(y = analysisdf[\"Not 10 Apps\"], x = yr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d67fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING THE TOTAL PVAL\n",
    "analysisdf.loc[\"ALL YRS\"] = analysisdf.sum()\n",
    "top15suc = []\n",
    "not15suc = []\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    t15 = analysisdf.loc[yr[x]].iloc[0]*analysisdf.loc[yr[x]].iloc[2]\n",
    "    n15 = analysisdf.loc[yr[x]].iloc[1]*analysisdf.loc[yr[x]].iloc[3]\n",
    "    top15suc.append(t15)\n",
    "    not15suc.append(n15)\n",
    "analysisdf.loc[\"ALL YRS\"].iloc[0] = sum(top15suc)/analysisdf.loc[\"ALL YRS\"].iloc[2]\n",
    "analysisdf.loc[\"ALL YRS\"].iloc[1] = sum(not15suc)/analysisdf.loc[\"ALL YRS\"].iloc[3]\n",
    "\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = analysisdf.loc[\"ALL YRS\"].iloc[0]\n",
    "    sample2_phat = analysisdf.loc[\"ALL YRS\"].iloc[1]\n",
    "    sample1_size = analysisdf.loc[\"ALL YRS\"].iloc[2]\n",
    "    sample2_size = analysisdf.loc[\"ALL YRS\"].iloc[3]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "analysisdf.loc[\"ALL YRS\"].iloc[4] = pval\n",
    "\n",
    "analysisdf = analysisdf.head(9).style.format(\"{:,.2f}\")\n",
    "analysisdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4966d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING A NEW COLUMN: SPLITTING THE REQUIRED EDUCATION LEVEL INTO 4 GROUPS\n",
    "df[\"REQ_EDU_GROUP\"] = np.nan\n",
    "for x in range(len(yr)):\n",
    "    df[\"REQ_EDU_GROUP\"] = np.where((df[\"MINIMUM_EDUCATION\"]==\"HIGH SCHOOL\")|(df[\"MINIMUM_EDUCATION\"]==\"ASSOCIATE'S\"),\"LOWER\",df[\"REQ_EDU_GROUP\"])\n",
    "    df[\"REQ_EDU_GROUP\"] = np.where(df[\"MINIMUM_EDUCATION\"]==\"BACHELOR'S\",\"COLLEGE\",df[\"REQ_EDU_GROUP\"])\n",
    "    df[\"REQ_EDU_GROUP\"] = np.where((df[\"MINIMUM_EDUCATION\"]==\"MASTER'S\")|(df[\"MINIMUM_EDUCATION\"]==\"DOCTORATE\"),\"UPPER\",df[\"REQ_EDU_GROUP\"])\n",
    "    df[\"REQ_EDU_GROUP\"] = np.where((df[\"MINIMUM_EDUCATION\"].isnull())|(df[\"MINIMUM_EDUCATION\"]==\"OTHER\"),\"OTHER\",df[\"REQ_EDU_GROUP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf90bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MINIMUM EDUCATION REQUIRED ANALYSIS\n",
    "# PLOTTING SUCCESS RATES OVER TIME BY MINIMUM EDUCATION REQUIRED\n",
    "\n",
    "lower = df[df[\"REQ_EDU_GROUP\"]==\"LOWER\"]\n",
    "college = df[df[\"REQ_EDU_GROUP\"]==\"COLLEGE\"]\n",
    "upper = df[df[\"REQ_EDU_GROUP\"]==\"UPPER\"]\n",
    "other = df[df[\"REQ_EDU_GROUP\"]==\"OTHER\"]\n",
    "\n",
    "\n",
    "\n",
    "lowerdf = pd.DataFrame()\n",
    "collegedf = pd.DataFrame()\n",
    "upperdf = pd.DataFrame()\n",
    "otherdf = pd.DataFrame()\n",
    "linregdf = pd.DataFrame()\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    lowe = pd.DataFrame([len(lower[(lower[\"YEAR\"]==yr[x])&(lower[\"CASE_STATUS\"]==1)])/len(lower[lower[\"YEAR\"]==yr[x]])])\n",
    "    coll = pd.DataFrame([len(college[(college[\"YEAR\"]==yr[x])&(college[\"CASE_STATUS\"]==1)])/len(college[college[\"YEAR\"]==yr[x]])])\n",
    "    upp = pd.DataFrame([len(upper[(upper[\"YEAR\"]==yr[x])&(upper[\"CASE_STATUS\"]==1)])/len(upper[upper[\"YEAR\"]==yr[x]])])\n",
    "    ott = pd.DataFrame([len(other[(other[\"YEAR\"]==yr[x])&(other[\"CASE_STATUS\"]==1)])/len(other[other[\"YEAR\"]==yr[x]])])\n",
    "    lowerdf = pd.concat([lowerdf,lowe])\n",
    "    collegedf = pd.concat([collegedf,coll])\n",
    "    upperdf = pd.concat([upperdf,upp])\n",
    "    otherdf = pd.concat([otherdf, ott])\n",
    "    linregdf = pd.concat([lowerdf,collegedf,upperdf,otherdf], axis = 1)\n",
    "    \n",
    "\n",
    "linregdf.index = yr\n",
    "linregdf.columns = [\"Lower Success %\", \"College Success %\", \"Upper Success %\",\"Other Success %\"]\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "linregdf.plot(kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Success Rates by Minimum Required Education, Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa99c8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARING DATA FOR LIN REGRESS AND GETTING P VALUE\n",
    "lowerapps = []\n",
    "collegeapps = []\n",
    "upperapps = []\n",
    "otherapps = []\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    l = (lower[\"YEAR\"]==yr[x]).sum()\n",
    "    c = (college[\"YEAR\"]==yr[x]).sum()\n",
    "    u = (upper[\"YEAR\"]==yr[x]).sum()\n",
    "    o = (other[\"YEAR\"]==yr[x]).sum()\n",
    "    lowerapps.append(l)\n",
    "    collegeapps.append(c)\n",
    "    upperapps.append(u)\n",
    "    otherapps.append(o)\n",
    "linregdf = linregdf.reset_index()\n",
    "linregdf[\"Lower Apps\"] = pd.DataFrame(lowerapps)\n",
    "linregdf[\"College Apps\"] = pd.DataFrame(collegeapps)\n",
    "linregdf[\"Upper Apps\"] = pd.DataFrame(upperapps)\n",
    "linregdf[\"Other Apps\"] = pd.DataFrame(otherapps)\n",
    "linregdf = linregdf.set_index(\"index\")\n",
    "# PLOTTING APPLICATION RATES OVER TIME BY EDUCATION LEVEL\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "linregdf.reset_index().plot(x = \"index\", y = [\"Lower Apps\",\"College Apps\",\"Upper Apps\",\"Other Apps\"], kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Application Rates by Minimum Required Education, Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3d64bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P VALUE PER YEAR\n",
    "lower_college = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[yr[x]][\"Lower Success %\"]\n",
    "    sample2_phat = linregdf.loc[yr[x]][\"College Success %\"]\n",
    "    sample1_size = linregdf.loc[yr[x]][\"Lower Apps\"]\n",
    "    sample2_size = linregdf.loc[yr[x]][\"College Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    lower_college.append(pval)\n",
    "\n",
    "pvaldf = pd.DataFrame(lower_college)\n",
    "\n",
    "lower_upper = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[yr[x]][\"Lower Success %\"]\n",
    "    sample2_phat = linregdf.loc[yr[x]][\"Upper Success %\"]\n",
    "    sample1_size = linregdf.loc[yr[x]][\"Lower Apps\"]\n",
    "    sample2_size = linregdf.loc[yr[x]][\"Upper Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    lower_upper.append(pval)\n",
    "pvaldf = pd.concat([pvaldf,pd.DataFrame(lower_upper)], axis = 1)\n",
    "\n",
    "lower_other = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[yr[x]][\"Lower Success %\"]\n",
    "    sample2_phat = linregdf.loc[yr[x]][\"Other Success %\"]\n",
    "    sample1_size = linregdf.loc[yr[x]][\"Lower Apps\"]\n",
    "    sample2_size = linregdf.loc[yr[x]][\"Other Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    lower_other.append(pval)\n",
    "pvaldf = pd.concat([pvaldf,pd.DataFrame(lower_other)], axis = 1)\n",
    "\n",
    "college_upper = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[yr[x]][\"College Success %\"]\n",
    "    sample2_phat = linregdf.loc[yr[x]][\"Upper Success %\"]\n",
    "    sample1_size = linregdf.loc[yr[x]][\"College Apps\"]\n",
    "    sample2_size = linregdf.loc[yr[x]][\"Upper Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    college_upper.append(pval)\n",
    "pvaldf = pd.concat([pvaldf,pd.DataFrame(college_upper)], axis = 1)\n",
    "\n",
    "college_other = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[yr[x]][\"College Success %\"]\n",
    "    sample2_phat = linregdf.loc[yr[x]][\"Other Success %\"]\n",
    "    sample1_size = linregdf.loc[yr[x]][\"College Apps\"]\n",
    "    sample2_size = linregdf.loc[yr[x]][\"Other Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    college_other.append(pval)\n",
    "pvaldf = pd.concat([pvaldf,pd.DataFrame(college_other)], axis = 1)\n",
    "\n",
    "upper_other = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[yr[x]][\"Upper Success %\"]\n",
    "    sample2_phat = linregdf.loc[yr[x]][\"Other Success %\"]\n",
    "    sample1_size = linregdf.loc[yr[x]][\"Upper Apps\"]\n",
    "    sample2_size = linregdf.loc[yr[x]][\"Other Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    upper_other.append(pval)\n",
    "pvaldf = pd.concat([pvaldf,pd.DataFrame(upper_other)], axis = 1)\n",
    "\n",
    "pvaldf.columns = [\"Lower_College\",\"Lower_Upper\",\"Lower_Other\",\"College_Upper\",\"College_Other\",\"Upper_Other\"]\n",
    "pvaldf.index = yr\n",
    "linregdf[[\"Lower_College\",\"Lower_Upper\",\"Lower_Other\",\"College_Upper\",\"College_Other\",\"Upper_Other\"]] = pvaldf[[\"Lower_College\",\"Lower_Upper\",\"Lower_Other\",\"College_Upper\",\"College_Other\",\"Upper_Other\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb16b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION BETWEEN Minimum education required and success\n",
    "print(\"Lower success rate over time:\",st.linregress(y = linregdf[\"Lower Success %\"], x = yr))\n",
    "print(\"\\nLower applications over time:\",st.linregress(y = linregdf[\"Lower Apps\"], x = yr))\n",
    "\n",
    "print(\"\\n\\nCollege success rate over time:\",st.linregress(y = linregdf[\"College Success %\"], x = yr))\n",
    "print(\"\\nCollege applications over time:\",st.linregress(y = linregdf[\"College Apps\"], x = yr))\n",
    "\n",
    "print(\"\\n\\nUpper success rate over time:\",st.linregress(y = linregdf[\"Upper Success %\"], x = yr))\n",
    "print(\"\\nUpper applications over time:\",st.linregress(y = linregdf[\"Upper Apps\"], x = yr))\n",
    "\n",
    "print(\"\\n\\nOther success rate over time:\",st.linregress(y = linregdf[\"Other Success %\"], x = yr))\n",
    "print(\"\\nOther applications over time:\",st.linregress(y = linregdf[\"Other Apps\"], x = yr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efce28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING THE TOTAL PVAL\n",
    "linregdf.loc[\"ALL YRS\"] = linregdf.sum()\n",
    "lowersuc = []\n",
    "collegesuc = []\n",
    "uppersuc = []\n",
    "othersuc = []\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    l = linregdf.loc[yr[x]].iloc[0]*linregdf.loc[yr[x]].iloc[4]\n",
    "    c = linregdf.loc[yr[x]].iloc[1]*linregdf.loc[yr[x]].iloc[5]\n",
    "    u = linregdf.loc[yr[x]].iloc[2]*linregdf.loc[yr[x]].iloc[6]\n",
    "    o = linregdf.loc[yr[x]].iloc[3]*linregdf.loc[yr[x]].iloc[7]\n",
    "    lowersuc.append(l)\n",
    "    collegesuc.append(c)\n",
    "    uppersuc.append(u)\n",
    "    othersuc.append(o)\n",
    "    \n",
    "linregdf.loc[\"ALL YRS\"].iloc[0] = sum(lowersuc)/linregdf.loc[\"ALL YRS\"].iloc[4]\n",
    "linregdf.loc[\"ALL YRS\"].iloc[1] = sum(collegesuc)/linregdf.loc[\"ALL YRS\"].iloc[5]\n",
    "linregdf.loc[\"ALL YRS\"].iloc[2] = sum(uppersuc)/linregdf.loc[\"ALL YRS\"].iloc[6]\n",
    "linregdf.loc[\"ALL YRS\"].iloc[3] = sum(othersuc)/linregdf.loc[\"ALL YRS\"].iloc[7]\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[\"ALL YRS\"].iloc[0]\n",
    "    sample2_phat = linregdf.loc[\"ALL YRS\"].iloc[1]\n",
    "    sample1_size = linregdf.loc[\"ALL YRS\"].iloc[4]\n",
    "    sample2_size = linregdf.loc[\"ALL YRS\"].iloc[5]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "linregdf.loc[\"ALL YRS\"].iloc[8] = pval\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[\"ALL YRS\"].iloc[0]\n",
    "    sample2_phat = linregdf.loc[\"ALL YRS\"].iloc[2]\n",
    "    sample1_size = linregdf.loc[\"ALL YRS\"].iloc[4]\n",
    "    sample2_size = linregdf.loc[\"ALL YRS\"].iloc[6]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "linregdf.loc[\"ALL YRS\"].iloc[9] = pval\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[\"ALL YRS\"].iloc[0]\n",
    "    sample2_phat = linregdf.loc[\"ALL YRS\"].iloc[3]\n",
    "    sample1_size = linregdf.loc[\"ALL YRS\"].iloc[4]\n",
    "    sample2_size = linregdf.loc[\"ALL YRS\"].iloc[7]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "linregdf.loc[\"ALL YRS\"].iloc[10] = pval\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[\"ALL YRS\"].iloc[1]\n",
    "    sample2_phat = linregdf.loc[\"ALL YRS\"].iloc[2]\n",
    "    sample1_size = linregdf.loc[\"ALL YRS\"].iloc[5]\n",
    "    sample2_size = linregdf.loc[\"ALL YRS\"].iloc[6]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "linregdf.loc[\"ALL YRS\"].iloc[11] = pval\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[\"ALL YRS\"].iloc[1]\n",
    "    sample2_phat = linregdf.loc[\"ALL YRS\"].iloc[3]\n",
    "    sample1_size = linregdf.loc[\"ALL YRS\"].iloc[5]\n",
    "    sample2_size = linregdf.loc[\"ALL YRS\"].iloc[7]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "linregdf.loc[\"ALL YRS\"].iloc[12] = pval\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[\"ALL YRS\"].iloc[2]\n",
    "    sample2_phat = linregdf.loc[\"ALL YRS\"].iloc[3]\n",
    "    sample1_size = linregdf.loc[\"ALL YRS\"].iloc[6]\n",
    "    sample2_size = linregdf.loc[\"ALL YRS\"].iloc[7]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "linregdf.loc[\"ALL YRS\"].iloc[13] = pval\n",
    "\n",
    "linregdf = linregdf.head(9).style.format(\"{:,.2f}\")\n",
    "linregdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660da76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIVING DEEPER INTO THE MINIMUM REQUIRED EDUCATION LEVEL\n",
    "#ANCHORED BY STATE: PW SKILL,APPLICATIONS, MEDIAN WAGE, YRS POST GRADE, MEDIAN DAYS\n",
    "\n",
    "code = \"LOWER\"\n",
    "# CHANGE THIS FOR ANY CODE\n",
    "\n",
    "# MEDIAN WAGE, MEDIAN YEARS AFTER GRADUATING, MEDIAN DAYS TO DECIDE : BY PW LEVEL\n",
    "topstate = df[df[\"REQ_EDU_GROUP\"] == code][\"WORKSITE_STATE\"].value_counts()[:15].index.tolist()\n",
    "pwskill = df[\"PW_SKILL_LEVEL\"].dropna().unique().tolist()\n",
    "pwskill.sort()\n",
    "\n",
    "\n",
    "statelist = []\n",
    "pwlist = pwskill*len(topstate)\n",
    "apps = []\n",
    "medwage = []\n",
    "yrsgrad = []\n",
    "meddays = []\n",
    "\n",
    "\n",
    "for x in range(len(topstate)):\n",
    "    s = (topstate[x]+\",\")*len(pwskill)\n",
    "    s = \"\".join(s).split(\",\")\n",
    "    s = list(filter((\"\").__ne__, s))\n",
    "    statelist.append(s) \n",
    "for x in range(len(topstate)):\n",
    "    for j in range(len(pwskill)):\n",
    "        a = len(df[(df[\"REQ_EDU_GROUP\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"REQ_EDU_GROUP\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]])\n",
    "        apps.append(a)\n",
    "        b = df[(df[\"REQ_EDU_GROUP\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"REQ_EDU_GROUP\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]][\"WO_A\"].median()\n",
    "        medwage.append(b)\n",
    "        c = df[(df[\"REQ_EDU_GROUP\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"REQ_EDU_GROUP\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]][\"YRSPOSTGRAD\"].median()\n",
    "        yrsgrad.append(c)\n",
    "        d = df[(df[\"REQ_EDU_GROUP\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"REQ_EDU_GROUP\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]][\"DAYS_TO_DECIDE\"].median()\n",
    "        meddays.append(d)\n",
    "codestatedf = pd.DataFrame([sum(statelist, []),pwlist,apps,medwage,yrsgrad,meddays]).transpose()\n",
    "codestatedf.columns = [\"STATE\",\"PW LEVEL\", \"TOTAL APPLICATIONS\",\"MEDIAN WAGE\",\"MEDIAN YRS POST GRAD\",\"MEDIAN DAYS TO DECIDE\"]\n",
    "codestatedf = codestatedf.set_index([\"STATE\",\"PW LEVEL\"])\n",
    "codestatedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15362775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PW LEVEL DISTRIBUTION ANALYSIS\n",
    "# PLOTTING SUCCESS RATES OVER TIME BY PW LEVEL SKILL\n",
    "\n",
    "lower = df[df[\"PW_SKILL_LEVEL\"]==\"LEVEL I\"]\n",
    "college = df[df[\"PW_SKILL_LEVEL\"]==\"LEVEL II\"]\n",
    "upper = df[df[\"PW_SKILL_LEVEL\"]==\"LEVEL III\"]\n",
    "other = df[df[\"PW_SKILL_LEVEL\"]==\"LEVEL IV\"]\n",
    "\n",
    "\n",
    "\n",
    "lowerdf = pd.DataFrame()\n",
    "collegedf = pd.DataFrame()\n",
    "upperdf = pd.DataFrame()\n",
    "otherdf = pd.DataFrame()\n",
    "linregdf = pd.DataFrame()\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    lowe = pd.DataFrame([len(lower[(lower[\"YEAR\"]==yr[x])&(lower[\"CASE_STATUS\"]==1)])/len(lower[lower[\"YEAR\"]==yr[x]])])\n",
    "    coll = pd.DataFrame([len(college[(college[\"YEAR\"]==yr[x])&(college[\"CASE_STATUS\"]==1)])/len(college[college[\"YEAR\"]==yr[x]])])\n",
    "    upp = pd.DataFrame([len(upper[(upper[\"YEAR\"]==yr[x])&(upper[\"CASE_STATUS\"]==1)])/len(upper[upper[\"YEAR\"]==yr[x]])])\n",
    "    ott = pd.DataFrame([len(other[(other[\"YEAR\"]==yr[x])&(other[\"CASE_STATUS\"]==1)])/len(other[other[\"YEAR\"]==yr[x]])])\n",
    "    lowerdf = pd.concat([lowerdf,lowe])\n",
    "    collegedf = pd.concat([collegedf,coll])\n",
    "    upperdf = pd.concat([upperdf,upp])\n",
    "    otherdf = pd.concat([otherdf, ott])\n",
    "    linregdf = pd.concat([lowerdf,collegedf,upperdf,otherdf], axis = 1)\n",
    "    \n",
    "\n",
    "linregdf.index = yr\n",
    "linregdf.columns = [\"Level 1 Success %\", \"Level 2 Success %\", \"Level 3 Success %\",\"Level 4 Success %\"]\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "linregdf.plot(kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Success Rates by PW Skill Level, Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473f22a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARING DATA FOR LIN REGRESS AND GETTING P VALUE\n",
    "lowerapps = []\n",
    "collegeapps = []\n",
    "upperapps = []\n",
    "otherapps = []\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    l = (lower[\"YEAR\"]==yr[x]).sum()\n",
    "    c = (college[\"YEAR\"]==yr[x]).sum()\n",
    "    u = (upper[\"YEAR\"]==yr[x]).sum()\n",
    "    o = (other[\"YEAR\"]==yr[x]).sum()\n",
    "    lowerapps.append(l)\n",
    "    collegeapps.append(c)\n",
    "    upperapps.append(u)\n",
    "    otherapps.append(o)\n",
    "linregdf = linregdf.reset_index()\n",
    "linregdf[\"Level 1 Apps\"] = pd.DataFrame(lowerapps)\n",
    "linregdf[\"Level 2 Apps\"] = pd.DataFrame(collegeapps)\n",
    "linregdf[\"Level 3 Apps\"] = pd.DataFrame(upperapps)\n",
    "linregdf[\"Level 4 Apps\"] = pd.DataFrame(otherapps)\n",
    "linregdf = linregdf.set_index(\"index\")\n",
    "# PLOTTING APPLICATION RATES OVER TIME BY SKILL LEVEL\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "linregdf.reset_index().plot(x = \"index\", y = [\"Level 1 Apps\",\"Level 2 Apps\",\"Level 3 Apps\",\"Level 4 Apps\"], kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Application Rates by PW Skill Level, Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4765aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P VALUE PER YEAR\n",
    "lower_college = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[yr[x]][\"Level 1 Success %\"]\n",
    "    sample2_phat = linregdf.loc[yr[x]][\"Level 2 Success %\"]\n",
    "    sample1_size = linregdf.loc[yr[x]][\"Level 1 Apps\"]\n",
    "    sample2_size = linregdf.loc[yr[x]][\"Level 2 Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    lower_college.append(pval)\n",
    "\n",
    "pvaldf = pd.DataFrame(lower_college)\n",
    "\n",
    "lower_upper = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[yr[x]][\"Level 1 Success %\"]\n",
    "    sample2_phat = linregdf.loc[yr[x]][\"Level 3 Success %\"]\n",
    "    sample1_size = linregdf.loc[yr[x]][\"Level 1 Apps\"]\n",
    "    sample2_size = linregdf.loc[yr[x]][\"Level 3 Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    lower_upper.append(pval)\n",
    "pvaldf = pd.concat([pvaldf,pd.DataFrame(lower_upper)], axis = 1)\n",
    "\n",
    "lower_other = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[yr[x]][\"Level 1 Success %\"]\n",
    "    sample2_phat = linregdf.loc[yr[x]][\"Level 4 Success %\"]\n",
    "    sample1_size = linregdf.loc[yr[x]][\"Level 1 Apps\"]\n",
    "    sample2_size = linregdf.loc[yr[x]][\"Level 4 Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    lower_other.append(pval)\n",
    "pvaldf = pd.concat([pvaldf,pd.DataFrame(lower_other)], axis = 1)\n",
    "\n",
    "college_upper = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[yr[x]][\"Level 2 Success %\"]\n",
    "    sample2_phat = linregdf.loc[yr[x]][\"Level 3 Success %\"]\n",
    "    sample1_size = linregdf.loc[yr[x]][\"Level 2 Apps\"]\n",
    "    sample2_size = linregdf.loc[yr[x]][\"Level 3 Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    college_upper.append(pval)\n",
    "pvaldf = pd.concat([pvaldf,pd.DataFrame(college_upper)], axis = 1)\n",
    "\n",
    "college_other = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[yr[x]][\"Level 2 Success %\"]\n",
    "    sample2_phat = linregdf.loc[yr[x]][\"Level 4 Success %\"]\n",
    "    sample1_size = linregdf.loc[yr[x]][\"Level 2 Apps\"]\n",
    "    sample2_size = linregdf.loc[yr[x]][\"Level 4 Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    college_other.append(pval)\n",
    "pvaldf = pd.concat([pvaldf,pd.DataFrame(college_other)], axis = 1)\n",
    "\n",
    "upper_other = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[yr[x]][\"Level 3 Success %\"]\n",
    "    sample2_phat = linregdf.loc[yr[x]][\"Level 4 Success %\"]\n",
    "    sample1_size = linregdf.loc[yr[x]][\"Level 3 Apps\"]\n",
    "    sample2_size = linregdf.loc[yr[x]][\"Level 4 Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    upper_other.append(pval)\n",
    "pvaldf = pd.concat([pvaldf,pd.DataFrame(upper_other)], axis = 1)\n",
    "\n",
    "pvaldf.columns = [\"L1_L2\",\"L1_L3\",\"L1_L4\",\"L2_L3\",\"L2_L4\",\"L3_L4\"]\n",
    "pvaldf.index = yr\n",
    "linregdf[[\"L1_L2\",\"L1_L3\",\"L1_L4\",\"L2_L3\",\"L2_L4\",\"L3_L4\"]] = pvaldf[[\"L1_L2\",\"L1_L3\",\"L1_L4\",\"L2_L3\",\"L2_L4\",\"L3_L4\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a116bd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION BETWEEN PW SKILL LEVEL and success\n",
    "print(\"Level 1 success rate over time:\",st.linregress(y = linregdf[\"Level 1 Success %\"], x = yr))\n",
    "print(\"\\nLevel 1 applications over time:\",st.linregress(y = linregdf[\"Level 1 Apps\"], x = yr))\n",
    "\n",
    "print(\"\\n\\nLevel 2 success rate over time:\",st.linregress(y = linregdf[\"Level 2 Success %\"], x = yr))\n",
    "print(\"\\nLevel 2 applications over time:\",st.linregress(y = linregdf[\"Level 2 Apps\"], x = yr))\n",
    "\n",
    "print(\"\\n\\nLevel 3 success rate over time:\",st.linregress(y = linregdf[\"Level 3 Success %\"], x = yr))\n",
    "print(\"\\nLevel 3 applications over time:\",st.linregress(y = linregdf[\"Level 3 Apps\"], x = yr))\n",
    "\n",
    "print(\"\\n\\nLevel 4 success rate over time:\",st.linregress(y = linregdf[\"Level 4 Success %\"], x = yr))\n",
    "print(\"\\nLevel 4 applications over time:\",st.linregress(y = linregdf[\"Level 4 Apps\"], x = yr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8d909e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING THE TOTAL PVAL\n",
    "linregdf.loc[\"ALL YRS\"] = linregdf.sum()\n",
    "lowersuc = []\n",
    "collegesuc = []\n",
    "uppersuc = []\n",
    "othersuc = []\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    l = linregdf.loc[yr[x]].iloc[0]*linregdf.loc[yr[x]].iloc[4]\n",
    "    c = linregdf.loc[yr[x]].iloc[1]*linregdf.loc[yr[x]].iloc[5]\n",
    "    u = linregdf.loc[yr[x]].iloc[2]*linregdf.loc[yr[x]].iloc[6]\n",
    "    o = linregdf.loc[yr[x]].iloc[3]*linregdf.loc[yr[x]].iloc[7]\n",
    "    lowersuc.append(l)\n",
    "    collegesuc.append(c)\n",
    "    uppersuc.append(u)\n",
    "    othersuc.append(o)\n",
    "    \n",
    "linregdf.loc[\"ALL YRS\"].iloc[0] = sum(lowersuc)/linregdf.loc[\"ALL YRS\"].iloc[4]\n",
    "linregdf.loc[\"ALL YRS\"].iloc[1] = sum(collegesuc)/linregdf.loc[\"ALL YRS\"].iloc[5]\n",
    "linregdf.loc[\"ALL YRS\"].iloc[2] = sum(uppersuc)/linregdf.loc[\"ALL YRS\"].iloc[6]\n",
    "linregdf.loc[\"ALL YRS\"].iloc[3] = sum(othersuc)/linregdf.loc[\"ALL YRS\"].iloc[7]\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[\"ALL YRS\"].iloc[0]\n",
    "    sample2_phat = linregdf.loc[\"ALL YRS\"].iloc[1]\n",
    "    sample1_size = linregdf.loc[\"ALL YRS\"].iloc[4]\n",
    "    sample2_size = linregdf.loc[\"ALL YRS\"].iloc[5]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "linregdf.loc[\"ALL YRS\"].iloc[8] = pval\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[\"ALL YRS\"].iloc[0]\n",
    "    sample2_phat = linregdf.loc[\"ALL YRS\"].iloc[2]\n",
    "    sample1_size = linregdf.loc[\"ALL YRS\"].iloc[4]\n",
    "    sample2_size = linregdf.loc[\"ALL YRS\"].iloc[6]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "linregdf.loc[\"ALL YRS\"].iloc[9] = pval\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[\"ALL YRS\"].iloc[0]\n",
    "    sample2_phat = linregdf.loc[\"ALL YRS\"].iloc[3]\n",
    "    sample1_size = linregdf.loc[\"ALL YRS\"].iloc[4]\n",
    "    sample2_size = linregdf.loc[\"ALL YRS\"].iloc[7]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "linregdf.loc[\"ALL YRS\"].iloc[10] = pval\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[\"ALL YRS\"].iloc[1]\n",
    "    sample2_phat = linregdf.loc[\"ALL YRS\"].iloc[2]\n",
    "    sample1_size = linregdf.loc[\"ALL YRS\"].iloc[5]\n",
    "    sample2_size = linregdf.loc[\"ALL YRS\"].iloc[6]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "linregdf.loc[\"ALL YRS\"].iloc[11] = pval\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[\"ALL YRS\"].iloc[1]\n",
    "    sample2_phat = linregdf.loc[\"ALL YRS\"].iloc[3]\n",
    "    sample1_size = linregdf.loc[\"ALL YRS\"].iloc[5]\n",
    "    sample2_size = linregdf.loc[\"ALL YRS\"].iloc[7]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "linregdf.loc[\"ALL YRS\"].iloc[12] = pval\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[\"ALL YRS\"].iloc[2]\n",
    "    sample2_phat = linregdf.loc[\"ALL YRS\"].iloc[3]\n",
    "    sample1_size = linregdf.loc[\"ALL YRS\"].iloc[6]\n",
    "    sample2_size = linregdf.loc[\"ALL YRS\"].iloc[7]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "linregdf.loc[\"ALL YRS\"].iloc[13] = pval\n",
    "\n",
    "linregdf = linregdf.head(9).style.format(\"{:,.2f}\")\n",
    "linregdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39a5131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEEPER DIVE INTO ALL PW SKILL LEVELS!\n",
    "# ANCHORED BY STATE: EDU LEVEL,APPLICATIONS, MEDIAN WAGE, YRS POST GRADE, MEDIAN DAYS\n",
    "# MEDIAN WAGE, MEDIAN YEARS AFTER GRADUATING, MEDIAN DAYS TO DECIDE : BY EDU LEVEL\n",
    "code = \"LEVEL IV\"\n",
    "# CHANGE THIS FOR ANY CODE\n",
    "topstate = df[df[\"PW_SKILL_LEVEL\"] == code][\"WORKSITE_STATE\"].value_counts()[:15].index.tolist()\n",
    "edu = df[\"FOREIGN_WORKER_EDUCATION\"].dropna().unique().tolist()\n",
    "edu[0],edu[1],edu[2],edu[3],edu[4],edu[5] = edu[3],edu[5],edu[0],edu[2],edu[1],edu[4]\n",
    "\n",
    "statelist = []\n",
    "edulist = edu*len(topstate)\n",
    "apps = []\n",
    "medwage = []\n",
    "yrsgrad = []\n",
    "meddays = []\n",
    "\n",
    "\n",
    "for x in range(len(topstate)):\n",
    "    s = (topstate[x]+\",\")*len(edu)\n",
    "    s = \"\".join(s).split(\",\")\n",
    "    s = list(filter((\"\").__ne__, s))\n",
    "    statelist.append(s) \n",
    "for x in range(len(topstate)):\n",
    "    for j in range(len(edu)):\n",
    "        a = len(df[(df[\"PW_SKILL_LEVEL\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"PW_SKILL_LEVEL\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]])\n",
    "        apps.append(a)\n",
    "        b = df[(df[\"PW_SKILL_LEVEL\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"PW_SKILL_LEVEL\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]][\"WO_A\"].median()\n",
    "        medwage.append(b)\n",
    "        c = df[(df[\"PW_SKILL_LEVEL\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"PW_SKILL_LEVEL\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]][\"YRSPOSTGRAD\"].median()\n",
    "        yrsgrad.append(c)\n",
    "        d = df[(df[\"PW_SKILL_LEVEL\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"PW_SKILL_LEVEL\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]][\"DAYS_TO_DECIDE\"].median()\n",
    "        meddays.append(d)\n",
    "codestatedf1 = pd.DataFrame([sum(statelist, []),edulist,apps,medwage,yrsgrad,meddays]).transpose()\n",
    "codestatedf1.columns = [\"STATE\",\"EDU LEVEL\", \"TOTAL APPLICATIONS\",\"MEDIAN WAGE\",\"MEDIAN YRS POST GRAD\",\"MEDIAN DAYS TO DECIDE\"]\n",
    "codestatedf1 = codestatedf1.set_index([\"STATE\",\"EDU LEVEL\"])\n",
    "codestatedf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc9eb5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PROFESSIONAL OCCUPATION?\n",
    "# DIVING DEEPER INTO IF PROFESSIONAL OCCUPATION!\n",
    "#ANCHORED BY STATE: PW SKILL,APPLICATIONS, MEDIAN WAGE, YRS POST GRADE, MEDIAN DAYS\n",
    "\n",
    "code = 1\n",
    "# CHANGE THIS FOR ANY CODE\n",
    "\n",
    "# MEDIAN WAGE, MEDIAN YEARS AFTER GRADUATING, MEDIAN DAYS TO DECIDE : FOR EACH STATE IN THE TOP 15 OF THE COMPANY AND BY PW LEVEL\n",
    "topstate = df[df[\"PROFESSIONAL_OCCUPATION\"] == code][\"WORKSITE_STATE\"].value_counts()[:15].index.tolist()\n",
    "pwskill = df[\"PW_SKILL_LEVEL\"].dropna().unique().tolist()\n",
    "pwskill.sort()\n",
    "\n",
    "\n",
    "statelist = []\n",
    "pwlist = pwskill*len(topstate)\n",
    "apps = []\n",
    "medwage = []\n",
    "yrsgrad = []\n",
    "meddays = []\n",
    "\n",
    "\n",
    "for x in range(len(topstate)):\n",
    "    s = (topstate[x]+\",\")*len(pwskill)\n",
    "    s = \"\".join(s).split(\",\")\n",
    "    s = list(filter((\"\").__ne__, s))\n",
    "    statelist.append(s) \n",
    "for x in range(len(topstate)):\n",
    "    for j in range(len(pwskill)):\n",
    "        a = len(df[(df[\"PROFESSIONAL_OCCUPATION\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"PROFESSIONAL_OCCUPATION\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]])\n",
    "        apps.append(a)\n",
    "        b = df[(df[\"PROFESSIONAL_OCCUPATION\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"PROFESSIONAL_OCCUPATION\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]][\"WO_A\"].median()\n",
    "        medwage.append(b)\n",
    "        c = df[(df[\"PROFESSIONAL_OCCUPATION\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"PROFESSIONAL_OCCUPATION\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]][\"YRSPOSTGRAD\"].median()\n",
    "        yrsgrad.append(c)\n",
    "        d = df[(df[\"PROFESSIONAL_OCCUPATION\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"PROFESSIONAL_OCCUPATION\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]][\"DAYS_TO_DECIDE\"].median()\n",
    "        meddays.append(d)\n",
    "codestatedf = pd.DataFrame([sum(statelist, []),pwlist,apps,medwage,yrsgrad,meddays]).transpose()\n",
    "codestatedf.columns = [\"STATE\",\"PW LEVEL\", \"TOTAL APPLICATIONS\",\"MEDIAN WAGE\",\"MEDIAN YRS POST GRAD\",\"MEDIAN DAYS TO DECIDE\"]\n",
    "codestatedf = codestatedf.set_index([\"STATE\",\"PW LEVEL\"])\n",
    "codestatedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc96887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANCHORED BY STATE: EDU LEVEL,APPLICATIONS, MEDIAN WAGE, YRS POST GRADE, MEDIAN DAYS\n",
    "# MEDIAN WAGE, MEDIAN YEARS AFTER GRADUATING, MEDIAN DAYS TO DECIDE : FOR EACH STATE IN THE TOP 15 OF THE COMPANY AND BY EDU LEVEL\n",
    "edu = df[\"FOREIGN_WORKER_EDUCATION\"].dropna().unique().tolist()\n",
    "edu[0],edu[1],edu[2],edu[3],edu[4],edu[5] = edu[3],edu[5],edu[0],edu[2],edu[1],edu[4]\n",
    "\n",
    "statelist = []\n",
    "edulist = edu*len(topstate)\n",
    "apps = []\n",
    "medwage = []\n",
    "yrsgrad = []\n",
    "meddays = []\n",
    "\n",
    "\n",
    "for x in range(len(topstate)):\n",
    "    s = (topstate[x]+\",\")*len(edu)\n",
    "    s = \"\".join(s).split(\",\")\n",
    "    s = list(filter((\"\").__ne__, s))\n",
    "    statelist.append(s) \n",
    "for x in range(len(topstate)):\n",
    "    for j in range(len(edu)):\n",
    "        a = len(df[(df[\"PROFESSIONAL_OCCUPATION\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"PROFESSIONAL_OCCUPATION\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]])\n",
    "        apps.append(a)\n",
    "        b = df[(df[\"PROFESSIONAL_OCCUPATION\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"PROFESSIONAL_OCCUPATION\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]][\"WO_A\"].median()\n",
    "        medwage.append(b)\n",
    "        c = df[(df[\"PROFESSIONAL_OCCUPATION\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"PROFESSIONAL_OCCUPATION\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]][\"YRSPOSTGRAD\"].median()\n",
    "        yrsgrad.append(c)\n",
    "        d = df[(df[\"PROFESSIONAL_OCCUPATION\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"PROFESSIONAL_OCCUPATION\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]][\"DAYS_TO_DECIDE\"].median()\n",
    "        meddays.append(d)\n",
    "codestatedf1 = pd.DataFrame([sum(statelist, []),edulist,apps,medwage,yrsgrad,meddays]).transpose()\n",
    "codestatedf1.columns = [\"STATE\",\"EDU LEVEL\", \"TOTAL APPLICATIONS\",\"MEDIAN WAGE\",\"MEDIAN YRS POST GRAD\",\"MEDIAN DAYS TO DECIDE\"]\n",
    "codestatedf1 = codestatedf1.set_index([\"STATE\",\"EDU LEVEL\"])\n",
    "codestatedf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbed0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING THE DIFFERENT SUCCESS RATES OVER TIME BY Professional Occupation or not (IN TERMS OF ANNUAL APPLICATIONS)\n",
    "\n",
    "top15 = df[df[\"PROFESSIONAL_OCCUPATION\"]==1]\n",
    "not15 = df[df[\"PROFESSIONAL_OCCUPATION\"]==0]\n",
    "\n",
    "topdf = pd.DataFrame()\n",
    "notdf = pd.DataFrame()\n",
    "analysisdf = pd.DataFrame()\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    j = pd.DataFrame([len(top15[(top15[\"YEAR\"]==yr[x])&(top15[\"CASE_STATUS\"]==1)])/len(top15[top15[\"YEAR\"]==yr[x]])])\n",
    "    b = pd.DataFrame([len(not15[(not15[\"YEAR\"]==yr[x])&(not15[\"CASE_STATUS\"]==1)])/len(not15[not15[\"YEAR\"]==yr[x]])])\n",
    "    topdf = pd.concat([topdf,j])\n",
    "    notdf = pd.concat([notdf,b])\n",
    "    analysisdf = pd.concat([topdf,notdf], axis = 1)\n",
    "\n",
    "analysisdf.index = yr\n",
    "analysisdf.columns = [\"Professional Success %\", \"Not Professional Success %\"]\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "analysisdf.plot(kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Success Rates by Professional and not Professional Jobs, Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2e40c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARING DATA FOR LIN REGRESS AND GETTING P VALUE\n",
    "top15apps = []\n",
    "not15apps = []\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    t = (top15[\"YEAR\"]==yr[x]).sum()\n",
    "    n = (not15[\"YEAR\"]==yr[x]).sum()\n",
    "    top15apps.append(t)\n",
    "    not15apps.append(n)\n",
    "analysisdf = analysisdf.reset_index()\n",
    "analysisdf[\"Professional Apps\"] = pd.DataFrame(top15apps)\n",
    "analysisdf[\"Not Professional Apps\"] = pd.DataFrame(not15apps)\n",
    "analysisdf = analysisdf.set_index(\"index\")\n",
    "# PLOTTING APPLICATION RATES OVER TIME BY Professional or not\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "analysisdf.reset_index().plot(x = \"index\", y = [\"Professional Apps\",\"Not Professional Apps\"], kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Application Rates by Professional and not Professional Jobs, Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb89ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P VALUE PER YEAR: ON THE SUCCESS RATE\n",
    "top_not = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = analysisdf.loc[yr[x]][\"Professional Success %\"]\n",
    "    sample2_phat = analysisdf.loc[yr[x]][\"Not Professional Success %\"]\n",
    "    sample1_size = analysisdf.loc[yr[x]][\"Professional Apps\"]\n",
    "    sample2_size = analysisdf.loc[yr[x]][\"Not Professional Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    top_not.append(pval)\n",
    "\n",
    "pvaldf = pd.DataFrame(top_not)\n",
    "pvaldf.columns = [\"Professional Vs. Not Professional\"]\n",
    "pvaldf.index = yr\n",
    "analysisdf[\"PVAL Success %\"] = pvaldf[\"Professional Vs. Not Professional\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651d2700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION BETWEEN Professional Occupation AND SUCCESS RATE OVER TIME\n",
    "print(\"Professional success rate over time:\",st.linregress(y = analysisdf[\"Professional Success %\"], x = yr))\n",
    "print(\"\\nProfessional applications over time:\",st.linregress(y = analysisdf[\"Professional Apps\"], x = yr))\n",
    "\n",
    "print(\"\\n\\nNot professional success rate over time:\",st.linregress(y = analysisdf[\"Not Professional Success %\"], x = yr))\n",
    "print(\"\\nNot Professional applications over time:\",st.linregress(y = analysisdf[\"Not Professional Apps\"], x = yr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1601067c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING THE TOTAL PVAL\n",
    "analysisdf.loc[\"ALL YRS\"] = analysisdf.sum()\n",
    "top15suc = []\n",
    "not15suc = []\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    t15 = analysisdf.loc[yr[x]].iloc[0]*analysisdf.loc[yr[x]].iloc[2]\n",
    "    n15 = analysisdf.loc[yr[x]].iloc[1]*analysisdf.loc[yr[x]].iloc[3]\n",
    "    top15suc.append(t15)\n",
    "    not15suc.append(n15)\n",
    "analysisdf.loc[\"ALL YRS\"].iloc[0] = sum(top15suc)/analysisdf.loc[\"ALL YRS\"].iloc[2]\n",
    "analysisdf.loc[\"ALL YRS\"].iloc[1] = sum(not15suc)/analysisdf.loc[\"ALL YRS\"].iloc[3]\n",
    "\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = analysisdf.loc[\"ALL YRS\"].iloc[0]\n",
    "    sample2_phat = analysisdf.loc[\"ALL YRS\"].iloc[1]\n",
    "    sample1_size = analysisdf.loc[\"ALL YRS\"].iloc[2]\n",
    "    sample2_size = analysisdf.loc[\"ALL YRS\"].iloc[3]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "analysisdf.loc[\"ALL YRS\"].iloc[4] = pval\n",
    "\n",
    "analysisdf = analysisdf.head(9).style.format(\"{:,.2f}\")\n",
    "analysisdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c68c7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOST COMMON NEWSPAPER (FIRST) NAMES\n",
    "print(df['FIRST_NEWSPAPER_NAME'].nunique(),\"unique newspapers.\")\n",
    "print(\"After cleaning,\",df['FIRST_NEWSPAPER_NAME'].str.replace('[^\\w\\s]','', regex = True).str.replace(\" \",\"\").str.replace(\"THE\",\"\").nunique(),\"newspapers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37d45c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW COLUMN, CLEANED UP VERSION OF NEWSPAPER NAMES: TOP 10 OR NOT\n",
    "df[\"CLEANED_FNEWS\"] = df['FIRST_NEWSPAPER_NAME'].str.replace('[^\\w\\s]','', regex = True).str.replace(\" \",\"\").str.replace(\"THE\",\"\")\n",
    "\n",
    "# Top 10 WORK Cities of all time 2014-2021\n",
    "allsoc = pd.DataFrame(df[\"CLEANED_FNEWS\"].value_counts()[:10]).reset_index()\n",
    "allsoc.columns = [\"APaper\",\"COUNT APaper\"]\n",
    "sucsoc = pd.DataFrame(df[df[\"CASE_STATUS\"]==1][\"CLEANED_FNEWS\"].value_counts()[:10]).reset_index()\n",
    "sucsoc.columns = [\"SPaper\",\"COUNT SPaper\"]\n",
    "pd.concat([allsoc,sucsoc], axis = 1)\n",
    "# Grabbing the top 5 for each year state worked in: total applications\n",
    "top5aemp = pd.DataFrame()\n",
    "def grab (year):\n",
    "    return df[df[\"YEAR\"]==year][\"CLEANED_FNEWS\"].value_counts()[:10]\n",
    "for x in range(len(yr)):\n",
    "    j = pd.DataFrame(grab(yr[x]))\n",
    "    top5aemp = pd.concat([top5aemp,j], axis = 1)\n",
    "top5aemp.columns= yr\n",
    "# King of Consistency: HOW MANY YEARS IS THIS state NOT IN THE TOP 5?: TOP CONSISTENT states...\n",
    "kingconsA = top5aemp.isnull().replace(False,0).replace(True,1)\n",
    "kingconsA[\"Yrs not in Top 10\"] = top5aemp.isnull().replace(False,0).replace(True,1).sum(axis = 1)\n",
    "kingconsA[\"Yrs not in Top 10\"].sort_values()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66457876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW COLUMN: 1 is in the top 10 of the YEAR, 0 is not: Applications\n",
    "filler = pd.DataFrame()\n",
    "for x in range(len(yr)):\n",
    "    listy = top5aemp[yr[x]].dropna().index.tolist()\n",
    "    filler = pd.concat([filler,pd.DataFrame((df[df[\"YEAR\"]==yr[x]][\"CLEANED_FNEWS\"].isin(listy)).replace(True,1).replace(False,0))])\n",
    "\n",
    "filler = filler.reset_index().sort_values(by = \"index\").set_index(\"index\")\n",
    "filler.columns = [\"TOP10NEWS\"]\n",
    "df[\"TOP10NEWS\"] = filler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3542815b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING THE DIFFERENT SUCCESS RATES OVER TIME BY TOP 10 Newspaper (IN TERMS OF ANNUAL APPLICATIONS)\n",
    "\n",
    "top15 = df[df[\"TOP10NEWS\"]==1]\n",
    "not15 = df[df[\"TOP10NEWS\"]==0]\n",
    "\n",
    "topdf = pd.DataFrame()\n",
    "notdf = pd.DataFrame()\n",
    "analysisdf = pd.DataFrame()\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    j = pd.DataFrame([len(top15[(top15[\"YEAR\"]==yr[x])&(top15[\"CASE_STATUS\"]==1)])/len(top15[top15[\"YEAR\"]==yr[x]])])\n",
    "    b = pd.DataFrame([len(not15[(not15[\"YEAR\"]==yr[x])&(not15[\"CASE_STATUS\"]==1)])/len(not15[not15[\"YEAR\"]==yr[x]])])\n",
    "    topdf = pd.concat([topdf,j])\n",
    "    notdf = pd.concat([notdf,b])\n",
    "    analysisdf = pd.concat([topdf,notdf], axis = 1)\n",
    "\n",
    "analysisdf.index = yr\n",
    "analysisdf.columns = [\"Top 10 Success %\", \"Not 10 Success %\"]\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "analysisdf.plot(kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Success Rates by Top 10 and not Top 10 Newspapers, Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de303cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARING DATA FOR LIN REGRESS AND GETTING P VALUE\n",
    "top15apps = []\n",
    "not15apps = []\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    t = (top15[\"YEAR\"]==yr[x]).sum()\n",
    "    n = (not15[\"YEAR\"]==yr[x]).sum()\n",
    "    top15apps.append(t)\n",
    "    not15apps.append(n)\n",
    "analysisdf = analysisdf.reset_index()\n",
    "analysisdf[\"Top 10 Apps\"] = pd.DataFrame(top15apps)\n",
    "analysisdf[\"Not 10 Apps\"] = pd.DataFrame(not15apps)\n",
    "analysisdf = analysisdf.set_index(\"index\")\n",
    "# PLOTTING APPLICATION RATES OVER TIME BY TOP 15 or NOT TOP 15\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "analysisdf.reset_index().plot(x = \"index\", y = [\"Top 10 Apps\",\"Not 10 Apps\"], kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Application Rates by Top 10 or not Top 10 Newspapers, Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed482936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P VALUE PER YEAR: ON THE SUCCESS RATE\n",
    "top_not = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = analysisdf.loc[yr[x]][\"Top 10 Success %\"]\n",
    "    sample2_phat = analysisdf.loc[yr[x]][\"Not 10 Success %\"]\n",
    "    sample1_size = analysisdf.loc[yr[x]][\"Top 10 Apps\"]\n",
    "    sample2_size = analysisdf.loc[yr[x]][\"Not 10 Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    top_not.append(pval)\n",
    "\n",
    "pvaldf = pd.DataFrame(top_not)\n",
    "pvaldf.columns = [\"Top 10 Vs. Not 10\"]\n",
    "pvaldf.index = yr\n",
    "analysisdf[\"PVAL Success %\"] = pvaldf[\"Top 10 Vs. Not 10\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc15f496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION BETWEEN Professional Occupation AND SUCCESS RATE OVER TIME\n",
    "print(\"Top 10 success rate over time:\",st.linregress(y = analysisdf[\"Top 10 Success %\"], x = yr))\n",
    "print(\"\\nTop 10 applications over time:\",st.linregress(y = analysisdf[\"Top 10 Apps\"], x = yr))\n",
    "\n",
    "print(\"\\n\\nNot 10 success rate over time:\",st.linregress(y = analysisdf[\"Not 10 Success %\"], x = yr))\n",
    "print(\"\\nNot 10 applications over time:\",st.linregress(y = analysisdf[\"Not 10 Apps\"], x = yr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4502196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING THE TOTAL PVAL\n",
    "analysisdf.loc[\"ALL YRS\"] = analysisdf.sum()\n",
    "top15suc = []\n",
    "not15suc = []\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    t15 = analysisdf.loc[yr[x]].iloc[0]*analysisdf.loc[yr[x]].iloc[2]\n",
    "    n15 = analysisdf.loc[yr[x]].iloc[1]*analysisdf.loc[yr[x]].iloc[3]\n",
    "    top15suc.append(t15)\n",
    "    not15suc.append(n15)\n",
    "analysisdf.loc[\"ALL YRS\"].iloc[0] = sum(top15suc)/analysisdf.loc[\"ALL YRS\"].iloc[2]\n",
    "analysisdf.loc[\"ALL YRS\"].iloc[1] = sum(not15suc)/analysisdf.loc[\"ALL YRS\"].iloc[3]\n",
    "\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = analysisdf.loc[\"ALL YRS\"].iloc[0]\n",
    "    sample2_phat = analysisdf.loc[\"ALL YRS\"].iloc[1]\n",
    "    sample1_size = analysisdf.loc[\"ALL YRS\"].iloc[2]\n",
    "    sample2_size = analysisdf.loc[\"ALL YRS\"].iloc[3]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "analysisdf.loc[\"ALL YRS\"].iloc[4] = pval\n",
    "\n",
    "analysisdf = analysisdf.head(9).style.format(\"{:,.2f}\")\n",
    "analysisdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c43c284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIRST AD TO DAYS\n",
    "# CREATING A NEW COLUMN: AD GROUP, SPLITS DAYS INTO THIRDS\n",
    "df[\"FIRST_AD_TO_APP_DAYS\"] = df[\"FIRST_AD_TO_APP_DAYS\"].replace(\"#VALUE!\",np.nan).astype(float)\n",
    "high = df.groupby(\"YEAR\")[\"FIRST_AD_TO_APP_DAYS\"].quantile(2/3).tolist()\n",
    "low = df.groupby(\"YEAR\")[\"FIRST_AD_TO_APP_DAYS\"].quantile(1/3).tolist()\n",
    "df[\"AD_GROUP\"] = np.nan\n",
    "for x in range(len(yr)):\n",
    "    df[\"AD_GROUP\"] = np.where((df[\"YEAR\"]==yr[x])&(df[\"FIRST_AD_TO_APP_DAYS\"]<low[x]),\"FAST\",df[\"AD_GROUP\"])\n",
    "    df[\"AD_GROUP\"] = np.where((df[\"YEAR\"]==yr[x])&(df[\"FIRST_AD_TO_APP_DAYS\"]>=low[x])&(df[\"FIRST_AD_TO_APP_DAYS\"]<high[x]),\"MEDIUM\",df[\"AD_GROUP\"])\n",
    "    df[\"AD_GROUP\"] = np.where((df[\"YEAR\"]==yr[x])&(df[\"FIRST_AD_TO_APP_DAYS\"]>=high[x]),\"SLOW\",df[\"AD_GROUP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d47c6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DIVING DEEPER INTO THE TOP CODES!\n",
    "#ANCHORED BY STATE: PW SKILL,APPLICATIONS, MEDIAN WAGE, YRS POST GRADE, MEDIAN DAYS\n",
    "\n",
    "code = \"FAST\"\n",
    "# CHANGE THIS FOR ANY CODE\n",
    "\n",
    "# MEDIAN WAGE, MEDIAN YEARS AFTER GRADUATING, MEDIAN DAYS TO DECIDE : BY PW LEVEL WITHIN AD GROUP\n",
    "topstate = df[df[\"AD_GROUP\"] == code][\"WORKSITE_STATE\"].value_counts()[:15].index.tolist()\n",
    "pwskill = df[\"PW_SKILL_LEVEL\"].dropna().unique().tolist()\n",
    "pwskill.sort()\n",
    "\n",
    "\n",
    "statelist = []\n",
    "pwlist = pwskill*len(topstate)\n",
    "apps = []\n",
    "medwage = []\n",
    "yrsgrad = []\n",
    "meddays = []\n",
    "\n",
    "\n",
    "for x in range(len(topstate)):\n",
    "    s = (topstate[x]+\",\")*len(pwskill)\n",
    "    s = \"\".join(s).split(\",\")\n",
    "    s = list(filter((\"\").__ne__, s))\n",
    "    statelist.append(s) \n",
    "for x in range(len(topstate)):\n",
    "    for j in range(len(pwskill)):\n",
    "        a = len(df[(df[\"AD_GROUP\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"AD_GROUP\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]])\n",
    "        apps.append(a)\n",
    "        b = df[(df[\"AD_GROUP\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"AD_GROUP\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]][\"WO_A\"].median()\n",
    "        medwage.append(b)\n",
    "        c = df[(df[\"AD_GROUP\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"AD_GROUP\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]][\"YRSPOSTGRAD\"].median()\n",
    "        yrsgrad.append(c)\n",
    "        d = df[(df[\"AD_GROUP\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"AD_GROUP\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]][\"DAYS_TO_DECIDE\"].median()\n",
    "        meddays.append(d)\n",
    "codestatedf = pd.DataFrame([sum(statelist, []),pwlist,apps,medwage,yrsgrad,meddays]).transpose()\n",
    "codestatedf.columns = [\"STATE\",\"PW LEVEL\", \"TOTAL APPLICATIONS\",\"MEDIAN WAGE\",\"MEDIAN YRS POST GRAD\",\"MEDIAN DAYS TO DECIDE\"]\n",
    "codestatedf = codestatedf.set_index([\"STATE\",\"PW LEVEL\"])\n",
    "codestatedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63e369b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANCHORED BY STATE: EDU LEVEL,APPLICATIONS, MEDIAN WAGE, YRS POST GRADE, MEDIAN DAYS\n",
    "# MEDIAN WAGE, MEDIAN YEARS AFTER GRADUATING, MEDIAN DAYS TO DECIDE : BY EDU LEVEL WITHIN AD GROUP\n",
    "edu = df[\"FOREIGN_WORKER_EDUCATION\"].dropna().unique().tolist()\n",
    "edu[0],edu[1],edu[2],edu[3],edu[4],edu[5] = edu[3],edu[5],edu[0],edu[2],edu[1],edu[4]\n",
    "\n",
    "statelist = []\n",
    "edulist = edu*len(topstate)\n",
    "apps = []\n",
    "medwage = []\n",
    "yrsgrad = []\n",
    "meddays = []\n",
    "\n",
    "\n",
    "for x in range(len(topstate)):\n",
    "    s = (topstate[x]+\",\")*len(edu)\n",
    "    s = \"\".join(s).split(\",\")\n",
    "    s = list(filter((\"\").__ne__, s))\n",
    "    statelist.append(s) \n",
    "for x in range(len(topstate)):\n",
    "    for j in range(len(edu)):\n",
    "        a = len(df[(df[\"AD_GROUP\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"AD_GROUP\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]])\n",
    "        apps.append(a)\n",
    "        b = df[(df[\"AD_GROUP\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"AD_GROUP\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]][\"WO_A\"].median()\n",
    "        medwage.append(b)\n",
    "        c = df[(df[\"AD_GROUP\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"AD_GROUP\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]][\"YRSPOSTGRAD\"].median()\n",
    "        yrsgrad.append(c)\n",
    "        d = df[(df[\"AD_GROUP\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"AD_GROUP\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]][\"DAYS_TO_DECIDE\"].median()\n",
    "        meddays.append(d)\n",
    "codestatedf1 = pd.DataFrame([sum(statelist, []),edulist,apps,medwage,yrsgrad,meddays]).transpose()\n",
    "codestatedf1.columns = [\"STATE\",\"EDU LEVEL\", \"TOTAL APPLICATIONS\",\"MEDIAN WAGE\",\"MEDIAN YRS POST GRAD\",\"MEDIAN DAYS TO DECIDE\"]\n",
    "codestatedf1 = codestatedf1.set_index([\"STATE\",\"EDU LEVEL\"])\n",
    "codestatedf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b736305e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRELATION ANALYSIS: AGGREGATE BY SIZE BRACKETS AND YEARLY\n",
    "# PLOTTING SUCCESS RATES OVER TIME BY SIZE OF COMPANY\n",
    "\n",
    "young = df[df[\"AD_GROUP\"]==\"FAST\"]\n",
    "mid = df[df[\"AD_GROUP\"]==\"MEDIUM\"]\n",
    "old = df[df[\"AD_GROUP\"]==\"SLOW\"]\n",
    "\n",
    "\n",
    "youndf = pd.DataFrame()\n",
    "middf = pd.DataFrame()\n",
    "olddf = pd.DataFrame()\n",
    "linregdf = pd.DataFrame()\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    j = pd.DataFrame([len(young[(young[\"YEAR\"]==yr[x])&(young[\"CASE_STATUS\"]==1)])/len(young[young[\"YEAR\"]==yr[x]])])\n",
    "    med = pd.DataFrame([len(mid[(mid[\"YEAR\"]==yr[x])&(mid[\"CASE_STATUS\"]==1)])/len(mid[mid[\"YEAR\"]==yr[x]])])\n",
    "    oldy = pd.DataFrame([len(old[(old[\"YEAR\"]==yr[x])&(old[\"CASE_STATUS\"]==1)])/len(old[old[\"YEAR\"]==yr[x]])])\n",
    "    youndf = pd.concat([youndf,j])\n",
    "    middf = pd.concat([middf,med])\n",
    "    olddf = pd.concat([olddf,oldy])\n",
    "    linregdf = pd.concat([youndf,middf,olddf], axis = 1)\n",
    "    \n",
    "\n",
    "linregdf.index = yr\n",
    "linregdf.columns = [\"Fast Success %\", \"Medium Success %\", \"Slow Success %\"]\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "linregdf.plot(kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Success Rates by Time To Apply Since First Ad, Over Time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda3a6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARING DATA FOR LIN REGRESS AND GETTING P VALUE\n",
    "youngapps = []\n",
    "midapps = []\n",
    "oldapps = []\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    y = (young[\"YEAR\"]==yr[x]).sum()\n",
    "    m = (mid[\"YEAR\"]==yr[x]).sum()\n",
    "    o = (old[\"YEAR\"]==yr[x]).sum()\n",
    "    youngapps.append(y)\n",
    "    midapps.append(m)\n",
    "    oldapps.append(o)\n",
    "linregdf = linregdf.reset_index()\n",
    "linregdf[\"Fast Apps\"] = pd.DataFrame(youngapps)\n",
    "linregdf[\"Medium Apps\"] = pd.DataFrame(midapps)\n",
    "linregdf[\"Slow Apps\"] = pd.DataFrame(oldapps)\n",
    "linregdf = linregdf.set_index(\"index\")\n",
    "# PLOTTING APPLICATION RATES OVER TIME BY SIZE OF COMPANY\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "linregdf.reset_index().plot(x = \"index\", y = [\"Fast Apps\",\"Medium Apps\",\"Slow Apps\"], kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Application Rates by Time To Apply Since First Ad, Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5963867b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P VALUE PER YEAR\n",
    "young_med = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[yr[x]][\"Fast Success %\"]\n",
    "    sample2_phat = linregdf.loc[yr[x]][\"Medium Success %\"]\n",
    "    sample1_size = linregdf.loc[yr[x]][\"Fast Apps\"]\n",
    "    sample2_size = linregdf.loc[yr[x]][\"Medium Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    young_med.append(pval)\n",
    "\n",
    "pvaldf = pd.DataFrame(young_med)\n",
    "\n",
    "young_old = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[yr[x]][\"Fast Success %\"]\n",
    "    sample2_phat = linregdf.loc[yr[x]][\"Slow Success %\"]\n",
    "    sample1_size = linregdf.loc[yr[x]][\"Fast Apps\"]\n",
    "    sample2_size = linregdf.loc[yr[x]][\"Slow Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    young_old.append(pval)\n",
    "pvaldf = pd.concat([pvaldf,pd.DataFrame(young_old)], axis = 1)\n",
    "\n",
    "med_old = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[yr[x]][\"Medium Success %\"]\n",
    "    sample2_phat = linregdf.loc[yr[x]][\"Slow Success %\"]\n",
    "    sample1_size = linregdf.loc[yr[x]][\"Medium Apps\"]\n",
    "    sample2_size = linregdf.loc[yr[x]][\"Slow Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    med_old.append(pval)\n",
    "pvaldf = pd.concat([pvaldf,pd.DataFrame(med_old)], axis = 1)\n",
    "pvaldf.columns = [\"Fast_Med\",\"Fast_Slow\",\"Med_Slow\"]\n",
    "pvaldf.index = yr\n",
    "linregdf[[\"Fast_Med\",\"Fast_Slow\",\"Med_Slow\"]] = pvaldf[[\"Fast_Med\",\"Fast_Slow\",\"Med_Slow\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f50e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION BETWEEN Speed to Apply and success\n",
    "print(\"Fast success rate over time:\",st.linregress(y = linregdf[\"Fast Success %\"], x = yr))\n",
    "print(\"\\nFast applications over time:\",st.linregress(y = linregdf[\"Fast Apps\"], x = yr))\n",
    "\n",
    "print(\"\\n\\nMedium success rate over time:\",st.linregress(y = linregdf[\"Medium Success %\"], x = yr))\n",
    "print(\"\\nMedium applications over time:\",st.linregress(y = linregdf[\"Medium Apps\"], x = yr))\n",
    "\n",
    "print(\"\\n\\nSlow success rate over time:\",st.linregress(y = linregdf[\"Slow Success %\"], x = yr))\n",
    "print(\"\\nSlow applications over time:\",st.linregress(y = linregdf[\"Slow Apps\"], x = yr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a25ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING THE TOTAL PVAL\n",
    "linregdf.loc[\"ALL YRS\"] = linregdf.sum()\n",
    "youngsuc = []\n",
    "medsuc = []\n",
    "oldsuc = []\n",
    "for x in range(len(yr)):\n",
    "    y = linregdf.loc[yr[x]].iloc[0]*linregdf.loc[yr[x]].iloc[3]\n",
    "    m = linregdf.loc[yr[x]].iloc[1]*linregdf.loc[yr[x]].iloc[4]\n",
    "    o = linregdf.loc[yr[x]].iloc[2]*linregdf.loc[yr[x]].iloc[5]\n",
    "    youngsuc.append(y)\n",
    "    medsuc.append(m)\n",
    "    oldsuc.append(o)\n",
    "linregdf.loc[\"ALL YRS\"].iloc[0] = sum(youngsuc)/linregdf.loc[\"ALL YRS\"].iloc[3]\n",
    "linregdf.loc[\"ALL YRS\"].iloc[1] = sum(medsuc)/linregdf.loc[\"ALL YRS\"].iloc[4]\n",
    "linregdf.loc[\"ALL YRS\"].iloc[2] = sum(oldsuc)/linregdf.loc[\"ALL YRS\"].iloc[5]\n",
    "\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[\"ALL YRS\"].iloc[0]\n",
    "    sample2_phat = linregdf.loc[\"ALL YRS\"].iloc[1]\n",
    "    sample1_size = linregdf.loc[\"ALL YRS\"].iloc[3]\n",
    "    sample2_size = linregdf.loc[\"ALL YRS\"].iloc[4]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "linregdf.loc[\"ALL YRS\"].iloc[6] = pval\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[\"ALL YRS\"].iloc[0]\n",
    "    sample2_phat = linregdf.loc[\"ALL YRS\"].iloc[2]\n",
    "    sample1_size = linregdf.loc[\"ALL YRS\"].iloc[3]\n",
    "    sample2_size = linregdf.loc[\"ALL YRS\"].iloc[5]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "linregdf.loc[\"ALL YRS\"].iloc[7] = pval\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[\"ALL YRS\"].iloc[1]\n",
    "    sample2_phat = linregdf.loc[\"ALL YRS\"].iloc[2]\n",
    "    sample1_size = linregdf.loc[\"ALL YRS\"].iloc[4]\n",
    "    sample2_size = linregdf.loc[\"ALL YRS\"].iloc[5]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "linregdf.loc[\"ALL YRS\"].iloc[8] = pval\n",
    "linregdf = linregdf.head(9).style.format(\"{:,.2f}\")\n",
    "linregdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c50287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMPLOYEE REF PROGRAM\n",
    "# PLOTTING THE DIFFERENT SUCCESS RATES OVER TIME BY employee ref or not (IN TERMS OF ANNUAL APPLICATIONS)\n",
    "\n",
    "top15 = df[df[\"EMPLOYEE_REF_PROG\"]==1]\n",
    "not15 = df[df[\"EMPLOYEE_REF_PROG\"]==0]\n",
    "\n",
    "topdf = pd.DataFrame()\n",
    "notdf = pd.DataFrame()\n",
    "analysisdf = pd.DataFrame()\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    j = pd.DataFrame([len(top15[(top15[\"YEAR\"]==yr[x])&(top15[\"CASE_STATUS\"]==1)])/len(top15[top15[\"YEAR\"]==yr[x]])])\n",
    "    b = pd.DataFrame([len(not15[(not15[\"YEAR\"]==yr[x])&(not15[\"CASE_STATUS\"]==1)])/len(not15[not15[\"YEAR\"]==yr[x]])])\n",
    "    topdf = pd.concat([topdf,j])\n",
    "    notdf = pd.concat([notdf,b])\n",
    "    analysisdf = pd.concat([topdf,notdf], axis = 1)\n",
    "\n",
    "analysisdf.index = yr\n",
    "analysisdf.columns = [\"Ref Success %\", \"Not Ref Success %\"]\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "analysisdf.plot(kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Success Rates by advertisement with Referral Program or not, Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a5f866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARING DATA FOR LIN REGRESS AND GETTING P VALUE\n",
    "top15apps = []\n",
    "not15apps = []\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    t = (top15[\"YEAR\"]==yr[x]).sum()\n",
    "    n = (not15[\"YEAR\"]==yr[x]).sum()\n",
    "    top15apps.append(t)\n",
    "    not15apps.append(n)\n",
    "analysisdf = analysisdf.reset_index()\n",
    "analysisdf[\"Ref Apps\"] = pd.DataFrame(top15apps)\n",
    "analysisdf[\"Not Ref Apps\"] = pd.DataFrame(not15apps)\n",
    "analysisdf = analysisdf.set_index(\"index\")\n",
    "# PLOTTING APPLICATION RATES OVER TIME BY TOP 15 or NOT TOP 15\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "analysisdf.reset_index().plot(x = \"index\", y = [\"Ref Apps\",\"Not Ref Apps\"], kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Application Rates by advertisement with Referral Program or not, Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809bf53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P VALUE PER YEAR: ON THE SUCCESS RATE\n",
    "top_not = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = analysisdf.loc[yr[x]][\"Ref Success %\"]\n",
    "    sample2_phat = analysisdf.loc[yr[x]][\"Not Ref Success %\"]\n",
    "    sample1_size = analysisdf.loc[yr[x]][\"Ref Apps\"]\n",
    "    sample2_size = analysisdf.loc[yr[x]][\"Not Ref Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    top_not.append(pval)\n",
    "\n",
    "pvaldf = pd.DataFrame(top_not)\n",
    "pvaldf.columns = [\"Ref Vs. Not Ref\"]\n",
    "pvaldf.index = yr\n",
    "analysisdf[\"PVAL Success %\"] = pvaldf[\"Ref Vs. Not Ref\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492b5796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION BETWEEN Professional Occupation AND SUCCESS RATE OVER TIME\n",
    "print(\"Ref success rate over time:\",st.linregress(y = analysisdf[\"Ref Success %\"], x = yr))\n",
    "print(\"\\nRef applications over time:\",st.linregress(y = analysisdf[\"Ref Apps\"], x = yr))\n",
    "\n",
    "print(\"\\n\\nNot Ref success rate over time:\",st.linregress(y = analysisdf[\"Not Ref Success %\"], x = yr))\n",
    "print(\"\\nNot Ref applications over time:\",st.linregress(y = analysisdf[\"Not Ref Apps\"], x = yr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c22e076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING THE TOTAL PVAL\n",
    "analysisdf.loc[\"ALL YRS\"] = analysisdf.sum()\n",
    "top15suc = []\n",
    "not15suc = []\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    t15 = analysisdf.loc[yr[x]].iloc[0]*analysisdf.loc[yr[x]].iloc[2]\n",
    "    n15 = analysisdf.loc[yr[x]].iloc[1]*analysisdf.loc[yr[x]].iloc[3]\n",
    "    top15suc.append(t15)\n",
    "    not15suc.append(n15)\n",
    "analysisdf.loc[\"ALL YRS\"].iloc[0] = sum(top15suc)/analysisdf.loc[\"ALL YRS\"].iloc[2]\n",
    "analysisdf.loc[\"ALL YRS\"].iloc[1] = sum(not15suc)/analysisdf.loc[\"ALL YRS\"].iloc[3]\n",
    "\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = analysisdf.loc[\"ALL YRS\"].iloc[0]\n",
    "    sample2_phat = analysisdf.loc[\"ALL YRS\"].iloc[1]\n",
    "    sample1_size = analysisdf.loc[\"ALL YRS\"].iloc[2]\n",
    "    sample2_size = analysisdf.loc[\"ALL YRS\"].iloc[3]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "analysisdf.loc[\"ALL YRS\"].iloc[4] = pval\n",
    "\n",
    "analysisdf = analysisdf.head(9).style.format(\"{:,.2f}\")\n",
    "analysisdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e848be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DIVING DEEPER INTO THE REF PROGRAM!\n",
    "#ANCHORED BY STATE: PW SKILL,APPLICATIONS, MEDIAN WAGE, YRS POST GRADE, MEDIAN DAYS\n",
    "\n",
    "code = 1\n",
    "# CHANGE THIS FOR ANY CODE\n",
    "\n",
    "# MEDIAN WAGE, MEDIAN YEARS AFTER GRADUATING, MEDIAN DAYS TO DECIDE : BY PW LEVEL\n",
    "topstate = df[df[\"EMPLOYEE_REF_PROG\"] == code][\"WORKSITE_STATE\"].value_counts()[:15].index.tolist()\n",
    "pwskill = df[\"PW_SKILL_LEVEL\"].dropna().unique().tolist()\n",
    "pwskill.sort()\n",
    "\n",
    "\n",
    "statelist = []\n",
    "pwlist = pwskill*len(topstate)\n",
    "apps = []\n",
    "medwage = []\n",
    "yrsgrad = []\n",
    "meddays = []\n",
    "\n",
    "\n",
    "for x in range(len(topstate)):\n",
    "    s = (topstate[x]+\",\")*len(pwskill)\n",
    "    s = \"\".join(s).split(\",\")\n",
    "    s = list(filter((\"\").__ne__, s))\n",
    "    statelist.append(s) \n",
    "for x in range(len(topstate)):\n",
    "    for j in range(len(pwskill)):\n",
    "        a = len(df[(df[\"EMPLOYEE_REF_PROG\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"EMPLOYEE_REF_PROG\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]])\n",
    "        apps.append(a)\n",
    "        b = df[(df[\"EMPLOYEE_REF_PROG\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"EMPLOYEE_REF_PROG\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]][\"WO_A\"].median()\n",
    "        medwage.append(b)\n",
    "        c = df[(df[\"EMPLOYEE_REF_PROG\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"EMPLOYEE_REF_PROG\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]][\"YRSPOSTGRAD\"].median()\n",
    "        yrsgrad.append(c)\n",
    "        d = df[(df[\"EMPLOYEE_REF_PROG\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"EMPLOYEE_REF_PROG\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]][\"DAYS_TO_DECIDE\"].median()\n",
    "        meddays.append(d)\n",
    "codestatedf = pd.DataFrame([sum(statelist, []),pwlist,apps,medwage,yrsgrad,meddays]).transpose()\n",
    "codestatedf.columns = [\"STATE\",\"PW LEVEL\", \"TOTAL APPLICATIONS\",\"MEDIAN WAGE\",\"MEDIAN YRS POST GRAD\",\"MEDIAN DAYS TO DECIDE\"]\n",
    "codestatedf = codestatedf.set_index([\"STATE\",\"PW LEVEL\"])\n",
    "codestatedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbb33c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANCHORED BY STATE: EDU LEVEL,APPLICATIONS, MEDIAN WAGE, YRS POST GRADE, MEDIAN DAYS\n",
    "# MEDIAN WAGE, MEDIAN YEARS AFTER GRADUATING, MEDIAN DAYS TO DECIDE : BY EDU LEVEL\n",
    "edu = df[\"FOREIGN_WORKER_EDUCATION\"].dropna().unique().tolist()\n",
    "edu[0],edu[1],edu[2],edu[3],edu[4],edu[5] = edu[3],edu[5],edu[0],edu[2],edu[1],edu[4]\n",
    "\n",
    "statelist = []\n",
    "edulist = edu*len(topstate)\n",
    "apps = []\n",
    "medwage = []\n",
    "yrsgrad = []\n",
    "meddays = []\n",
    "\n",
    "\n",
    "for x in range(len(topstate)):\n",
    "    s = (topstate[x]+\",\")*len(edu)\n",
    "    s = \"\".join(s).split(\",\")\n",
    "    s = list(filter((\"\").__ne__, s))\n",
    "    statelist.append(s) \n",
    "for x in range(len(topstate)):\n",
    "    for j in range(len(edu)):\n",
    "        a = len(df[(df[\"EMPLOYEE_REF_PROG\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"EMPLOYEE_REF_PROG\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]])\n",
    "        apps.append(a)\n",
    "        b = df[(df[\"EMPLOYEE_REF_PROG\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"EMPLOYEE_REF_PROG\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]][\"WO_A\"].median()\n",
    "        medwage.append(b)\n",
    "        c = df[(df[\"EMPLOYEE_REF_PROG\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"EMPLOYEE_REF_PROG\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]][\"YRSPOSTGRAD\"].median()\n",
    "        yrsgrad.append(c)\n",
    "        d = df[(df[\"EMPLOYEE_REF_PROG\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"EMPLOYEE_REF_PROG\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]][\"DAYS_TO_DECIDE\"].median()\n",
    "        meddays.append(d)\n",
    "codestatedf1 = pd.DataFrame([sum(statelist, []),edulist,apps,medwage,yrsgrad,meddays]).transpose()\n",
    "codestatedf1.columns = [\"STATE\",\"EDU LEVEL\", \"TOTAL APPLICATIONS\",\"MEDIAN WAGE\",\"MEDIAN YRS POST GRAD\",\"MEDIAN DAYS TO DECIDE\"]\n",
    "codestatedf1 = codestatedf1.set_index([\"STATE\",\"EDU LEVEL\"])\n",
    "codestatedf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283dfd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADVERTISEMENT THROUGH PROFESSIONAL ADVERTISEMENT?\n",
    "# PLOTTING THE DIFFERENT SUCCESS RATES OVER TIME BY employee ref or not (IN TERMS OF ANNUAL APPLICATIONS)\n",
    "\n",
    "top15 = df[df[\"PRO_ORG_AD\"]==1]\n",
    "not15 = df[df[\"PRO_ORG_AD\"]==0]\n",
    "\n",
    "topdf = pd.DataFrame()\n",
    "notdf = pd.DataFrame()\n",
    "analysisdf = pd.DataFrame()\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    j = pd.DataFrame([len(top15[(top15[\"YEAR\"]==yr[x])&(top15[\"CASE_STATUS\"]==1)])/len(top15[top15[\"YEAR\"]==yr[x]])])\n",
    "    b = pd.DataFrame([len(not15[(not15[\"YEAR\"]==yr[x])&(not15[\"CASE_STATUS\"]==1)])/len(not15[not15[\"YEAR\"]==yr[x]])])\n",
    "    topdf = pd.concat([topdf,j])\n",
    "    notdf = pd.concat([notdf,b])\n",
    "    analysisdf = pd.concat([topdf,notdf], axis = 1)\n",
    "\n",
    "analysisdf.index = yr\n",
    "analysisdf.columns = [\"ProOrg Success %\", \"Not ProOrg Success %\"]\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "analysisdf.plot(kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Success Rates by advertisement with Professional Organization or not, Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbea0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARING DATA FOR LIN REGRESS AND GETTING P VALUE\n",
    "top15apps = []\n",
    "not15apps = []\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    t = (top15[\"YEAR\"]==yr[x]).sum()\n",
    "    n = (not15[\"YEAR\"]==yr[x]).sum()\n",
    "    top15apps.append(t)\n",
    "    not15apps.append(n)\n",
    "analysisdf = analysisdf.reset_index()\n",
    "analysisdf[\"ProOrg Apps\"] = pd.DataFrame(top15apps)\n",
    "analysisdf[\"Not ProOrg Apps\"] = pd.DataFrame(not15apps)\n",
    "analysisdf = analysisdf.set_index(\"index\")\n",
    "# PLOTTING APPLICATION RATES OVER TIME BY TOP 15 or NOT TOP 15\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "analysisdf.reset_index().plot(x = \"index\", y = [\"ProOrg Apps\",\"Not ProOrg Apps\"], kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Application Rates by advertisement with Professional Organization or not, Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a767d99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P VALUE PER YEAR: ON THE SUCCESS RATE\n",
    "top_not = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = analysisdf.loc[yr[x]][\"ProOrg Success %\"]\n",
    "    sample2_phat = analysisdf.loc[yr[x]][\"Not ProOrg Success %\"]\n",
    "    sample1_size = analysisdf.loc[yr[x]][\"ProOrg Apps\"]\n",
    "    sample2_size = analysisdf.loc[yr[x]][\"Not ProOrg Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    top_not.append(pval)\n",
    "\n",
    "pvaldf = pd.DataFrame(top_not)\n",
    "pvaldf.columns = [\"ProOrg Vs. Not ProOrg\"]\n",
    "pvaldf.index = yr\n",
    "analysisdf[\"PVAL Success %\"] = pvaldf[\"ProOrg Vs. Not ProOrg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5087be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION BETWEEN Professional Advertisement AND SUCCESS RATE OVER TIME\n",
    "print(\"ProOrg success rate over time:\",st.linregress(y = analysisdf[\"ProOrg Success %\"], x = yr))\n",
    "print(\"\\nProOrg applications over time:\",st.linregress(y = analysisdf[\"ProOrg Apps\"], x = yr))\n",
    "\n",
    "print(\"\\n\\nNot ProOrg success rate over time:\",st.linregress(y = analysisdf[\"Not ProOrg Success %\"], x = yr))\n",
    "print(\"\\nNot ProOrg applications over time:\",st.linregress(y = analysisdf[\"Not ProOrg Apps\"], x = yr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff23f2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING THE TOTAL PVAL\n",
    "analysisdf.loc[\"ALL YRS\"] = analysisdf.sum()\n",
    "top15suc = []\n",
    "not15suc = []\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    t15 = analysisdf.loc[yr[x]].iloc[0]*analysisdf.loc[yr[x]].iloc[2]\n",
    "    n15 = analysisdf.loc[yr[x]].iloc[1]*analysisdf.loc[yr[x]].iloc[3]\n",
    "    top15suc.append(t15)\n",
    "    not15suc.append(n15)\n",
    "analysisdf.loc[\"ALL YRS\"].iloc[0] = sum(top15suc)/analysisdf.loc[\"ALL YRS\"].iloc[2]\n",
    "analysisdf.loc[\"ALL YRS\"].iloc[1] = sum(not15suc)/analysisdf.loc[\"ALL YRS\"].iloc[3]\n",
    "\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = analysisdf.loc[\"ALL YRS\"].iloc[0]\n",
    "    sample2_phat = analysisdf.loc[\"ALL YRS\"].iloc[1]\n",
    "    sample1_size = analysisdf.loc[\"ALL YRS\"].iloc[2]\n",
    "    sample2_size = analysisdf.loc[\"ALL YRS\"].iloc[3]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "analysisdf.loc[\"ALL YRS\"].iloc[4] = pval\n",
    "\n",
    "analysisdf = analysisdf.head(9).style.format(\"{:,.2f}\")\n",
    "analysisdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe9ed8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DIVING DEEPER INTO THE pro org ad\n",
    "#ANCHORED BY STATE: PW SKILL,APPLICATIONS, MEDIAN WAGE, YRS POST GRADE, MEDIAN DAYS\n",
    "\n",
    "code = 1\n",
    "# CHANGE THIS FOR ANY CODE\n",
    "\n",
    "# MEDIAN WAGE, MEDIAN YEARS AFTER GRADUATING, MEDIAN DAYS TO DECIDE : BY PW LEVEL\n",
    "topstate = df[df[\"PRO_ORG_AD\"] == code][\"WORKSITE_STATE\"].value_counts()[:15].index.tolist()\n",
    "pwskill = df[\"PW_SKILL_LEVEL\"].dropna().unique().tolist()\n",
    "pwskill.sort()\n",
    "\n",
    "\n",
    "statelist = []\n",
    "pwlist = pwskill*len(topstate)\n",
    "apps = []\n",
    "medwage = []\n",
    "yrsgrad = []\n",
    "meddays = []\n",
    "\n",
    "\n",
    "for x in range(len(topstate)):\n",
    "    s = (topstate[x]+\",\")*len(pwskill)\n",
    "    s = \"\".join(s).split(\",\")\n",
    "    s = list(filter((\"\").__ne__, s))\n",
    "    statelist.append(s) \n",
    "for x in range(len(topstate)):\n",
    "    for j in range(len(pwskill)):\n",
    "        a = len(df[(df[\"PRO_ORG_AD\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"PRO_ORG_AD\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]])\n",
    "        apps.append(a)\n",
    "        b = df[(df[\"PRO_ORG_AD\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"PRO_ORG_AD\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]][\"WO_A\"].median()\n",
    "        medwage.append(b)\n",
    "        c = df[(df[\"PRO_ORG_AD\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"PRO_ORG_AD\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]][\"YRSPOSTGRAD\"].median()\n",
    "        yrsgrad.append(c)\n",
    "        d = df[(df[\"PRO_ORG_AD\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"PRO_ORG_AD\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]][\"DAYS_TO_DECIDE\"].median()\n",
    "        meddays.append(d)\n",
    "codestatedf = pd.DataFrame([sum(statelist, []),pwlist,apps,medwage,yrsgrad,meddays]).transpose()\n",
    "codestatedf.columns = [\"STATE\",\"PW LEVEL\", \"TOTAL APPLICATIONS\",\"MEDIAN WAGE\",\"MEDIAN YRS POST GRAD\",\"MEDIAN DAYS TO DECIDE\"]\n",
    "codestatedf = codestatedf.set_index([\"STATE\",\"PW LEVEL\"])\n",
    "codestatedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3286087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANCHORED BY STATE: EDU LEVEL,APPLICATIONS, MEDIAN WAGE, YRS POST GRADE, MEDIAN DAYS\n",
    "# MEDIAN WAGE, MEDIAN YEARS AFTER GRADUATING, MEDIAN DAYS TO DECIDE : BY EDU LEVEL\n",
    "edu = df[\"FOREIGN_WORKER_EDUCATION\"].dropna().unique().tolist()\n",
    "edu[0],edu[1],edu[2],edu[3],edu[4],edu[5] = edu[3],edu[5],edu[0],edu[2],edu[1],edu[4]\n",
    "\n",
    "statelist = []\n",
    "edulist = edu*len(topstate)\n",
    "apps = []\n",
    "medwage = []\n",
    "yrsgrad = []\n",
    "meddays = []\n",
    "\n",
    "\n",
    "for x in range(len(topstate)):\n",
    "    s = (topstate[x]+\",\")*len(edu)\n",
    "    s = \"\".join(s).split(\",\")\n",
    "    s = list(filter((\"\").__ne__, s))\n",
    "    statelist.append(s) \n",
    "for x in range(len(topstate)):\n",
    "    for j in range(len(edu)):\n",
    "        a = len(df[(df[\"PRO_ORG_AD\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"PRO_ORG_AD\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]])\n",
    "        apps.append(a)\n",
    "        b = df[(df[\"PRO_ORG_AD\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"PRO_ORG_AD\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]][\"WO_A\"].median()\n",
    "        medwage.append(b)\n",
    "        c = df[(df[\"PRO_ORG_AD\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"PRO_ORG_AD\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]][\"YRSPOSTGRAD\"].median()\n",
    "        yrsgrad.append(c)\n",
    "        d = df[(df[\"PRO_ORG_AD\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"PRO_ORG_AD\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]][\"DAYS_TO_DECIDE\"].median()\n",
    "        meddays.append(d)\n",
    "codestatedf1 = pd.DataFrame([sum(statelist, []),edulist,apps,medwage,yrsgrad,meddays]).transpose()\n",
    "codestatedf1.columns = [\"STATE\",\"EDU LEVEL\", \"TOTAL APPLICATIONS\",\"MEDIAN WAGE\",\"MEDIAN YRS POST GRAD\",\"MEDIAN DAYS TO DECIDE\"]\n",
    "codestatedf1 = codestatedf1.set_index([\"STATE\",\"EDU LEVEL\"])\n",
    "codestatedf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342ffb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AP TO PW DETERMINE SPEED\n",
    "# CREATING A NEW COLUMN: APP GROUP, SPLITS DAYS INTO THIRDS\n",
    "df[\"APP_TO_PW_DETERMINE\"] = df[\"APP_TO_PW_DETERMINE\"].replace(\"#VALUE!\",np.nan).astype(float)\n",
    "high = df.groupby(\"YEAR\")[\"APP_TO_PW_DETERMINE\"].quantile(2/3).tolist()\n",
    "low = df.groupby(\"YEAR\")[\"APP_TO_PW_DETERMINE\"].quantile(1/3).tolist()\n",
    "df[\"APP_GROUP\"] = np.nan\n",
    "for x in range(len(yr)):\n",
    "    df[\"APP_GROUP\"] = np.where((df[\"YEAR\"]==yr[x])&(df[\"APP_TO_PW_DETERMINE\"]<low[x]),\"FAST\",df[\"APP_GROUP\"])\n",
    "    df[\"APP_GROUP\"] = np.where((df[\"YEAR\"]==yr[x])&(df[\"APP_TO_PW_DETERMINE\"]>=low[x])&(df[\"APP_TO_PW_DETERMINE\"]<high[x]),\"MEDIUM\",df[\"APP_GROUP\"])\n",
    "    df[\"APP_GROUP\"] = np.where((df[\"YEAR\"]==yr[x])&(df[\"APP_TO_PW_DETERMINE\"]>=high[x]),\"SLOW\",df[\"APP_GROUP\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6180655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRELATION ANALYSIS: AGGREGATE BY SIZE BRACKETS AND YEARLY\n",
    "# PLOTTING SUCCESS RATES OVER TIME BY SIZE OF COMPANY\n",
    "\n",
    "young = df[df[\"APP_GROUP\"]==\"FAST\"]\n",
    "mid = df[df[\"APP_GROUP\"]==\"MEDIUM\"]\n",
    "old = df[df[\"APP_GROUP\"]==\"SLOW\"]\n",
    "\n",
    "\n",
    "youndf = pd.DataFrame()\n",
    "middf = pd.DataFrame()\n",
    "olddf = pd.DataFrame()\n",
    "linregdf = pd.DataFrame()\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    j = pd.DataFrame([len(young[(young[\"YEAR\"]==yr[x])&(young[\"CASE_STATUS\"]==1)])/len(young[young[\"YEAR\"]==yr[x]])])\n",
    "    med = pd.DataFrame([len(mid[(mid[\"YEAR\"]==yr[x])&(mid[\"CASE_STATUS\"]==1)])/len(mid[mid[\"YEAR\"]==yr[x]])])\n",
    "    oldy = pd.DataFrame([len(old[(old[\"YEAR\"]==yr[x])&(old[\"CASE_STATUS\"]==1)])/len(old[old[\"YEAR\"]==yr[x]])])\n",
    "    youndf = pd.concat([youndf,j])\n",
    "    middf = pd.concat([middf,med])\n",
    "    olddf = pd.concat([olddf,oldy])\n",
    "    linregdf = pd.concat([youndf,middf,olddf], axis = 1)\n",
    "    \n",
    "\n",
    "linregdf.index = yr\n",
    "linregdf.columns = [\"Fast Success %\", \"Medium Success %\", \"Slow Success %\"]\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "linregdf.plot(kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Success Rates by Time To Apply Since PW Determine, Over Time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93239eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARING DATA FOR LIN REGRESS AND GETTING P VALUE\n",
    "youngapps = []\n",
    "midapps = []\n",
    "oldapps = []\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    y = (young[\"YEAR\"]==yr[x]).sum()\n",
    "    m = (mid[\"YEAR\"]==yr[x]).sum()\n",
    "    o = (old[\"YEAR\"]==yr[x]).sum()\n",
    "    youngapps.append(y)\n",
    "    midapps.append(m)\n",
    "    oldapps.append(o)\n",
    "linregdf = linregdf.reset_index()\n",
    "linregdf[\"Fast Apps\"] = pd.DataFrame(youngapps)\n",
    "linregdf[\"Medium Apps\"] = pd.DataFrame(midapps)\n",
    "linregdf[\"Slow Apps\"] = pd.DataFrame(oldapps)\n",
    "linregdf = linregdf.set_index(\"index\")\n",
    "# PLOTTING APPLICATION RATES OVER TIME BY SIZE OF COMPANY\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "linregdf.reset_index().plot(x = \"index\", y = [\"Fast Apps\",\"Medium Apps\",\"Slow Apps\"], kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Application Rates by Time To Apply Since PW Determine, Over Time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3915efe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P VALUE PER YEAR\n",
    "young_med = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[yr[x]][\"Fast Success %\"]\n",
    "    sample2_phat = linregdf.loc[yr[x]][\"Medium Success %\"]\n",
    "    sample1_size = linregdf.loc[yr[x]][\"Fast Apps\"]\n",
    "    sample2_size = linregdf.loc[yr[x]][\"Medium Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    young_med.append(pval)\n",
    "\n",
    "pvaldf = pd.DataFrame(young_med)\n",
    "\n",
    "young_old = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[yr[x]][\"Fast Success %\"]\n",
    "    sample2_phat = linregdf.loc[yr[x]][\"Slow Success %\"]\n",
    "    sample1_size = linregdf.loc[yr[x]][\"Fast Apps\"]\n",
    "    sample2_size = linregdf.loc[yr[x]][\"Slow Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    young_old.append(pval)\n",
    "pvaldf = pd.concat([pvaldf,pd.DataFrame(young_old)], axis = 1)\n",
    "\n",
    "med_old = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[yr[x]][\"Medium Success %\"]\n",
    "    sample2_phat = linregdf.loc[yr[x]][\"Slow Success %\"]\n",
    "    sample1_size = linregdf.loc[yr[x]][\"Medium Apps\"]\n",
    "    sample2_size = linregdf.loc[yr[x]][\"Slow Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    med_old.append(pval)\n",
    "pvaldf = pd.concat([pvaldf,pd.DataFrame(med_old)], axis = 1)\n",
    "pvaldf.columns = [\"Fast_Med\",\"Fast_Slow\",\"Med_Slow\"]\n",
    "pvaldf.index = yr\n",
    "linregdf[[\"Fast_Med\",\"Fast_Slow\",\"Med_Slow\"]] = pvaldf[[\"Fast_Med\",\"Fast_Slow\",\"Med_Slow\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1ec92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION BETWEEN Speed to Apply and success\n",
    "print(\"Fast success rate over time:\",st.linregress(y = linregdf[\"Fast Success %\"], x = yr))\n",
    "print(\"\\nFast applications over time:\",st.linregress(y = linregdf[\"Fast Apps\"], x = yr))\n",
    "\n",
    "print(\"\\n\\nMedium success rate over time:\",st.linregress(y = linregdf[\"Medium Success %\"], x = yr))\n",
    "print(\"\\nMedium applications over time:\",st.linregress(y = linregdf[\"Medium Apps\"], x = yr))\n",
    "\n",
    "print(\"\\n\\nSlow success rate over time:\",st.linregress(y = linregdf[\"Slow Success %\"], x = yr))\n",
    "print(\"\\nSlow applications over time:\",st.linregress(y = linregdf[\"Slow Apps\"], x = yr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e32cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING THE TOTAL PVAL\n",
    "linregdf.loc[\"ALL YRS\"] = linregdf.sum()\n",
    "youngsuc = []\n",
    "medsuc = []\n",
    "oldsuc = []\n",
    "for x in range(len(yr)):\n",
    "    y = linregdf.loc[yr[x]].iloc[0]*linregdf.loc[yr[x]].iloc[3]\n",
    "    m = linregdf.loc[yr[x]].iloc[1]*linregdf.loc[yr[x]].iloc[4]\n",
    "    o = linregdf.loc[yr[x]].iloc[2]*linregdf.loc[yr[x]].iloc[5]\n",
    "    youngsuc.append(y)\n",
    "    medsuc.append(m)\n",
    "    oldsuc.append(o)\n",
    "linregdf.loc[\"ALL YRS\"].iloc[0] = sum(youngsuc)/linregdf.loc[\"ALL YRS\"].iloc[3]\n",
    "linregdf.loc[\"ALL YRS\"].iloc[1] = sum(medsuc)/linregdf.loc[\"ALL YRS\"].iloc[4]\n",
    "linregdf.loc[\"ALL YRS\"].iloc[2] = sum(oldsuc)/linregdf.loc[\"ALL YRS\"].iloc[5]\n",
    "\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[\"ALL YRS\"].iloc[0]\n",
    "    sample2_phat = linregdf.loc[\"ALL YRS\"].iloc[1]\n",
    "    sample1_size = linregdf.loc[\"ALL YRS\"].iloc[3]\n",
    "    sample2_size = linregdf.loc[\"ALL YRS\"].iloc[4]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "linregdf.loc[\"ALL YRS\"].iloc[6] = pval\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[\"ALL YRS\"].iloc[0]\n",
    "    sample2_phat = linregdf.loc[\"ALL YRS\"].iloc[2]\n",
    "    sample1_size = linregdf.loc[\"ALL YRS\"].iloc[3]\n",
    "    sample2_size = linregdf.loc[\"ALL YRS\"].iloc[5]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "linregdf.loc[\"ALL YRS\"].iloc[7] = pval\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = linregdf.loc[\"ALL YRS\"].iloc[1]\n",
    "    sample2_phat = linregdf.loc[\"ALL YRS\"].iloc[2]\n",
    "    sample1_size = linregdf.loc[\"ALL YRS\"].iloc[4]\n",
    "    sample2_size = linregdf.loc[\"ALL YRS\"].iloc[5]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "linregdf.loc[\"ALL YRS\"].iloc[8] = pval\n",
    "linregdf = linregdf.head(9).style.format(\"{:,.2f}\")\n",
    "linregdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5abbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADVERTISEMENT THROUGH PVT EMPLOYMENT\n",
    "# PLOTTING THE DIFFERENT SUCCESS RATES OVER TIME BY PVT FIRM or not (IN TERMS OF ANNUAL APPLICATIONS)\n",
    "\n",
    "top15 = df[df[\"PVT_EMPLOYMENT_FIRM\"]==1]\n",
    "not15 = df[df[\"PVT_EMPLOYMENT_FIRM\"]==0]\n",
    "\n",
    "topdf = pd.DataFrame()\n",
    "notdf = pd.DataFrame()\n",
    "analysisdf = pd.DataFrame()\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    j = pd.DataFrame([len(top15[(top15[\"YEAR\"]==yr[x])&(top15[\"CASE_STATUS\"]==1)])/len(top15[top15[\"YEAR\"]==yr[x]])])\n",
    "    b = pd.DataFrame([len(not15[(not15[\"YEAR\"]==yr[x])&(not15[\"CASE_STATUS\"]==1)])/len(not15[not15[\"YEAR\"]==yr[x]])])\n",
    "    topdf = pd.concat([topdf,j])\n",
    "    notdf = pd.concat([notdf,b])\n",
    "    analysisdf = pd.concat([topdf,notdf], axis = 1)\n",
    "\n",
    "analysisdf.index = yr\n",
    "analysisdf.columns = [\"PVT Success %\", \"Not PVT Success %\"]\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "analysisdf.plot(kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Success Rates by advertisement with Private Employment Firm or not, Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4235db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARING DATA FOR LIN REGRESS AND GETTING P VALUE\n",
    "top15apps = []\n",
    "not15apps = []\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    t = (top15[\"YEAR\"]==yr[x]).sum()\n",
    "    n = (not15[\"YEAR\"]==yr[x]).sum()\n",
    "    top15apps.append(t)\n",
    "    not15apps.append(n)\n",
    "analysisdf = analysisdf.reset_index()\n",
    "analysisdf[\"PVT Apps\"] = pd.DataFrame(top15apps)\n",
    "analysisdf[\"Not PVT Apps\"] = pd.DataFrame(not15apps)\n",
    "analysisdf = analysisdf.set_index(\"index\")\n",
    "# PLOTTING APPLICATION RATES OVER TIME BY TOP 15 or NOT TOP 15\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "analysisdf.reset_index().plot(x = \"index\", y = [\"PVT Apps\",\"Not PVT Apps\"], kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Application Rates by advertisement with Private Employment Firm or not, Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdb29d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P VALUE PER YEAR: ON THE SUCCESS RATE\n",
    "top_not = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = analysisdf.loc[yr[x]][\"PVT Success %\"]\n",
    "    sample2_phat = analysisdf.loc[yr[x]][\"Not PVT Success %\"]\n",
    "    sample1_size = analysisdf.loc[yr[x]][\"PVT Apps\"]\n",
    "    sample2_size = analysisdf.loc[yr[x]][\"Not PVT Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    top_not.append(pval)\n",
    "\n",
    "pvaldf = pd.DataFrame(top_not)\n",
    "pvaldf.columns = [\"PVT Vs. Not PVT\"]\n",
    "pvaldf.index = yr\n",
    "analysisdf[\"PVAL Success %\"] = pvaldf[\"PVT Vs. Not PVT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ca72c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION BETWEEN private employment firm AND SUCCESS RATE OVER TIME\n",
    "print(\"PVT success rate over time:\",st.linregress(y = analysisdf[\"PVT Success %\"], x = yr))\n",
    "print(\"\\nPVT applications over time:\",st.linregress(y = analysisdf[\"PVT Apps\"], x = yr))\n",
    "\n",
    "print(\"\\n\\nNot PVT success rate over time:\",st.linregress(y = analysisdf[\"Not PVT Success %\"], x = yr))\n",
    "print(\"\\nNot PVT applications over time:\",st.linregress(y = analysisdf[\"Not PVT Apps\"], x = yr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c695592d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING THE TOTAL PVAL\n",
    "analysisdf.loc[\"ALL YRS\"] = analysisdf.sum()\n",
    "top15suc = []\n",
    "not15suc = []\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    t15 = analysisdf.loc[yr[x]].iloc[0]*analysisdf.loc[yr[x]].iloc[2]\n",
    "    n15 = analysisdf.loc[yr[x]].iloc[1]*analysisdf.loc[yr[x]].iloc[3]\n",
    "    top15suc.append(t15)\n",
    "    not15suc.append(n15)\n",
    "analysisdf.loc[\"ALL YRS\"].iloc[0] = sum(top15suc)/analysisdf.loc[\"ALL YRS\"].iloc[2]\n",
    "analysisdf.loc[\"ALL YRS\"].iloc[1] = sum(not15suc)/analysisdf.loc[\"ALL YRS\"].iloc[3]\n",
    "\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = analysisdf.loc[\"ALL YRS\"].iloc[0]\n",
    "    sample2_phat = analysisdf.loc[\"ALL YRS\"].iloc[1]\n",
    "    sample1_size = analysisdf.loc[\"ALL YRS\"].iloc[2]\n",
    "    sample2_size = analysisdf.loc[\"ALL YRS\"].iloc[3]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "analysisdf.loc[\"ALL YRS\"].iloc[4] = pval\n",
    "\n",
    "analysisdf = analysisdf.head(9).style.format(\"{:,.2f}\")\n",
    "analysisdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261412f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EMPLOYEE RELATED ANALYSIS ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cce197",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLASS OF ADMISSION\n",
    "df[\"CLASS_OF_ADMISSION\"].value_counts()[:15].iplot(kind = \"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ced62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H1B really different??\n",
    "# PLOTTING THE DIFFERENT SUCCESS RATES OVER TIME BY class (IN TERMS OF ANNUAL APPLICATIONS)\n",
    "\n",
    "top15 = df[df[\"CLASS_OF_ADMISSION\"]==\"H-1B\"]\n",
    "not15 = df[df[\"CLASS_OF_ADMISSION\"]!=\"H-1B\"]\n",
    "\n",
    "topdf = pd.DataFrame()\n",
    "notdf = pd.DataFrame()\n",
    "analysisdf = pd.DataFrame()\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    j = pd.DataFrame([len(top15[(top15[\"YEAR\"]==yr[x])&(top15[\"CASE_STATUS\"]==1)])/len(top15[top15[\"YEAR\"]==yr[x]])])\n",
    "    b = pd.DataFrame([len(not15[(not15[\"YEAR\"]==yr[x])&(not15[\"CASE_STATUS\"]==1)])/len(not15[not15[\"YEAR\"]==yr[x]])])\n",
    "    topdf = pd.concat([topdf,j])\n",
    "    notdf = pd.concat([notdf,b])\n",
    "    analysisdf = pd.concat([topdf,notdf], axis = 1)\n",
    "\n",
    "analysisdf.index = yr\n",
    "analysisdf.columns = [\"H1B Success %\", \"Not H1B Success %\"]\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "analysisdf.plot(kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Success Rates by H1B or not, Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5593e94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARING DATA FOR LIN REGRESS AND GETTING P VALUE\n",
    "top15apps = []\n",
    "not15apps = []\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    t = (top15[\"YEAR\"]==yr[x]).sum()\n",
    "    n = (not15[\"YEAR\"]==yr[x]).sum()\n",
    "    top15apps.append(t)\n",
    "    not15apps.append(n)\n",
    "analysisdf = analysisdf.reset_index()\n",
    "analysisdf[\"H1B Apps\"] = pd.DataFrame(top15apps)\n",
    "analysisdf[\"Not H1B Apps\"] = pd.DataFrame(not15apps)\n",
    "analysisdf = analysisdf.set_index(\"index\")\n",
    "# PLOTTING APPLICATION RATES OVER TIME BY TOP 15 or NOT TOP 15\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "analysisdf.reset_index().plot(x = \"index\", y = [\"H1B Apps\",\"Not H1B Apps\"], kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Application Rates by H1B or not, Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e15064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P VALUE PER YEAR: ON THE SUCCESS RATE\n",
    "top_not = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = analysisdf.loc[yr[x]][\"H1B Success %\"]\n",
    "    sample2_phat = analysisdf.loc[yr[x]][\"Not H1B Success %\"]\n",
    "    sample1_size = analysisdf.loc[yr[x]][\"H1B Apps\"]\n",
    "    sample2_size = analysisdf.loc[yr[x]][\"Not H1B Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    top_not.append(pval)\n",
    "\n",
    "pvaldf = pd.DataFrame(top_not)\n",
    "pvaldf.columns = [\"H1B Vs. Not H1B\"]\n",
    "pvaldf.index = yr\n",
    "analysisdf[\"PVAL Success %\"] = pvaldf[\"H1B Vs. Not H1B\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461357b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION BETWEEN Professional Occupation AND SUCCESS RATE OVER TIME\n",
    "print(\"H1B success rate over time:\",st.linregress(y = analysisdf[\"H1B Success %\"], x = yr))\n",
    "print(\"\\nH1B applications over time:\",st.linregress(y = analysisdf[\"H1B Apps\"], x = yr))\n",
    "\n",
    "print(\"\\n\\nNot H1B success rate over time:\",st.linregress(y = analysisdf[\"Not H1B Success %\"], x = yr))\n",
    "print(\"\\nNot H1B applications over time:\",st.linregress(y = analysisdf[\"Not H1B Apps\"], x = yr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b43ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING THE TOTAL PVAL\n",
    "analysisdf.loc[\"ALL YRS\"] = analysisdf.sum()\n",
    "top15suc = []\n",
    "not15suc = []\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    t15 = analysisdf.loc[yr[x]].iloc[0]*analysisdf.loc[yr[x]].iloc[2]\n",
    "    n15 = analysisdf.loc[yr[x]].iloc[1]*analysisdf.loc[yr[x]].iloc[3]\n",
    "    top15suc.append(t15)\n",
    "    not15suc.append(n15)\n",
    "analysisdf.loc[\"ALL YRS\"].iloc[0] = sum(top15suc)/analysisdf.loc[\"ALL YRS\"].iloc[2]\n",
    "analysisdf.loc[\"ALL YRS\"].iloc[1] = sum(not15suc)/analysisdf.loc[\"ALL YRS\"].iloc[3]\n",
    "\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = analysisdf.loc[\"ALL YRS\"].iloc[0]\n",
    "    sample2_phat = analysisdf.loc[\"ALL YRS\"].iloc[1]\n",
    "    sample1_size = analysisdf.loc[\"ALL YRS\"].iloc[2]\n",
    "    sample2_size = analysisdf.loc[\"ALL YRS\"].iloc[3]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "analysisdf.loc[\"ALL YRS\"].iloc[4] = pval\n",
    "\n",
    "analysisdf = analysisdf.head(9).style.format(\"{:,.2f}\")\n",
    "analysisdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f5d953",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DIVING DEEPER INTO H1B\n",
    "#ANCHORED BY STATE: PW SKILL,APPLICATIONS, MEDIAN WAGE, YRS POST GRADE, MEDIAN DAYS\n",
    "\n",
    "code = \"H-1B\"\n",
    "# CHANGE THIS FOR ANY CODE\n",
    "\n",
    "# MEDIAN WAGE, MEDIAN YEARS AFTER GRADUATING, MEDIAN DAYS TO DECIDE : BY PW LEVEL\n",
    "topstate = df[df[\"CLASS_OF_ADMISSION\"] == code][\"WORKSITE_STATE\"].value_counts()[:15].index.tolist()\n",
    "pwskill = df[\"PW_SKILL_LEVEL\"].dropna().unique().tolist()\n",
    "pwskill.sort()\n",
    "\n",
    "\n",
    "statelist = []\n",
    "pwlist = pwskill*len(topstate)\n",
    "apps = []\n",
    "medwage = []\n",
    "yrsgrad = []\n",
    "meddays = []\n",
    "\n",
    "\n",
    "for x in range(len(topstate)):\n",
    "    s = (topstate[x]+\",\")*len(pwskill)\n",
    "    s = \"\".join(s).split(\",\")\n",
    "    s = list(filter((\"\").__ne__, s))\n",
    "    statelist.append(s) \n",
    "for x in range(len(topstate)):\n",
    "    for j in range(len(pwskill)):\n",
    "        a = len(df[(df[\"CLASS_OF_ADMISSION\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"CLASS_OF_ADMISSION\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]])\n",
    "        apps.append(a)\n",
    "        b = df[(df[\"CLASS_OF_ADMISSION\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"CLASS_OF_ADMISSION\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]][\"WO_A\"].median()\n",
    "        medwage.append(b)\n",
    "        c = df[(df[\"CLASS_OF_ADMISSION\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"CLASS_OF_ADMISSION\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]][\"YRSPOSTGRAD\"].median()\n",
    "        yrsgrad.append(c)\n",
    "        d = df[(df[\"CLASS_OF_ADMISSION\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"CLASS_OF_ADMISSION\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]][\"DAYS_TO_DECIDE\"].median()\n",
    "        meddays.append(d)\n",
    "codestatedf = pd.DataFrame([sum(statelist, []),pwlist,apps,medwage,yrsgrad,meddays]).transpose()\n",
    "codestatedf.columns = [\"STATE\",\"PW LEVEL\", \"TOTAL APPLICATIONS\",\"MEDIAN WAGE\",\"MEDIAN YRS POST GRAD\",\"MEDIAN DAYS TO DECIDE\"]\n",
    "codestatedf = codestatedf.set_index([\"STATE\",\"PW LEVEL\"])\n",
    "codestatedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019ff501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANCHORED BY STATE: EDU LEVEL,APPLICATIONS, MEDIAN WAGE, YRS POST GRADE, MEDIAN DAYS\n",
    "# MEDIAN WAGE, MEDIAN YEARS AFTER GRADUATING, MEDIAN DAYS TO DECIDE : BY EDU LEVEL\n",
    "edu = df[\"FOREIGN_WORKER_EDUCATION\"].dropna().unique().tolist()\n",
    "edu[0],edu[1],edu[2],edu[3],edu[4],edu[5] = edu[3],edu[5],edu[0],edu[2],edu[1],edu[4]\n",
    "\n",
    "statelist = []\n",
    "edulist = edu*len(topstate)\n",
    "apps = []\n",
    "medwage = []\n",
    "yrsgrad = []\n",
    "meddays = []\n",
    "\n",
    "\n",
    "for x in range(len(topstate)):\n",
    "    s = (topstate[x]+\",\")*len(edu)\n",
    "    s = \"\".join(s).split(\",\")\n",
    "    s = list(filter((\"\").__ne__, s))\n",
    "    statelist.append(s) \n",
    "for x in range(len(topstate)):\n",
    "    for j in range(len(edu)):\n",
    "        a = len(df[(df[\"CLASS_OF_ADMISSION\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"CLASS_OF_ADMISSION\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]])\n",
    "        apps.append(a)\n",
    "        b = df[(df[\"CLASS_OF_ADMISSION\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"CLASS_OF_ADMISSION\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]][\"WO_A\"].median()\n",
    "        medwage.append(b)\n",
    "        c = df[(df[\"CLASS_OF_ADMISSION\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"CLASS_OF_ADMISSION\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]][\"YRSPOSTGRAD\"].median()\n",
    "        yrsgrad.append(c)\n",
    "        d = df[(df[\"CLASS_OF_ADMISSION\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"CLASS_OF_ADMISSION\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]][\"DAYS_TO_DECIDE\"].median()\n",
    "        meddays.append(d)\n",
    "codestatedf1 = pd.DataFrame([sum(statelist, []),edulist,apps,medwage,yrsgrad,meddays]).transpose()\n",
    "codestatedf1.columns = [\"STATE\",\"EDU LEVEL\", \"TOTAL APPLICATIONS\",\"MEDIAN WAGE\",\"MEDIAN YRS POST GRAD\",\"MEDIAN DAYS TO DECIDE\"]\n",
    "codestatedf1 = codestatedf1.set_index([\"STATE\",\"EDU LEVEL\"])\n",
    "codestatedf1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef82c395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHAT ABOUT L1 OR F1 Vs THE REST - except for H1B...\n",
    "# ADVERTISEMENT THROUGH PROFESSIONAL ADVERTISEMENT?\n",
    "# PLOTTING THE DIFFERENT SUCCESS RATES OVER TIME BY employee ref or not (IN TERMS OF ANNUAL APPLICATIONS)\n",
    "\n",
    "top15 = df[(df[\"CLASS_OF_ADMISSION\"]==\"L-1\")|(df[\"CLASS_OF_ADMISSION\"]==\"F-1\")]\n",
    "not15 = df[(df[\"CLASS_OF_ADMISSION\"]!=\"L-1\")&(df[\"CLASS_OF_ADMISSION\"]!=\"F-1\")&(df[\"CLASS_OF_ADMISSION\"]!=\"H-1B\")]\n",
    "\n",
    "topdf = pd.DataFrame()\n",
    "notdf = pd.DataFrame()\n",
    "analysisdf = pd.DataFrame()\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    j = pd.DataFrame([len(top15[(top15[\"YEAR\"]==yr[x])&(top15[\"CASE_STATUS\"]==1)])/len(top15[top15[\"YEAR\"]==yr[x]])])\n",
    "    b = pd.DataFrame([len(not15[(not15[\"YEAR\"]==yr[x])&(not15[\"CASE_STATUS\"]==1)])/len(not15[not15[\"YEAR\"]==yr[x]])])\n",
    "    topdf = pd.concat([topdf,j])\n",
    "    notdf = pd.concat([notdf,b])\n",
    "    analysisdf = pd.concat([topdf,notdf], axis = 1)\n",
    "\n",
    "analysisdf.index = yr\n",
    "analysisdf.columns = [\"L1F1 Success %\", \"Not L1F1 Success %\"]\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "analysisdf.plot(kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Success Rates by L1/F1 or not, Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c97c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARING DATA FOR LIN REGRESS AND GETTING P VALUE\n",
    "top15apps = []\n",
    "not15apps = []\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    t = (top15[\"YEAR\"]==yr[x]).sum()\n",
    "    n = (not15[\"YEAR\"]==yr[x]).sum()\n",
    "    top15apps.append(t)\n",
    "    not15apps.append(n)\n",
    "analysisdf = analysisdf.reset_index()\n",
    "analysisdf[\"L1F1 Apps\"] = pd.DataFrame(top15apps)\n",
    "analysisdf[\"Not L1F1 Apps\"] = pd.DataFrame(not15apps)\n",
    "analysisdf = analysisdf.set_index(\"index\")\n",
    "# PLOTTING APPLICATION RATES OVER TIME BY TOP 15 or NOT TOP 15\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "analysisdf.reset_index().plot(x = \"index\", y = [\"L1F1 Apps\",\"Not L1F1 Apps\"], kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Application Rates by L1/F1 or not, Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81094949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P VALUE PER YEAR: ON THE SUCCESS RATE\n",
    "top_not = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = analysisdf.loc[yr[x]][\"L1F1 Success %\"]\n",
    "    sample2_phat = analysisdf.loc[yr[x]][\"Not L1F1 Success %\"]\n",
    "    sample1_size = analysisdf.loc[yr[x]][\"L1F1 Apps\"]\n",
    "    sample2_size = analysisdf.loc[yr[x]][\"Not L1F1 Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    top_not.append(pval)\n",
    "\n",
    "pvaldf = pd.DataFrame(top_not)\n",
    "pvaldf.columns = [\"L1F1 Vs. Not L1F1\"]\n",
    "pvaldf.index = yr\n",
    "analysisdf[\"PVAL Success %\"] = pvaldf[\"L1F1 Vs. Not L1F1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdabaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION BETWEEN L1/F1 Occupation AND SUCCESS RATE OVER TIME\n",
    "print(\"L1F1 success rate over time:\",st.linregress(y = analysisdf[\"L1F1 Success %\"], x = yr))\n",
    "print(\"\\nL1F1 applications over time:\",st.linregress(y = analysisdf[\"L1F1 Apps\"], x = yr))\n",
    "\n",
    "print(\"\\n\\nNot L1F1 success rate over time:\",st.linregress(y = analysisdf[\"Not L1F1 Success %\"], x = yr))\n",
    "print(\"\\nNot L1F1 applications over time:\",st.linregress(y = analysisdf[\"Not L1F1 Apps\"], x = yr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cda3d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING THE TOTAL PVAL\n",
    "analysisdf.loc[\"ALL YRS\"] = analysisdf.sum()\n",
    "top15suc = []\n",
    "not15suc = []\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    t15 = analysisdf.loc[yr[x]].iloc[0]*analysisdf.loc[yr[x]].iloc[2]\n",
    "    n15 = analysisdf.loc[yr[x]].iloc[1]*analysisdf.loc[yr[x]].iloc[3]\n",
    "    top15suc.append(t15)\n",
    "    not15suc.append(n15)\n",
    "analysisdf.loc[\"ALL YRS\"].iloc[0] = sum(top15suc)/analysisdf.loc[\"ALL YRS\"].iloc[2]\n",
    "analysisdf.loc[\"ALL YRS\"].iloc[1] = sum(not15suc)/analysisdf.loc[\"ALL YRS\"].iloc[3]\n",
    "\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = analysisdf.loc[\"ALL YRS\"].iloc[0]\n",
    "    sample2_phat = analysisdf.loc[\"ALL YRS\"].iloc[1]\n",
    "    sample1_size = analysisdf.loc[\"ALL YRS\"].iloc[2]\n",
    "    sample2_size = analysisdf.loc[\"ALL YRS\"].iloc[3]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "analysisdf.loc[\"ALL YRS\"].iloc[4] = pval\n",
    "\n",
    "analysisdf = analysisdf.head(9).style.format(\"{:,.2f}\")\n",
    "analysisdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6e4039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING A COLUMN JUST FOR L1F1\n",
    "df[\"L1F1\"] = np.where((df[\"CLASS_OF_ADMISSION\"]==\"L-1\")|(df[\"CLASS_OF_ADMISSION\"]==\"F-1\"),1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6111e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DIVING DEEPER INTO THE L1/F1\n",
    "#ANCHORED BY STATE: PW SKILL,APPLICATIONS, MEDIAN WAGE, YRS POST GRADE, MEDIAN DAYS\n",
    "\n",
    "code = 1\n",
    "# CHANGE THIS FOR ANY CODE\n",
    "\n",
    "# MEDIAN WAGE, MEDIAN YEARS AFTER GRADUATING, MEDIAN DAYS TO DECIDE : BY PW LEVEL\n",
    "topstate = df[df[\"L1F1\"] == code][\"WORKSITE_STATE\"].value_counts()[:15].index.tolist()\n",
    "pwskill = df[\"PW_SKILL_LEVEL\"].dropna().unique().tolist()\n",
    "pwskill.sort()\n",
    "\n",
    "\n",
    "statelist = []\n",
    "pwlist = pwskill*len(topstate)\n",
    "apps = []\n",
    "medwage = []\n",
    "yrsgrad = []\n",
    "meddays = []\n",
    "\n",
    "\n",
    "for x in range(len(topstate)):\n",
    "    s = (topstate[x]+\",\")*len(pwskill)\n",
    "    s = \"\".join(s).split(\",\")\n",
    "    s = list(filter((\"\").__ne__, s))\n",
    "    statelist.append(s) \n",
    "for x in range(len(topstate)):\n",
    "    for j in range(len(pwskill)):\n",
    "        a = len(df[(df[\"L1F1\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"L1F1\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]])\n",
    "        apps.append(a)\n",
    "        b = df[(df[\"L1F1\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"L1F1\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]][\"WO_A\"].median()\n",
    "        medwage.append(b)\n",
    "        c = df[(df[\"L1F1\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"L1F1\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]][\"YRSPOSTGRAD\"].median()\n",
    "        yrsgrad.append(c)\n",
    "        d = df[(df[\"L1F1\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"L1F1\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]][\"DAYS_TO_DECIDE\"].median()\n",
    "        meddays.append(d)\n",
    "codestatedf = pd.DataFrame([sum(statelist, []),pwlist,apps,medwage,yrsgrad,meddays]).transpose()\n",
    "codestatedf.columns = [\"STATE\",\"PW LEVEL\", \"TOTAL APPLICATIONS\",\"MEDIAN WAGE\",\"MEDIAN YRS POST GRAD\",\"MEDIAN DAYS TO DECIDE\"]\n",
    "codestatedf = codestatedf.set_index([\"STATE\",\"PW LEVEL\"])\n",
    "codestatedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32001bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANCHORED BY STATE: EDU LEVEL,APPLICATIONS, MEDIAN WAGE, YRS POST GRADE, MEDIAN DAYS\n",
    "# MEDIAN WAGE, MEDIAN YEARS AFTER GRADUATING, MEDIAN DAYS TO DECIDE : BY EDU LEVEL\n",
    "edu = df[\"FOREIGN_WORKER_EDUCATION\"].dropna().unique().tolist()\n",
    "edu[0],edu[1],edu[2],edu[3],edu[4],edu[5] = edu[3],edu[5],edu[0],edu[2],edu[1],edu[4]\n",
    "\n",
    "statelist = []\n",
    "edulist = edu*len(topstate)\n",
    "apps = []\n",
    "medwage = []\n",
    "yrsgrad = []\n",
    "meddays = []\n",
    "\n",
    "\n",
    "for x in range(len(topstate)):\n",
    "    s = (topstate[x]+\",\")*len(edu)\n",
    "    s = \"\".join(s).split(\",\")\n",
    "    s = list(filter((\"\").__ne__, s))\n",
    "    statelist.append(s) \n",
    "for x in range(len(topstate)):\n",
    "    for j in range(len(edu)):\n",
    "        a = len(df[(df[\"L1F1\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"L1F1\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]])\n",
    "        apps.append(a)\n",
    "        b = df[(df[\"L1F1\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"L1F1\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]][\"WO_A\"].median()\n",
    "        medwage.append(b)\n",
    "        c = df[(df[\"L1F1\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"L1F1\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]][\"YRSPOSTGRAD\"].median()\n",
    "        yrsgrad.append(c)\n",
    "        d = df[(df[\"L1F1\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"L1F1\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]][\"DAYS_TO_DECIDE\"].median()\n",
    "        meddays.append(d)\n",
    "codestatedf1 = pd.DataFrame([sum(statelist, []),edulist,apps,medwage,yrsgrad,meddays]).transpose()\n",
    "codestatedf1.columns = [\"STATE\",\"EDU LEVEL\", \"TOTAL APPLICATIONS\",\"MEDIAN WAGE\",\"MEDIAN YRS POST GRAD\",\"MEDIAN DAYS TO DECIDE\"]\n",
    "codestatedf1 = codestatedf1.set_index([\"STATE\",\"EDU LEVEL\"])\n",
    "codestatedf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de205dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOREIGN WORKER TRAINING COMPLETED? THERE WILL BE A LOT 0 DUE TO N/A or missing \n",
    "# PLOTTING THE DIFFERENT SUCCESS RATES OVER TIME BY TRAINING COMPLETE or not/not applicable (IN TERMS OF ANNUAL APPLICATIONS)\n",
    "\n",
    "top15 = df[df[\"FOREIGN_WORKER_TRAINING_COMP\"]==1]\n",
    "not15 = df[df[\"FOREIGN_WORKER_TRAINING_COMP\"]==0]\n",
    "\n",
    "topdf = pd.DataFrame()\n",
    "notdf = pd.DataFrame()\n",
    "analysisdf = pd.DataFrame()\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    j = pd.DataFrame([len(top15[(top15[\"YEAR\"]==yr[x])&(top15[\"CASE_STATUS\"]==1)])/len(top15[top15[\"YEAR\"]==yr[x]])])\n",
    "    b = pd.DataFrame([len(not15[(not15[\"YEAR\"]==yr[x])&(not15[\"CASE_STATUS\"]==1)])/len(not15[not15[\"YEAR\"]==yr[x]])])\n",
    "    topdf = pd.concat([topdf,j])\n",
    "    notdf = pd.concat([notdf,b])\n",
    "    analysisdf = pd.concat([topdf,notdf], axis = 1)\n",
    "\n",
    "analysisdf.index = yr\n",
    "analysisdf.columns = [\"Training Success %\", \"Not Training Success %\"]\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "analysisdf.plot(kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Success Rates by Required Training Completed or not, Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b686a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARING DATA FOR LIN REGRESS AND GETTING P VALUE\n",
    "top15apps = []\n",
    "not15apps = []\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    t = (top15[\"YEAR\"]==yr[x]).sum()\n",
    "    n = (not15[\"YEAR\"]==yr[x]).sum()\n",
    "    top15apps.append(t)\n",
    "    not15apps.append(n)\n",
    "analysisdf = analysisdf.reset_index()\n",
    "analysisdf[\"Training Apps\"] = pd.DataFrame(top15apps)\n",
    "analysisdf[\"Not Training Apps\"] = pd.DataFrame(not15apps)\n",
    "analysisdf = analysisdf.set_index(\"index\")\n",
    "# PLOTTING APPLICATION RATES OVER TIME BY TOP 15 or NOT TOP 15\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "analysisdf.reset_index().plot(x = \"index\", y = [\"Training Apps\",\"Not Training Apps\"], kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Application Rates by Required Training or not, Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69677d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P VALUE PER YEAR: ON THE SUCCESS RATE\n",
    "top_not = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = analysisdf.loc[yr[x]][\"Training Success %\"]\n",
    "    sample2_phat = analysisdf.loc[yr[x]][\"Not Training Success %\"]\n",
    "    sample1_size = analysisdf.loc[yr[x]][\"Training Apps\"]\n",
    "    sample2_size = analysisdf.loc[yr[x]][\"Not Training Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    top_not.append(pval)\n",
    "\n",
    "pvaldf = pd.DataFrame(top_not)\n",
    "pvaldf.columns = [\"Training Vs. Not Training\"]\n",
    "pvaldf.index = yr\n",
    "analysisdf[\"PVAL Success %\"] = pvaldf[\"Training Vs. Not Training\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af980451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION BETWEEN required training completed AND SUCCESS RATE OVER TIME\n",
    "print(\"Training success rate over time:\",st.linregress(y = analysisdf[\"Training Success %\"], x = yr))\n",
    "print(\"\\nTraining applications over time:\",st.linregress(y = analysisdf[\"Training Apps\"], x = yr))\n",
    "\n",
    "print(\"\\n\\nNot Training success rate over time:\",st.linregress(y = analysisdf[\"Not Training Success %\"], x = yr))\n",
    "print(\"\\nNot Training applications over time:\",st.linregress(y = analysisdf[\"Not Training Apps\"], x = yr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cb3ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING THE TOTAL PVAL\n",
    "analysisdf.loc[\"ALL YRS\"] = analysisdf.sum()\n",
    "top15suc = []\n",
    "not15suc = []\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    t15 = analysisdf.loc[yr[x]].iloc[0]*analysisdf.loc[yr[x]].iloc[2]\n",
    "    n15 = analysisdf.loc[yr[x]].iloc[1]*analysisdf.loc[yr[x]].iloc[3]\n",
    "    top15suc.append(t15)\n",
    "    not15suc.append(n15)\n",
    "analysisdf.loc[\"ALL YRS\"].iloc[0] = sum(top15suc)/analysisdf.loc[\"ALL YRS\"].iloc[2]\n",
    "analysisdf.loc[\"ALL YRS\"].iloc[1] = sum(not15suc)/analysisdf.loc[\"ALL YRS\"].iloc[3]\n",
    "\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = analysisdf.loc[\"ALL YRS\"].iloc[0]\n",
    "    sample2_phat = analysisdf.loc[\"ALL YRS\"].iloc[1]\n",
    "    sample1_size = analysisdf.loc[\"ALL YRS\"].iloc[2]\n",
    "    sample2_size = analysisdf.loc[\"ALL YRS\"].iloc[3]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "analysisdf.loc[\"ALL YRS\"].iloc[4] = pval\n",
    "\n",
    "analysisdf = analysisdf.head(9).style.format(\"{:,.2f}\")\n",
    "analysisdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffcfb02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DIVING DEEPER INTO required training jobs\n",
    "#ANCHORED BY STATE: PW SKILL,APPLICATIONS, MEDIAN WAGE, YRS POST GRADE, MEDIAN DAYS\n",
    "\n",
    "code = 1\n",
    "# CHANGE THIS FOR ANY CODE\n",
    "\n",
    "# MEDIAN WAGE, MEDIAN YEARS AFTER GRADUATING, MEDIAN DAYS TO DECIDE : BY PW LEVEL\n",
    "topstate = df[df[\"FOREIGN_WORKER_TRAINING_COMP\"] == code][\"WORKSITE_STATE\"].value_counts()[:15].index.tolist()\n",
    "pwskill = df[\"PW_SKILL_LEVEL\"].dropna().unique().tolist()\n",
    "pwskill.sort()\n",
    "\n",
    "\n",
    "statelist = []\n",
    "pwlist = pwskill*len(topstate)\n",
    "apps = []\n",
    "medwage = []\n",
    "yrsgrad = []\n",
    "meddays = []\n",
    "\n",
    "\n",
    "for x in range(len(topstate)):\n",
    "    s = (topstate[x]+\",\")*len(pwskill)\n",
    "    s = \"\".join(s).split(\",\")\n",
    "    s = list(filter((\"\").__ne__, s))\n",
    "    statelist.append(s) \n",
    "for x in range(len(topstate)):\n",
    "    for j in range(len(pwskill)):\n",
    "        a = len(df[(df[\"FOREIGN_WORKER_TRAINING_COMP\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"FOREIGN_WORKER_TRAINING_COMP\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]])\n",
    "        apps.append(a)\n",
    "        b = df[(df[\"FOREIGN_WORKER_TRAINING_COMP\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"FOREIGN_WORKER_TRAINING_COMP\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]][\"WO_A\"].median()\n",
    "        medwage.append(b)\n",
    "        c = df[(df[\"FOREIGN_WORKER_TRAINING_COMP\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"FOREIGN_WORKER_TRAINING_COMP\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]][\"YRSPOSTGRAD\"].median()\n",
    "        yrsgrad.append(c)\n",
    "        d = df[(df[\"FOREIGN_WORKER_TRAINING_COMP\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"FOREIGN_WORKER_TRAINING_COMP\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]][\"DAYS_TO_DECIDE\"].median()\n",
    "        meddays.append(d)\n",
    "codestatedf = pd.DataFrame([sum(statelist, []),pwlist,apps,medwage,yrsgrad,meddays]).transpose()\n",
    "codestatedf.columns = [\"STATE\",\"PW LEVEL\", \"TOTAL APPLICATIONS\",\"MEDIAN WAGE\",\"MEDIAN YRS POST GRAD\",\"MEDIAN DAYS TO DECIDE\"]\n",
    "codestatedf = codestatedf.set_index([\"STATE\",\"PW LEVEL\"])\n",
    "codestatedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39696d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANCHORED BY STATE: EDU LEVEL,APPLICATIONS, MEDIAN WAGE, YRS POST GRADE, MEDIAN DAYS\n",
    "# MEDIAN WAGE, MEDIAN YEARS AFTER GRADUATING, MEDIAN DAYS TO DECIDE : BY EDU LEVEL\n",
    "edu = df[\"FOREIGN_WORKER_EDUCATION\"].dropna().unique().tolist()\n",
    "edu[0],edu[1],edu[2],edu[3],edu[4],edu[5] = edu[3],edu[5],edu[0],edu[2],edu[1],edu[4]\n",
    "\n",
    "statelist = []\n",
    "edulist = edu*len(topstate)\n",
    "apps = []\n",
    "medwage = []\n",
    "yrsgrad = []\n",
    "meddays = []\n",
    "\n",
    "\n",
    "for x in range(len(topstate)):\n",
    "    s = (topstate[x]+\",\")*len(edu)\n",
    "    s = \"\".join(s).split(\",\")\n",
    "    s = list(filter((\"\").__ne__, s))\n",
    "    statelist.append(s) \n",
    "for x in range(len(topstate)):\n",
    "    for j in range(len(edu)):\n",
    "        a = len(df[(df[\"FOREIGN_WORKER_TRAINING_COMP\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"FOREIGN_WORKER_TRAINING_COMP\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]])\n",
    "        apps.append(a)\n",
    "        b = df[(df[\"FOREIGN_WORKER_TRAINING_COMP\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"FOREIGN_WORKER_TRAINING_COMP\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]][\"WO_A\"].median()\n",
    "        medwage.append(b)\n",
    "        c = df[(df[\"FOREIGN_WORKER_TRAINING_COMP\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"FOREIGN_WORKER_TRAINING_COMP\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]][\"YRSPOSTGRAD\"].median()\n",
    "        yrsgrad.append(c)\n",
    "        d = df[(df[\"FOREIGN_WORKER_TRAINING_COMP\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"FOREIGN_WORKER_TRAINING_COMP\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]][\"DAYS_TO_DECIDE\"].median()\n",
    "        meddays.append(d)\n",
    "codestatedf1 = pd.DataFrame([sum(statelist, []),edulist,apps,medwage,yrsgrad,meddays]).transpose()\n",
    "codestatedf1.columns = [\"STATE\",\"EDU LEVEL\", \"TOTAL APPLICATIONS\",\"MEDIAN WAGE\",\"MEDIAN YRS POST GRAD\",\"MEDIAN DAYS TO DECIDE\"]\n",
    "codestatedf1 = codestatedf1.set_index([\"STATE\",\"EDU LEVEL\"])\n",
    "codestatedf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e194505f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOOKS LIKE A LOT OF CLUSTER AROUND LEVEL 1 AND LEVEL 2: FOR TRAINING REQUIRED\n",
    "# THAT IS ODD... LETS SEE WHAT KIND OF EMPLOYER HAS REQUIRED TRAINING\n",
    "df[df[\"FOREIGN_WORKER_TRAINING_COMP\"]==1][\"NAICS_CODE\"].value_counts()[:5]\n",
    "#621111 and 622110 are medical related, 611310 is University... very odd that the cluster is around level 1 jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4524f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PW SKILL DISTRIBUTION OF TRAINING REQ... DOESNT MAKE SENSE...\n",
    "df[df[\"FOREIGN_WORKER_TRAINING_COMP\"]==1][\"PW_SKILL_LEVEL\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b968b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOP JOB CODE IS GENERAL INTERNISTS, ADULT CARE... \n",
    "df[(df[\"FOREIGN_WORKER_TRAINING_COMP\"]==1)&(df[\"PW_SKILL_LEVEL\"]==\"LEVEL I\")][\"PW_SOC_CODE\"].value_counts()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8f8a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUCCESS RATE FOR THE TOP 3 NAICS CODES vs the rest of naics codes: THAT IS WHY!\n",
    "reqtrain = df[df[\"FOREIGN_WORKER_TRAINING_COMP\"]==1]\n",
    "print(\"SUCCESS RATE FOR TOP 3:\",reqtrain[(reqtrain[\"NAICS_CODE\"] == \"621111\")|(reqtrain[\"NAICS_CODE\"] == \"622110\")|(reqtrain[\"NAICS_CODE\"] == \"611310\")][\"CASE_STATUS\"].sum()/len(reqtrain[(reqtrain[\"NAICS_CODE\"] == \"621111\")|(reqtrain[\"NAICS_CODE\"] == \"622110\")|(reqtrain[\"NAICS_CODE\"] == \"611310\")]))\n",
    "print(\"SUCCESS RATE FOR THE REST:\",reqtrain[(reqtrain[\"NAICS_CODE\"] != \"621111\")&(reqtrain[\"NAICS_CODE\"] != \"622110\")&(reqtrain[\"NAICS_CODE\"] != \"611310\")][\"CASE_STATUS\"].sum()/len(reqtrain[(reqtrain[\"NAICS_CODE\"] != \"621111\")&(reqtrain[\"NAICS_CODE\"] != \"622110\")&(reqtrain[\"NAICS_CODE\"] != \"611310\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c6c2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOREIGN WORKER REQ EXPERIENCE MET? THERE WILL BE SOME 0 due to N/A or missing\n",
    "# FOREIGN WORKER REQ EXPERIENCE COMPLETED? THERE WILL BE A LOT 0 DUE TO N/A or missing \n",
    "# PLOTTING THE DIFFERENT SUCCESS RATES OVER TIME BY TRAINING COMPLETE or not/not applicable (IN TERMS OF ANNUAL APPLICATIONS)\n",
    "\n",
    "top15 = df[df[\"FOREIGN_WORKER_REQ_EXPERIENCE\"]==1]\n",
    "not15 = df[df[\"FOREIGN_WORKER_REQ_EXPERIENCE\"]==0]\n",
    "\n",
    "topdf = pd.DataFrame()\n",
    "notdf = pd.DataFrame()\n",
    "analysisdf = pd.DataFrame()\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    j = pd.DataFrame([len(top15[(top15[\"YEAR\"]==yr[x])&(top15[\"CASE_STATUS\"]==1)])/len(top15[top15[\"YEAR\"]==yr[x]])])\n",
    "    b = pd.DataFrame([len(not15[(not15[\"YEAR\"]==yr[x])&(not15[\"CASE_STATUS\"]==1)])/len(not15[not15[\"YEAR\"]==yr[x]])])\n",
    "    topdf = pd.concat([topdf,j])\n",
    "    notdf = pd.concat([notdf,b])\n",
    "    analysisdf = pd.concat([topdf,notdf], axis = 1)\n",
    "\n",
    "analysisdf.index = yr\n",
    "analysisdf.columns = [\"Exp Success %\", \"Not Exp Success %\"]\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "analysisdf.plot(kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Success Rates by Required Experience Completed or not, Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2e4c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARING DATA FOR LIN REGRESS AND GETTING P VALUE\n",
    "top15apps = []\n",
    "not15apps = []\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    t = (top15[\"YEAR\"]==yr[x]).sum()\n",
    "    n = (not15[\"YEAR\"]==yr[x]).sum()\n",
    "    top15apps.append(t)\n",
    "    not15apps.append(n)\n",
    "analysisdf = analysisdf.reset_index()\n",
    "analysisdf[\"Exp Apps\"] = pd.DataFrame(top15apps)\n",
    "analysisdf[\"Not Exp Apps\"] = pd.DataFrame(not15apps)\n",
    "analysisdf = analysisdf.set_index(\"index\")\n",
    "# PLOTTING APPLICATION RATES OVER TIME BY TOP 15 or NOT TOP 15\n",
    "plt.figure(figsize = (12,10), dpi = 100)\n",
    "analysisdf.reset_index().plot(x = \"index\", y = [\"Exp Apps\",\"Not Exp Apps\"], kind = \"bar\")\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Application Rates by Required Experience Completed or not, Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b42571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P VALUE PER YEAR: ON THE SUCCESS RATE\n",
    "top_not = []\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = analysisdf.loc[yr[x]][\"Exp Success %\"]\n",
    "    sample2_phat = analysisdf.loc[yr[x]][\"Not Exp Success %\"]\n",
    "    sample1_size = analysisdf.loc[yr[x]][\"Exp Apps\"]\n",
    "    sample2_size = analysisdf.loc[yr[x]][\"Not Exp Apps\"]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "    top_not.append(pval)\n",
    "\n",
    "pvaldf = pd.DataFrame(top_not)\n",
    "pvaldf.columns = [\"Exp Vs. Not Exp\"]\n",
    "pvaldf.index = yr\n",
    "analysisdf[\"PVAL Success %\"] = pvaldf[\"Exp Vs. Not Exp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b195337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION BETWEEN Experience Required Completed AND SUCCESS RATE OVER TIME\n",
    "print(\"Exp success rate over time:\",st.linregress(y = analysisdf[\"Exp Success %\"], x = yr))\n",
    "print(\"\\nExp applications over time:\",st.linregress(y = analysisdf[\"Exp Apps\"], x = yr))\n",
    "\n",
    "print(\"\\n\\nNot Exp success rate over time:\",st.linregress(y = analysisdf[\"Not Exp Success %\"], x = yr))\n",
    "print(\"\\nNot Exp applications over time:\",st.linregress(y = analysisdf[\"Not Exp Apps\"], x = yr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a66fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING THE TOTAL PVAL\n",
    "analysisdf.loc[\"ALL YRS\"] = analysisdf.sum()\n",
    "top15suc = []\n",
    "not15suc = []\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    t15 = analysisdf.loc[yr[x]].iloc[0]*analysisdf.loc[yr[x]].iloc[2]\n",
    "    n15 = analysisdf.loc[yr[x]].iloc[1]*analysisdf.loc[yr[x]].iloc[3]\n",
    "    top15suc.append(t15)\n",
    "    not15suc.append(n15)\n",
    "analysisdf.loc[\"ALL YRS\"].iloc[0] = sum(top15suc)/analysisdf.loc[\"ALL YRS\"].iloc[2]\n",
    "analysisdf.loc[\"ALL YRS\"].iloc[1] = sum(not15suc)/analysisdf.loc[\"ALL YRS\"].iloc[3]\n",
    "\n",
    "\n",
    "for x in range(len(yr)):\n",
    "    sample1_phat = analysisdf.loc[\"ALL YRS\"].iloc[0]\n",
    "    sample2_phat = analysisdf.loc[\"ALL YRS\"].iloc[1]\n",
    "    sample1_size = analysisdf.loc[\"ALL YRS\"].iloc[2]\n",
    "    sample2_size = analysisdf.loc[\"ALL YRS\"].iloc[3]\n",
    "    phat = (sample1_phat*sample1_size+sample2_phat*sample2_size)/(sample1_size+sample2_size)\n",
    "    zscore = (sample1_phat - sample2_phat)/np.sqrt(phat*(1-phat)*((1/sample1_size)+(1/sample2_size)))    \n",
    "    pval = st.norm.sf(abs(zscore))*2\n",
    "analysisdf.loc[\"ALL YRS\"].iloc[4] = pval\n",
    "\n",
    "analysisdf = analysisdf.head(9).style.format(\"{:,.2f}\")\n",
    "analysisdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2e0838",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DIVING DEEPER INTO Experience Required \n",
    "#ANCHORED BY STATE: PW SKILL,APPLICATIONS, MEDIAN WAGE, YRS POST GRADE, MEDIAN DAYS\n",
    "\n",
    "code = 1\n",
    "# CHANGE THIS FOR ANY CODE\n",
    "\n",
    "# MEDIAN WAGE, MEDIAN YEARS AFTER GRADUATING, MEDIAN DAYS TO DECIDE : FOR EACH STATE IN THE TOP 15 OF THE COMPANY AND BY PW LEVEL\n",
    "topstate = df[df[\"FOREIGN_WORKER_REQ_EXPERIENCE\"] == code][\"WORKSITE_STATE\"].value_counts()[:15].index.tolist()\n",
    "pwskill = df[\"PW_SKILL_LEVEL\"].dropna().unique().tolist()\n",
    "pwskill.sort()\n",
    "\n",
    "\n",
    "statelist = []\n",
    "pwlist = pwskill*len(topstate)\n",
    "apps = []\n",
    "medwage = []\n",
    "yrsgrad = []\n",
    "meddays = []\n",
    "\n",
    "\n",
    "for x in range(len(topstate)):\n",
    "    s = (topstate[x]+\",\")*len(pwskill)\n",
    "    s = \"\".join(s).split(\",\")\n",
    "    s = list(filter((\"\").__ne__, s))\n",
    "    statelist.append(s) \n",
    "for x in range(len(topstate)):\n",
    "    for j in range(len(pwskill)):\n",
    "        a = len(df[(df[\"FOREIGN_WORKER_REQ_EXPERIENCE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"FOREIGN_WORKER_REQ_EXPERIENCE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]])\n",
    "        apps.append(a)\n",
    "        b = df[(df[\"FOREIGN_WORKER_REQ_EXPERIENCE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"FOREIGN_WORKER_REQ_EXPERIENCE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]][\"WO_A\"].median()\n",
    "        medwage.append(b)\n",
    "        c = df[(df[\"FOREIGN_WORKER_REQ_EXPERIENCE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"FOREIGN_WORKER_REQ_EXPERIENCE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]][\"YRSPOSTGRAD\"].median()\n",
    "        yrsgrad.append(c)\n",
    "        d = df[(df[\"FOREIGN_WORKER_REQ_EXPERIENCE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"FOREIGN_WORKER_REQ_EXPERIENCE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"PW_SKILL_LEVEL\"]==pwskill[j]][\"DAYS_TO_DECIDE\"].median()\n",
    "        meddays.append(d)\n",
    "codestatedf = pd.DataFrame([sum(statelist, []),pwlist,apps,medwage,yrsgrad,meddays]).transpose()\n",
    "codestatedf.columns = [\"STATE\",\"PW LEVEL\", \"TOTAL APPLICATIONS\",\"MEDIAN WAGE\",\"MEDIAN YRS POST GRAD\",\"MEDIAN DAYS TO DECIDE\"]\n",
    "codestatedf = codestatedf.set_index([\"STATE\",\"PW LEVEL\"])\n",
    "codestatedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91afde3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANCHORED BY STATE: EDU LEVEL,APPLICATIONS, MEDIAN WAGE, YRS POST GRADE, MEDIAN DAYS\n",
    "# MEDIAN WAGE, MEDIAN YEARS AFTER GRADUATING, MEDIAN DAYS TO DECIDE : BY EDU LEVEL\n",
    "edu = df[\"FOREIGN_WORKER_EDUCATION\"].dropna().unique().tolist()\n",
    "edu[0],edu[1],edu[2],edu[3],edu[4],edu[5] = edu[3],edu[5],edu[0],edu[2],edu[1],edu[4]\n",
    "\n",
    "statelist = []\n",
    "edulist = edu*len(topstate)\n",
    "apps = []\n",
    "medwage = []\n",
    "yrsgrad = []\n",
    "meddays = []\n",
    "\n",
    "\n",
    "for x in range(len(topstate)):\n",
    "    s = (topstate[x]+\",\")*len(edu)\n",
    "    s = \"\".join(s).split(\",\")\n",
    "    s = list(filter((\"\").__ne__, s))\n",
    "    statelist.append(s) \n",
    "for x in range(len(topstate)):\n",
    "    for j in range(len(edu)):\n",
    "        a = len(df[(df[\"FOREIGN_WORKER_REQ_EXPERIENCE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"FOREIGN_WORKER_REQ_EXPERIENCE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]])\n",
    "        apps.append(a)\n",
    "        b = df[(df[\"FOREIGN_WORKER_REQ_EXPERIENCE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"FOREIGN_WORKER_REQ_EXPERIENCE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]][\"WO_A\"].median()\n",
    "        medwage.append(b)\n",
    "        c = df[(df[\"FOREIGN_WORKER_REQ_EXPERIENCE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"FOREIGN_WORKER_REQ_EXPERIENCE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]][\"YRSPOSTGRAD\"].median()\n",
    "        yrsgrad.append(c)\n",
    "        d = df[(df[\"FOREIGN_WORKER_REQ_EXPERIENCE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][df[(df[\"FOREIGN_WORKER_REQ_EXPERIENCE\"] == code)&(df[\"WORKSITE_STATE\"] == topstate[x])][\"FOREIGN_WORKER_EDUCATION\"]==edu[j]][\"DAYS_TO_DECIDE\"].median()\n",
    "        meddays.append(d)\n",
    "codestatedf1 = pd.DataFrame([sum(statelist, []),edulist,apps,medwage,yrsgrad,meddays]).transpose()\n",
    "codestatedf1.columns = [\"STATE\",\"EDU LEVEL\", \"TOTAL APPLICATIONS\",\"MEDIAN WAGE\",\"MEDIAN YRS POST GRAD\",\"MEDIAN DAYS TO DECIDE\"]\n",
    "codestatedf1 = codestatedf1.set_index([\"STATE\",\"EDU LEVEL\"])\n",
    "codestatedf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1913442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THAT IS ODD... LETS SEE WHAT KIND OF EMPLOYER HAS REQUIRED EXPERIENCE\n",
    "# PW SKILL DISTRIBUTION OF EXP REQ... DOESNT MAKE SENSE...\n",
    "df[df[\"FOREIGN_WORKER_REQ_EXPERIENCE\"]==1][\"PW_SKILL_LEVEL\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9047249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"FOREIGN_WORKER_REQ_EXPERIENCE\"]==1][\"NAICS_CODE\"].value_counts()[:5]\n",
    "#541511 and 541512 are computer systems and software related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2655e1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOP JOB CODE IS SOFTWARE DEVELOPER... \n",
    "df[(df[\"FOREIGN_WORKER_REQ_EXPERIENCE\"]==1)&(df[\"PW_SKILL_LEVEL\"]==\"LEVEL II\")][\"PW_SOC_CODE\"].value_counts()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d3a18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOP JOB CODE IS SOFTWARE DEVELOPER... \n",
    "df[(df[\"FOREIGN_WORKER_REQ_EXPERIENCE\"]==1)&(df[\"PW_SKILL_LEVEL\"]==\"LEVEL IV\")][\"PW_SOC_CODE\"].value_counts()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229a6fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUCCESS RATE FOR THE TOP 2 NAICS CODES vs the rest of naics codes: drop off is not so steep..\n",
    "reqtrain = df[df[\"FOREIGN_WORKER_REQ_EXPERIENCE\"]==1]\n",
    "print(\"SUCCESS RATE FOR TOP 2:\",reqtrain[(reqtrain[\"NAICS_CODE\"] == \"541511\")|(reqtrain[\"NAICS_CODE\"] == \"541512\")][\"CASE_STATUS\"].sum()/len(reqtrain[(reqtrain[\"NAICS_CODE\"] == \"541511\")|(reqtrain[\"NAICS_CODE\"] == \"541512\")]))\n",
    "print(\"SUCCESS RATE FOR THE REST:\",reqtrain[(reqtrain[\"NAICS_CODE\"] != \"541511\")&(reqtrain[\"NAICS_CODE\"] != \"541512\")][\"CASE_STATUS\"].sum()/len(reqtrain[(reqtrain[\"NAICS_CODE\"] != \"541511\")&(reqtrain[\"NAICS_CODE\"] != \"541512\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d418a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUCCESS RATE FOR top 3 job codes\n",
    "reqtrain = df[df[\"FOREIGN_WORKER_REQ_EXPERIENCE\"]==1]\n",
    "print(\"SUCCESS RATE FOR TOP 3:\",reqtrain[(reqtrain[\"PW_SOC_CODE\"] == \"15-1132\")|(reqtrain[\"PW_SOC_CODE\"] == \"15-1121\")|(reqtrain[\"PW_SOC_CODE\"] == \"15-1133\")][\"CASE_STATUS\"].sum()/len(reqtrain[(reqtrain[\"PW_SOC_CODE\"] == \"15-1132\")|(reqtrain[\"PW_SOC_CODE\"] == \"15-1121\")|(reqtrain[\"PW_SOC_CODE\"] == \"15-1133\")]))\n",
    "print(\"SUCCESS RATE FOR THE REST:\",reqtrain[(reqtrain[\"PW_SOC_CODE\"] != \"15-1132\")&(reqtrain[\"PW_SOC_CODE\"] != \"15-1121\")&(reqtrain[\"PW_SOC_CODE\"] != \"15-1133\")][\"CASE_STATUS\"].sum()/len(reqtrain[(reqtrain[\"PW_SOC_CODE\"] != \"15-1132\")&(reqtrain[\"PW_SOC_CODE\"] != \"15-1121\")&(reqtrain[\"PW_SOC_CODE\"] != \"15-1133\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3818e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING A NEW COLUMN: SPLITTING THE APPLICANT's EDUCATION LEVEL INTO 4 GROUPS\n",
    "df[\"EDU_GROUP\"] = np.nan\n",
    "for x in range(len(yr)):\n",
    "    df[\"EDU_GROUP\"] = np.where((df[\"FOREIGN_WORKER_EDUCATION\"]==\"HIGH SCHOOL\")|(df[\"FOREIGN_WORKER_EDUCATION\"]==\"ASSOCIATE'S\"),\"LOWER\",df[\"EDU_GROUP\"])\n",
    "    df[\"EDU_GROUP\"] = np.where(df[\"FOREIGN_WORKER_EDUCATION\"]==\"BACHELOR'S\",\"COLLEGE\",df[\"REQ_EDU_GROUP\"])\n",
    "    df[\"EDU_GROUP\"] = np.where((df[\"FOREIGN_WORKER_EDUCATION\"]==\"MASTER'S\")|(df[\"FOREIGN_WORKER_EDUCATION\"]==\"DOCTORATE\"),\"UPPER\",df[\"EDU_GROUP\"])\n",
    "    df[\"EDU_GROUP\"] = np.where((df[\"FOREIGN_WORKER_EDUCATION\"].isnull())|(df[\"FOREIGN_WORKER_EDUCATION\"]==\"OTHER\"),\"OTHER\",df[\"EDU_GROUP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5c010e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROPPING ALL #VALUE!\n",
    "listy = df.columns.tolist()\n",
    "for x in range(len(listy)):\n",
    "    df = df[df[listy[x]]!=\"#VALUE!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbd6173",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcaadbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### PREPARING THE DATA FOR MACHINE LEARNING ###\n",
    "# DROPPING THE COLUMNS NOT USED FOR ML\n",
    "model = df[['CASE_STATUS',\n",
    "'DAYS_TO_DECIDE',\n",
    "'REFILE',\n",
    "'EMPLOYER_STATE_PROVINCE',\n",
    "'EMPLOYER_COUNTRY',\n",
    "'EMPLOYER_NUM_EMPLOYEES',\n",
    "'EMPLOYER_YEAR_COMMENCED_BUSINESS',\n",
    "'PW_SOC_CODE',\n",
    "'PW_SKILL_LEVEL',\n",
    "'PW_DURATION',\n",
    "'APP_TO_PW_DETERMINE',\n",
    "'WAGE_OFFER_UNIT_OF_PAY',\n",
    "'WORKSITE_STATE',\n",
    "'REQUIRED_TRAINING',\n",
    "'REQUIRED_EXPERIENCE',\n",
    "'ACCEPT_FOREIGN_EDUCATION',\n",
    "'ACCEPT_ALT_OCCUPATION',\n",
    "'JOB_OPP_REQUIREMENTS_NORMAL',\n",
    "'FOREIGN_LANGUAGE_REQUIRED',\n",
    "'PROFESSIONAL_OCCUPATION',\n",
    "'SWA_DURATION',\n",
    "'FIRST_AD_TO_APP_DAYS',\n",
    "'SECOND_AD_TO_APP_DAYS',\n",
    "'JOB_FAIR',\n",
    "'ON_CAMPUS_RECRUITING',\n",
    "'EMPLOYER_WEBSITE',\n",
    "'PRO_ORG_AD',\n",
    "'JOB_SEARCH_WEBSITE',\n",
    "'PVT_EMPLOYMENT_FIRM',\n",
    "'EMPLOYEE_REF_PROG',\n",
    "'CAMPUS_PLACEMENT',\n",
    "'LOCAL_ETHNIC_PAPER',\n",
    "'RADIO_TV_AD',\n",
    "'CLASS_OF_ADMISSION',\n",
    "'FOREIGN_WORKER_YRS_ED_COMP',\n",
    "'FOREIGN_WORKER_TRAINING_COMP',\n",
    "'FOREIGN_WORKER_REQ_EXPERIENCE',\n",
    "'FOREIGN_WORKER_ALT_ED_EXP',\n",
    "'FOREIGN_WORKER_ALT_OCC_EXP',\n",
    "'MONTH',\n",
    "'YEAR',\n",
    "'WO_A',\n",
    "'PW_A',\n",
    "'WO-PW',\n",
    "'WORKSITE_REGION',\n",
    "'EMPLOYER_REGION',\n",
    "'DISC_ST',\n",
    "'DISC_CTY',\n",
    "'DISC_RGION',\n",
    "'DISC_UNIT',\n",
    "'YRSPOSTGRAD',\n",
    "'EMPLOYER_SIZE',\n",
    "'EMPLOYER_AGE',\n",
    "'REQ_EDU_GROUP',\n",
    "'EDU_GROUP']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b408dd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM DUMMY VARIABLES FOR MODEL DF\n",
    "model[\"H1L1F1\"] = np.where((model[\"CLASS_OF_ADMISSION\"]==\"L-1\")|(model[\"CLASS_OF_ADMISSION\"]==\"F-1\")|(model[\"CLASS_OF_ADMISSION\"]==\"H-1B\"),1,0)\n",
    "model = model.drop(\"CLASS_OF_ADMISSION\", axis = 1)\n",
    "model[\"PW_SOC_CODE\"] = model[\"PW_SOC_CODE\"].str[:2]\n",
    "model = pd.concat([model,pd.get_dummies(model[\"PW_SOC_CODE\"], drop_first = True)],axis = 1).drop(\"PW_SOC_CODE\",axis = 1)\n",
    "model[\"cosM\"] = np.cos(model[\"MONTH\"]*2*np.pi/12)\n",
    "model[\"sinM\"] = np.sin(model[\"MONTH\"]*2*np.pi/12)\n",
    "model = model.drop(\"MONTH\",axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed78ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = ['EMPLOYER_STATE_PROVINCE',\n",
    "'EMPLOYER_COUNTRY',\n",
    "'PW_SKILL_LEVEL',\n",
    "'WAGE_OFFER_UNIT_OF_PAY',\n",
    "'WORKSITE_STATE',\n",
    "'WORKSITE_REGION',\n",
    "'EMPLOYER_REGION',\n",
    "'EMPLOYER_SIZE',\n",
    "'EMPLOYER_AGE',\n",
    "'REQ_EDU_GROUP',\n",
    "'EDU_GROUP']\n",
    "\n",
    "for x in range(len(dummy)):\n",
    "    model = pd.concat([model,pd.get_dummies(model[dummy[x]], drop_first = True)],axis = 1).drop(dummy[x],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4563d42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to_csv(\"model.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
